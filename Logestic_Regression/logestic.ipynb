{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nihar Muniraju"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Load Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3779567 ,  1.04389498,  1.04349443, ..., -0.0671922 ,\n",
       "         0.17547148, -1.04964564],\n",
       "       [-0.32525851,  1.27626282, -0.68612327, ...,  1.00663329,\n",
       "        -0.83369182,  0.95774417],\n",
       "       [ 0.73901891, -0.60090284, -0.17729436, ..., -0.21898072,\n",
       "         0.87864296, -1.25774001],\n",
       "       ...,\n",
       "       [ 0.67556288, -0.53841971, -1.29950008, ...,  2.04333597,\n",
       "         0.94738793,  0.79035376],\n",
       "       [ 2.62971021, -2.45289885, -1.35978523, ...,  0.37889809,\n",
       "        -1.97189411, -0.2522504 ],\n",
       "       [-1.79149103, -0.12190773,  0.53515332, ..., -1.94135733,\n",
       "         0.58900166, -1.00748218]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting the data to data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.377957</td>\n",
       "      <td>1.043895</td>\n",
       "      <td>1.043494</td>\n",
       "      <td>-0.101838</td>\n",
       "      <td>-1.617442</td>\n",
       "      <td>0.402713</td>\n",
       "      <td>0.913601</td>\n",
       "      <td>-0.067192</td>\n",
       "      <td>0.175471</td>\n",
       "      <td>-1.049646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.325259</td>\n",
       "      <td>1.276263</td>\n",
       "      <td>-0.686123</td>\n",
       "      <td>-2.463205</td>\n",
       "      <td>-0.489426</td>\n",
       "      <td>-0.240715</td>\n",
       "      <td>-1.469496</td>\n",
       "      <td>1.006633</td>\n",
       "      <td>-0.833692</td>\n",
       "      <td>0.957744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.739019</td>\n",
       "      <td>-0.600903</td>\n",
       "      <td>-0.177294</td>\n",
       "      <td>1.335714</td>\n",
       "      <td>-0.817332</td>\n",
       "      <td>-0.790047</td>\n",
       "      <td>1.457365</td>\n",
       "      <td>-0.218981</td>\n",
       "      <td>0.878643</td>\n",
       "      <td>-1.257740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.474312</td>\n",
       "      <td>-1.103002</td>\n",
       "      <td>1.189936</td>\n",
       "      <td>-0.800186</td>\n",
       "      <td>0.912377</td>\n",
       "      <td>-0.406451</td>\n",
       "      <td>-1.130950</td>\n",
       "      <td>1.985111</td>\n",
       "      <td>1.379029</td>\n",
       "      <td>1.041768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.927365</td>\n",
       "      <td>1.114796</td>\n",
       "      <td>0.080284</td>\n",
       "      <td>1.261064</td>\n",
       "      <td>0.761179</td>\n",
       "      <td>0.921563</td>\n",
       "      <td>0.440832</td>\n",
       "      <td>0.184645</td>\n",
       "      <td>-1.567739</td>\n",
       "      <td>-0.142107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1.538272</td>\n",
       "      <td>0.171629</td>\n",
       "      <td>0.075371</td>\n",
       "      <td>-0.957658</td>\n",
       "      <td>-1.066219</td>\n",
       "      <td>1.158096</td>\n",
       "      <td>-0.036964</td>\n",
       "      <td>0.123689</td>\n",
       "      <td>0.927871</td>\n",
       "      <td>-0.225003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-0.060266</td>\n",
       "      <td>0.095018</td>\n",
       "      <td>-0.271685</td>\n",
       "      <td>1.830560</td>\n",
       "      <td>0.219445</td>\n",
       "      <td>-0.341269</td>\n",
       "      <td>1.180088</td>\n",
       "      <td>-0.216876</td>\n",
       "      <td>-1.752938</td>\n",
       "      <td>-0.810152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.675563</td>\n",
       "      <td>-0.538420</td>\n",
       "      <td>-1.299500</td>\n",
       "      <td>0.747835</td>\n",
       "      <td>1.733898</td>\n",
       "      <td>-0.268044</td>\n",
       "      <td>-0.520953</td>\n",
       "      <td>2.043336</td>\n",
       "      <td>0.947388</td>\n",
       "      <td>0.790354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>2.629710</td>\n",
       "      <td>-2.452899</td>\n",
       "      <td>-1.359785</td>\n",
       "      <td>1.592065</td>\n",
       "      <td>0.854157</td>\n",
       "      <td>1.618828</td>\n",
       "      <td>0.621701</td>\n",
       "      <td>0.378898</td>\n",
       "      <td>-1.971894</td>\n",
       "      <td>-0.252250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-1.791491</td>\n",
       "      <td>-0.121908</td>\n",
       "      <td>0.535153</td>\n",
       "      <td>-0.588085</td>\n",
       "      <td>-1.929461</td>\n",
       "      <td>-0.659900</td>\n",
       "      <td>0.754921</td>\n",
       "      <td>-1.941357</td>\n",
       "      <td>0.589002</td>\n",
       "      <td>-1.007482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0   -0.377957  1.043895  1.043494 -0.101838 -1.617442  0.402713  0.913601   \n",
       "1   -0.325259  1.276263 -0.686123 -2.463205 -0.489426 -0.240715 -1.469496   \n",
       "2    0.739019 -0.600903 -0.177294  1.335714 -0.817332 -0.790047  1.457365   \n",
       "3    0.474312 -1.103002  1.189936 -0.800186  0.912377 -0.406451 -1.130950   \n",
       "4    0.927365  1.114796  0.080284  1.261064  0.761179  0.921563  0.440832   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "995  1.538272  0.171629  0.075371 -0.957658 -1.066219  1.158096 -0.036964   \n",
       "996 -0.060266  0.095018 -0.271685  1.830560  0.219445 -0.341269  1.180088   \n",
       "997  0.675563 -0.538420 -1.299500  0.747835  1.733898 -0.268044 -0.520953   \n",
       "998  2.629710 -2.452899 -1.359785  1.592065  0.854157  1.618828  0.621701   \n",
       "999 -1.791491 -0.121908  0.535153 -0.588085 -1.929461 -0.659900  0.754921   \n",
       "\n",
       "            7         8         9  \n",
       "0   -0.067192  0.175471 -1.049646  \n",
       "1    1.006633 -0.833692  0.957744  \n",
       "2   -0.218981  0.878643 -1.257740  \n",
       "3    1.985111  1.379029  1.041768  \n",
       "4    0.184645 -1.567739 -0.142107  \n",
       "..        ...       ...       ...  \n",
       "995  0.123689  0.927871 -0.225003  \n",
       "996 -0.216876 -1.752938 -0.810152  \n",
       "997  2.043336  0.947388  0.790354  \n",
       "998  0.378898 -1.971894 -0.252250  \n",
       "999 -1.941357  0.589002 -1.007482  \n",
       "\n",
       "[1000 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 1, 0, 0, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test  = train_test_split(X,y, test_size=0.30,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implement logestic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = LogisticRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "       0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = log.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.33104089e-01, 6.68959106e-02],\n",
       "       [6.59976832e-01, 3.40023168e-01],\n",
       "       [8.65439561e-01, 1.34560439e-01],\n",
       "       [9.59081747e-02, 9.04091825e-01],\n",
       "       [2.43108150e-01, 7.56891850e-01],\n",
       "       [5.76396931e-01, 4.23603069e-01],\n",
       "       [8.25188888e-01, 1.74811112e-01],\n",
       "       [2.29999952e-04, 9.99770000e-01],\n",
       "       [1.25363187e-02, 9.87463681e-01],\n",
       "       [2.73346597e-03, 9.97266534e-01],\n",
       "       [7.06227890e-03, 9.92937721e-01],\n",
       "       [5.68134296e-01, 4.31865704e-01],\n",
       "       [8.38796778e-02, 9.16120322e-01],\n",
       "       [9.73835584e-01, 2.61644162e-02],\n",
       "       [1.27501162e-02, 9.87249884e-01],\n",
       "       [8.06120072e-01, 1.93879928e-01],\n",
       "       [9.95125747e-01, 4.87425270e-03],\n",
       "       [1.78699988e-02, 9.82130001e-01],\n",
       "       [9.78717701e-01, 2.12822989e-02],\n",
       "       [3.90303620e-02, 9.60969638e-01],\n",
       "       [3.39139480e-02, 9.66086052e-01],\n",
       "       [3.13743686e-02, 9.68625631e-01],\n",
       "       [5.00274883e-02, 9.49972512e-01],\n",
       "       [8.18573651e-01, 1.81426349e-01],\n",
       "       [8.59038470e-01, 1.40961530e-01],\n",
       "       [9.12460521e-01, 8.75394788e-02],\n",
       "       [8.53285450e-01, 1.46714550e-01],\n",
       "       [8.63538338e-01, 1.36461662e-01],\n",
       "       [7.14069794e-01, 2.85930206e-01],\n",
       "       [1.50110810e-01, 8.49889190e-01],\n",
       "       [4.25748098e-04, 9.99574252e-01],\n",
       "       [6.18630454e-01, 3.81369546e-01],\n",
       "       [2.02970365e-02, 9.79702963e-01],\n",
       "       [4.31674190e-01, 5.68325810e-01],\n",
       "       [1.32602897e-04, 9.99867397e-01],\n",
       "       [9.79277212e-01, 2.07227884e-02],\n",
       "       [7.91505067e-05, 9.99920849e-01],\n",
       "       [2.79028629e-01, 7.20971371e-01],\n",
       "       [7.50191140e-01, 2.49808860e-01],\n",
       "       [9.78914019e-01, 2.10859811e-02],\n",
       "       [9.29298742e-01, 7.07012581e-02],\n",
       "       [5.07627084e-01, 4.92372916e-01],\n",
       "       [7.86334536e-01, 2.13665464e-01],\n",
       "       [1.02170326e-02, 9.89782967e-01],\n",
       "       [6.71001381e-01, 3.28998619e-01],\n",
       "       [2.28316835e-02, 9.77168316e-01],\n",
       "       [9.56906175e-04, 9.99043094e-01],\n",
       "       [2.80866655e-04, 9.99719133e-01],\n",
       "       [2.35383593e-04, 9.99764616e-01],\n",
       "       [5.95639599e-01, 4.04360401e-01],\n",
       "       [2.24986821e-01, 7.75013179e-01],\n",
       "       [1.87963545e-06, 9.99998120e-01],\n",
       "       [3.76191758e-03, 9.96238082e-01],\n",
       "       [1.47229167e-03, 9.98527708e-01],\n",
       "       [2.71064330e-02, 9.72893567e-01],\n",
       "       [1.06485998e-02, 9.89351400e-01],\n",
       "       [4.95368676e-01, 5.04631324e-01],\n",
       "       [9.70585084e-01, 2.94149162e-02],\n",
       "       [4.88523232e-02, 9.51147677e-01],\n",
       "       [5.89167160e-01, 4.10832840e-01],\n",
       "       [3.61611389e-01, 6.38388611e-01],\n",
       "       [9.39498551e-01, 6.05014489e-02],\n",
       "       [9.84757455e-01, 1.52425455e-02],\n",
       "       [6.89700912e-01, 3.10299088e-01],\n",
       "       [1.37200997e-03, 9.98627990e-01],\n",
       "       [1.09949522e-05, 9.99989005e-01],\n",
       "       [7.17407766e-01, 2.82592234e-01],\n",
       "       [9.33068344e-01, 6.69316556e-02],\n",
       "       [1.27246555e-02, 9.87275344e-01],\n",
       "       [9.95912281e-01, 4.08771888e-03],\n",
       "       [9.45014768e-01, 5.49852320e-02],\n",
       "       [9.72505405e-01, 2.74945952e-02],\n",
       "       [1.57767118e-02, 9.84223288e-01],\n",
       "       [7.02089863e-01, 2.97910137e-01],\n",
       "       [3.78336641e-03, 9.96216634e-01],\n",
       "       [7.88166769e-01, 2.11833231e-01],\n",
       "       [9.76950585e-03, 9.90230494e-01],\n",
       "       [9.08319915e-01, 9.16800853e-02],\n",
       "       [9.97668337e-01, 2.33166339e-03],\n",
       "       [9.78676555e-01, 2.13234453e-02],\n",
       "       [9.97859196e-01, 2.14080396e-03],\n",
       "       [9.62738467e-01, 3.72615329e-02],\n",
       "       [6.87123257e-01, 3.12876743e-01],\n",
       "       [9.65470028e-01, 3.45299723e-02],\n",
       "       [4.09424875e-03, 9.95905751e-01],\n",
       "       [6.80206407e-01, 3.19793593e-01],\n",
       "       [2.00265911e-02, 9.79973409e-01],\n",
       "       [9.70587240e-01, 2.94127604e-02],\n",
       "       [7.63313709e-01, 2.36686291e-01],\n",
       "       [5.93516932e-05, 9.99940648e-01],\n",
       "       [2.71967499e-03, 9.97280325e-01],\n",
       "       [1.16719400e-02, 9.88328060e-01],\n",
       "       [9.76638433e-01, 2.33615675e-02],\n",
       "       [1.90773686e-04, 9.99809226e-01],\n",
       "       [2.75472277e-01, 7.24527723e-01],\n",
       "       [2.66885863e-01, 7.33114137e-01],\n",
       "       [6.31794396e-01, 3.68205604e-01],\n",
       "       [2.23108158e-01, 7.76891842e-01],\n",
       "       [1.49891897e-04, 9.99850108e-01],\n",
       "       [5.41255917e-03, 9.94587441e-01],\n",
       "       [1.99221339e-01, 8.00778661e-01],\n",
       "       [8.53091208e-01, 1.46908792e-01],\n",
       "       [9.63745661e-01, 3.62543392e-02],\n",
       "       [9.68858604e-01, 3.11413963e-02],\n",
       "       [9.62889671e-01, 3.71103286e-02],\n",
       "       [9.90852701e-01, 9.14729857e-03],\n",
       "       [1.83724160e-02, 9.81627584e-01],\n",
       "       [9.52934925e-01, 4.70650748e-02],\n",
       "       [9.79501310e-01, 2.04986896e-02],\n",
       "       [9.74838735e-01, 2.51612650e-02],\n",
       "       [8.64561278e-04, 9.99135439e-01],\n",
       "       [2.63635565e-04, 9.99736364e-01],\n",
       "       [7.15816922e-01, 2.84183078e-01],\n",
       "       [5.20102473e-01, 4.79897527e-01],\n",
       "       [8.22118465e-01, 1.77881535e-01],\n",
       "       [9.93687723e-01, 6.31227662e-03],\n",
       "       [7.01689887e-01, 2.98310113e-01],\n",
       "       [4.08054578e-03, 9.95919454e-01],\n",
       "       [8.82580348e-01, 1.17419652e-01],\n",
       "       [9.18167868e-01, 8.18321316e-02],\n",
       "       [2.04191995e-01, 7.95808005e-01],\n",
       "       [8.30915134e-01, 1.69084866e-01],\n",
       "       [9.50759759e-01, 4.92402408e-02],\n",
       "       [9.95188524e-01, 4.81147615e-03],\n",
       "       [9.98081563e-01, 1.91843692e-03],\n",
       "       [9.94720644e-01, 5.27935599e-03],\n",
       "       [7.86910251e-01, 2.13089749e-01],\n",
       "       [9.59328264e-01, 4.06717360e-02],\n",
       "       [9.60584999e-01, 3.94150010e-02],\n",
       "       [2.78162831e-02, 9.72183717e-01],\n",
       "       [4.83504710e-01, 5.16495290e-01],\n",
       "       [9.76313056e-01, 2.36869436e-02],\n",
       "       [5.29536484e-03, 9.94704635e-01],\n",
       "       [8.95190218e-01, 1.04809782e-01],\n",
       "       [9.78725579e-01, 2.12744207e-02],\n",
       "       [5.70424328e-04, 9.99429576e-01],\n",
       "       [6.06597095e-04, 9.99393403e-01],\n",
       "       [9.97063519e-01, 2.93648136e-03],\n",
       "       [9.96770635e-01, 3.22936499e-03],\n",
       "       [2.80932364e-04, 9.99719068e-01],\n",
       "       [4.36681854e-02, 9.56331815e-01],\n",
       "       [3.67577620e-02, 9.63242238e-01],\n",
       "       [7.64929290e-02, 9.23507071e-01],\n",
       "       [9.99111854e-01, 8.88145827e-04],\n",
       "       [9.21724541e-01, 7.82754591e-02],\n",
       "       [8.47470908e-01, 1.52529092e-01],\n",
       "       [3.52211622e-03, 9.96477884e-01],\n",
       "       [1.99963628e-02, 9.80003637e-01],\n",
       "       [8.87815624e-01, 1.12184376e-01],\n",
       "       [2.04900489e-04, 9.99795100e-01],\n",
       "       [9.69272062e-01, 3.07279383e-02],\n",
       "       [5.93415564e-01, 4.06584436e-01],\n",
       "       [4.12434381e-01, 5.87565619e-01],\n",
       "       [9.62232914e-01, 3.77670862e-02],\n",
       "       [9.06571259e-01, 9.34287406e-02],\n",
       "       [9.68115138e-01, 3.18848620e-02],\n",
       "       [5.57913049e-02, 9.44208695e-01],\n",
       "       [1.71187994e-04, 9.99828812e-01],\n",
       "       [9.59561695e-01, 4.04383046e-02],\n",
       "       [4.27143830e-02, 9.57285617e-01],\n",
       "       [7.42046755e-01, 2.57953245e-01],\n",
       "       [9.97281089e-01, 2.71891147e-03],\n",
       "       [7.56358109e-01, 2.43641891e-01],\n",
       "       [1.43801088e-02, 9.85619891e-01],\n",
       "       [5.35312556e-01, 4.64687444e-01],\n",
       "       [4.45378258e-03, 9.95546217e-01],\n",
       "       [1.60029587e-01, 8.39970413e-01],\n",
       "       [9.38429814e-01, 6.15701856e-02],\n",
       "       [9.30104103e-01, 6.98958975e-02],\n",
       "       [7.97155690e-02, 9.20284431e-01],\n",
       "       [9.80799507e-01, 1.92004933e-02],\n",
       "       [3.40113855e-03, 9.96598861e-01],\n",
       "       [3.85767933e-03, 9.96142321e-01],\n",
       "       [9.23872609e-01, 7.61273909e-02],\n",
       "       [2.48244000e-03, 9.97517560e-01],\n",
       "       [1.42590198e-01, 8.57409802e-01],\n",
       "       [7.71857139e-01, 2.28142861e-01],\n",
       "       [7.15361824e-01, 2.84638176e-01],\n",
       "       [7.95802216e-03, 9.92041978e-01],\n",
       "       [2.90672971e-03, 9.97093270e-01],\n",
       "       [9.62804022e-01, 3.71959777e-02],\n",
       "       [3.95173272e-06, 9.99996048e-01],\n",
       "       [7.46521232e-01, 2.53478768e-01],\n",
       "       [2.68377139e-01, 7.31622861e-01],\n",
       "       [9.08001425e-01, 9.19985751e-02],\n",
       "       [6.29221189e-01, 3.70778811e-01],\n",
       "       [1.73092334e-04, 9.99826908e-01],\n",
       "       [2.88878674e-01, 7.11121326e-01],\n",
       "       [8.70348991e-01, 1.29651009e-01],\n",
       "       [4.65501083e-02, 9.53449892e-01],\n",
       "       [2.69925625e-04, 9.99730074e-01],\n",
       "       [9.18821208e-03, 9.90811788e-01],\n",
       "       [1.84058415e-05, 9.99981594e-01],\n",
       "       [9.21585305e-01, 7.84146951e-02],\n",
       "       [1.66148452e-01, 8.33851548e-01],\n",
       "       [4.28792808e-02, 9.57120719e-01],\n",
       "       [3.24218883e-01, 6.75781117e-01],\n",
       "       [2.88812742e-03, 9.97111873e-01],\n",
       "       [2.34274548e-03, 9.97657255e-01],\n",
       "       [5.66510205e-01, 4.33489795e-01],\n",
       "       [8.64460483e-02, 9.13553952e-01],\n",
       "       [1.63989586e-01, 8.36010414e-01],\n",
       "       [9.73141580e-01, 2.68584199e-02],\n",
       "       [1.48492890e-03, 9.98515071e-01],\n",
       "       [9.76142249e-01, 2.38577513e-02],\n",
       "       [9.20183372e-01, 7.98166284e-02],\n",
       "       [1.27799670e-04, 9.99872200e-01],\n",
       "       [8.14616098e-01, 1.85383902e-01],\n",
       "       [6.57619511e-02, 9.34238049e-01],\n",
       "       [8.05506424e-01, 1.94493576e-01],\n",
       "       [3.16841231e-02, 9.68315877e-01],\n",
       "       [3.85311660e-02, 9.61468834e-01],\n",
       "       [9.69954697e-01, 3.00453026e-02],\n",
       "       [6.65986768e-03, 9.93340132e-01],\n",
       "       [6.58151073e-01, 3.41848927e-01],\n",
       "       [4.54368859e-01, 5.45631141e-01],\n",
       "       [9.73084127e-01, 2.69158732e-02],\n",
       "       [9.90303001e-01, 9.69699858e-03],\n",
       "       [8.65783205e-01, 1.34216795e-01],\n",
       "       [7.15540687e-01, 2.84459313e-01],\n",
       "       [6.21203632e-01, 3.78796368e-01],\n",
       "       [9.14608618e-01, 8.53913819e-02],\n",
       "       [7.63263535e-01, 2.36736465e-01],\n",
       "       [1.27328969e-03, 9.98726710e-01],\n",
       "       [9.56775002e-01, 4.32249985e-02],\n",
       "       [8.81796794e-01, 1.18203206e-01],\n",
       "       [9.54741311e-01, 4.52586890e-02],\n",
       "       [9.37675309e-01, 6.23246915e-02],\n",
       "       [9.04350122e-01, 9.56498784e-02],\n",
       "       [4.36825205e-02, 9.56317480e-01],\n",
       "       [9.37566377e-01, 6.24336232e-02],\n",
       "       [2.67935975e-01, 7.32064025e-01],\n",
       "       [9.88332119e-01, 1.16678810e-02],\n",
       "       [9.37357607e-01, 6.26423933e-02],\n",
       "       [1.98870091e-01, 8.01129909e-01],\n",
       "       [7.53193594e-01, 2.46806406e-01],\n",
       "       [5.64637682e-05, 9.99943536e-01],\n",
       "       [2.35669383e-03, 9.97643306e-01],\n",
       "       [9.63223386e-01, 3.67766138e-02],\n",
       "       [9.91344273e-01, 8.65572690e-03],\n",
       "       [2.79414842e-02, 9.72058516e-01],\n",
       "       [1.73663772e-02, 9.82633623e-01],\n",
       "       [1.97654982e-05, 9.99980235e-01],\n",
       "       [9.97873229e-01, 2.12677091e-03],\n",
       "       [5.38568985e-02, 9.46143101e-01],\n",
       "       [1.14208934e-02, 9.88579107e-01],\n",
       "       [9.96664447e-01, 3.33555343e-03],\n",
       "       [9.34325785e-01, 6.56742147e-02],\n",
       "       [8.33456753e-05, 9.99916654e-01],\n",
       "       [4.38586824e-02, 9.56141318e-01],\n",
       "       [8.78527383e-01, 1.21472617e-01],\n",
       "       [1.02262699e-01, 8.97737301e-01],\n",
       "       [1.39702455e-01, 8.60297545e-01],\n",
       "       [7.38438745e-01, 2.61561255e-01],\n",
       "       [9.96218361e-01, 3.78163937e-03],\n",
       "       [2.96903603e-01, 7.03096397e-01],\n",
       "       [8.92354714e-01, 1.07645286e-01],\n",
       "       [9.84488536e-01, 1.55114638e-02],\n",
       "       [1.16290641e-04, 9.99883709e-01],\n",
       "       [9.80018027e-01, 1.99819730e-02],\n",
       "       [8.03911966e-01, 1.96088034e-01],\n",
       "       [1.38054725e-03, 9.98619453e-01],\n",
       "       [7.84488886e-01, 2.15511114e-01],\n",
       "       [1.03611401e-03, 9.98963886e-01],\n",
       "       [7.02865763e-01, 2.97134237e-01],\n",
       "       [6.23112023e-03, 9.93768880e-01],\n",
       "       [8.62299841e-01, 1.37700159e-01],\n",
       "       [9.53531098e-01, 4.64689022e-02],\n",
       "       [2.24165759e-01, 7.75834241e-01],\n",
       "       [2.89736651e-01, 7.10263349e-01],\n",
       "       [7.70379972e-01, 2.29620028e-01],\n",
       "       [8.96941856e-01, 1.03058144e-01],\n",
       "       [1.01542270e-02, 9.89845773e-01],\n",
       "       [3.29284863e-01, 6.70715137e-01],\n",
       "       [9.79115114e-01, 2.08848863e-02],\n",
       "       [3.21286101e-04, 9.99678714e-01],\n",
       "       [9.85080267e-02, 9.01491973e-01],\n",
       "       [6.73824799e-02, 9.32617520e-01],\n",
       "       [9.95957151e-01, 4.04284935e-03],\n",
       "       [1.46535483e-02, 9.85346452e-01],\n",
       "       [2.73619909e-05, 9.99972638e-01],\n",
       "       [9.70016883e-01, 2.99831167e-02],\n",
       "       [8.50873664e-01, 1.49126336e-01],\n",
       "       [6.52726598e-01, 3.47273402e-01],\n",
       "       [3.71328572e-01, 6.28671428e-01],\n",
       "       [9.57376655e-01, 4.26233447e-02],\n",
       "       [8.32142898e-01, 1.67857102e-01],\n",
       "       [8.56322585e-01, 1.43677415e-01],\n",
       "       [5.75653149e-02, 9.42434685e-01],\n",
       "       [8.93439186e-01, 1.06560814e-01],\n",
       "       [8.56747667e-01, 1.43252333e-01],\n",
       "       [8.86252236e-03, 9.91137478e-01],\n",
       "       [7.59051924e-01, 2.40948076e-01],\n",
       "       [2.86240052e-01, 7.13759948e-01],\n",
       "       [9.96145674e-01, 3.85432624e-03],\n",
       "       [6.95980974e-03, 9.93040190e-01],\n",
       "       [9.79049347e-01, 2.09506530e-02],\n",
       "       [4.46258951e-03, 9.95537410e-01],\n",
       "       [8.80047667e-01, 1.19952333e-01],\n",
       "       [9.71816536e-01, 2.81834640e-02]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.predict_proba(X_test) # for probaBILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 0.9166666666666666\n",
      "confusion [[146  11]\n",
      " [ 14 129]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92       157\n",
      "           1       0.92      0.90      0.91       143\n",
      "\n",
      "    accuracy                           0.92       300\n",
      "   macro avg       0.92      0.92      0.92       300\n",
      "weighted avg       0.92      0.92      0.92       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score =accuracy_score (y_test,y_pred)\n",
    "print(\"score\", score)\n",
    "confusion = confusion_matrix(y_test,y_pred)\n",
    "print(\"confusion\", confusion)\n",
    "\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning And Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "penality =['l1','l2','elasticnet']\n",
    "c_values = [100,10,1.0,0.1,0.01]\n",
    "solver=['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to put all the above values in the key value pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'penalty': ['l1', 'l2', 'elasticnet'],\n",
       " 'C': [100, 10, 1.0, 0.1, 0.01],\n",
       " 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params=dict(penalty=penality,C=c_values,solver=solver)\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search  CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "cv=StratifiedKFold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = GridSearchCV(estimator=model,param_grid=params,scoring='accuracy',cv=cv,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={&#x27;C&#x27;: [100, 10, 1.0, 0.1, 0.01],\n",
       "                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;elasticnet&#x27;],\n",
       "                         &#x27;solver&#x27;: [&#x27;newton-cg&#x27;, &#x27;lbfgs&#x27;, &#x27;liblinear&#x27;, &#x27;sag&#x27;,\n",
       "                                    &#x27;saga&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={&#x27;C&#x27;: [100, 10, 1.0, 0.1, 0.01],\n",
       "                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;elasticnet&#x27;],\n",
       "                         &#x27;solver&#x27;: [&#x27;newton-cg&#x27;, &#x27;lbfgs&#x27;, &#x27;liblinear&#x27;, &#x27;sag&#x27;,\n",
       "                                    &#x27;saga&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={'C': [100, 10, 1.0, 0.1, 0.01],\n",
       "                         'penalty': ['l1', 'l2', 'elasticnet'],\n",
       "                         'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag',\n",
       "                                    'saga']},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "200 fits failed out of a total of 375.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "                   ^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "                         ~~^~~~~~~~~~\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.91285714        nan 0.91285714 0.91285714\n",
      " 0.91285714 0.91285714 0.91285714 0.91285714        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.91285714\n",
      "        nan 0.91285714 0.91285714 0.91285714 0.91285714 0.91285714\n",
      " 0.91285714        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.91142857        nan 0.91142857 0.91142857\n",
      " 0.91142857 0.91142857 0.91142857 0.91142857        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.92285714\n",
      "        nan 0.92142857 0.91285714 0.91285714 0.91857143 0.91285714\n",
      " 0.91285714        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.92142857        nan 0.92428571 0.91285714\n",
      " 0.91285714 0.92285714 0.91285714 0.91285714        nan        nan\n",
      "        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={&#x27;C&#x27;: [100, 10, 1.0, 0.1, 0.01],\n",
       "                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;elasticnet&#x27;],\n",
       "                         &#x27;solver&#x27;: [&#x27;newton-cg&#x27;, &#x27;lbfgs&#x27;, &#x27;liblinear&#x27;, &#x27;sag&#x27;,\n",
       "                                    &#x27;saga&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={&#x27;C&#x27;: [100, 10, 1.0, 0.1, 0.01],\n",
       "                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;elasticnet&#x27;],\n",
       "                         &#x27;solver&#x27;: [&#x27;newton-cg&#x27;, &#x27;lbfgs&#x27;, &#x27;liblinear&#x27;, &#x27;sag&#x27;,\n",
       "                                    &#x27;saga&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={'C': [100, 10, 1.0, 0.1, 0.01],\n",
       "                         'penalty': ['l1', 'l2', 'elasticnet'],\n",
       "                         'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag',\n",
       "                                    'saga']},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.01, 'penalty': 'l1', 'solver': 'saga'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9242857142857142"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "       0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = grid.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.93       165\n",
      "           1       0.89      0.94      0.91       135\n",
      "\n",
      "    accuracy                           0.92       300\n",
      "   macro avg       0.92      0.92      0.92       300\n",
      "weighted avg       0.92      0.92      0.92       300\n",
      "\n",
      "[[149  16]\n",
      " [  8 127]]\n"
     ]
    }
   ],
   "source": [
    "score = accuracy_score(y_pred, y_test)\n",
    "print(score)\n",
    "print(classification_report(y_pred,y_test))\n",
    "print(confusion_matrix(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomized Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "randomcv = RandomizedSearchCV(estimator=model, param_distributions=params, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "30 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "                   ^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "                         ~~^~~~~~~~~~\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan 0.91142857 0.91142857 0.91285714\n",
      "        nan 0.91285714        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5, estimator=LogisticRegression(),\n",
       "                   param_distributions={&#x27;C&#x27;: [100, 10, 1.0, 0.1, 0.01],\n",
       "                                        &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;elasticnet&#x27;],\n",
       "                                        &#x27;solver&#x27;: [&#x27;newton-cg&#x27;, &#x27;lbfgs&#x27;,\n",
       "                                                   &#x27;liblinear&#x27;, &#x27;sag&#x27;,\n",
       "                                                   &#x27;saga&#x27;]},\n",
       "                   scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5, estimator=LogisticRegression(),\n",
       "                   param_distributions={&#x27;C&#x27;: [100, 10, 1.0, 0.1, 0.01],\n",
       "                                        &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;elasticnet&#x27;],\n",
       "                                        &#x27;solver&#x27;: [&#x27;newton-cg&#x27;, &#x27;lbfgs&#x27;,\n",
       "                                                   &#x27;liblinear&#x27;, &#x27;sag&#x27;,\n",
       "                                                   &#x27;saga&#x27;]},\n",
       "                   scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=LogisticRegression(),\n",
       "                   param_distributions={'C': [100, 10, 1.0, 0.1, 0.01],\n",
       "                                        'penalty': ['l1', 'l2', 'elasticnet'],\n",
       "                                        'solver': ['newton-cg', 'lbfgs',\n",
       "                                                   'liblinear', 'sag',\n",
       "                                                   'saga']},\n",
       "                   scoring='accuracy')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomcv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9128571428571428"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomcv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'solver': 'newton-cg', 'penalty': 'l2', 'C': 10}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomcv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "       0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = randomcv.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9166666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       160\n",
      "           1       0.90      0.92      0.91       140\n",
      "\n",
      "    accuracy                           0.92       300\n",
      "   macro avg       0.92      0.92      0.92       300\n",
      "weighted avg       0.92      0.92      0.92       300\n",
      "\n",
      "[[146  14]\n",
      " [ 11 129]]\n"
     ]
    }
   ],
   "source": [
    "score = accuracy_score(y_pred,y_test)\n",
    "print(score)\n",
    "print(classification_report(y_pred, y_test))\n",
    "print(confusion_matrix(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logestic regrssion fo multiclass classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=3, n_classes=3, random_state=42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 1, 1, 2, 0, 1, 2, 0, 1, 2, 0, 0, 0, 2, 0, 0, 1, 1, 2, 1,\n",
       "       0, 1, 1, 1, 0, 2, 1, 1, 0, 0, 1, 0, 1, 0, 2, 2, 0, 2, 0, 0, 0, 1,\n",
       "       2, 2, 1, 2, 1, 2, 0, 0, 1, 2, 2, 1, 0, 2, 2, 1, 0, 0, 1, 2, 2, 0,\n",
       "       2, 2, 1, 2, 2, 2, 1, 2, 1, 0, 1, 1, 2, 1, 1, 0, 1, 2, 2, 1, 2, 1,\n",
       "       1, 2, 2, 2, 2, 2, 2, 0, 0, 2, 1, 1, 1, 0, 1, 1, 1, 2, 2, 0, 0, 0,\n",
       "       0, 0, 1, 2, 0, 0, 0, 1, 0, 1, 1, 2, 1, 0, 2, 1, 0, 2, 1, 0, 1, 0,\n",
       "       1, 2, 2, 0, 0, 1, 2, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 2, 2, 1, 1, 2,\n",
       "       0, 0, 1, 0, 0, 2, 2, 2, 0, 0, 2, 1, 2, 1, 1, 1, 2, 2, 0, 0, 2, 0,\n",
       "       1, 0, 1, 2, 2, 2, 2, 1, 0, 0, 2, 2, 0, 1, 0, 1, 0, 1, 2, 1, 2, 0,\n",
       "       0, 2, 2, 1, 1, 0, 1, 0, 1, 0, 1, 2, 1, 1, 0, 2, 0, 2, 1, 0, 2, 2,\n",
       "       2, 0, 0, 1, 1, 2, 1, 1, 1, 0, 0, 2, 0, 1, 2, 1, 1, 1, 1, 0, 0, 2,\n",
       "       2, 0, 2, 0, 0, 2, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 2, 1, 1, 1, 1, 1,\n",
       "       2, 2, 0, 2, 2, 0, 1, 0, 2, 0, 1, 2, 2, 1, 1, 0, 1, 2, 2, 0, 2, 0,\n",
       "       2, 0, 0, 1, 1, 0, 1, 1, 1, 2, 0, 0, 0, 0, 1, 2, 2, 2, 0, 2, 1, 1,\n",
       "       2, 0, 1, 1, 0, 1, 2, 1, 0, 2, 1, 1, 0, 1, 2, 0, 2, 1, 2, 0, 2, 2,\n",
       "       1, 1, 1, 0, 2, 0, 2, 0, 0, 0, 1, 0, 2, 1, 2, 1, 1, 1, 0, 1, 2, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 2, 2, 2, 2, 0, 1, 0, 1, 0, 1, 2, 0,\n",
       "       2, 2, 2, 2, 1, 1, 0, 0, 2, 2, 0, 0, 2, 0, 2, 1, 2, 1, 0, 1, 2, 1,\n",
       "       1, 1, 1, 0, 0, 0, 1, 2, 1, 1, 2, 0, 0, 1, 1, 2, 2, 0, 2, 0, 2, 2,\n",
       "       0, 0, 1, 1, 1, 1, 2, 1, 1, 1, 2, 0, 1, 1, 2, 1, 2, 0, 0, 2, 1, 1,\n",
       "       1, 2, 2, 1, 2, 2, 0, 0, 0, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 0, 0, 1,\n",
       "       1, 2, 0, 0, 2, 2, 0, 0, 1, 0, 0, 1, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0,\n",
       "       0, 2, 2, 1, 2, 1, 1, 0, 2, 2, 2, 1, 2, 1, 1, 0, 0, 2, 2, 2, 2, 2,\n",
       "       2, 0, 1, 1, 1, 1, 0, 1, 2, 1, 1, 0, 0, 2, 1, 1, 1, 0, 2, 2, 2, 2,\n",
       "       2, 0, 1, 0, 0, 0, 1, 0, 2, 0, 1, 2, 0, 2, 0, 1, 1, 2, 0, 2, 0, 0,\n",
       "       0, 2, 1, 0, 1, 0, 2, 2, 1, 0, 1, 0, 0, 2, 2, 2, 1, 0, 1, 1, 0, 1,\n",
       "       2, 2, 0, 2, 2, 2, 0, 0, 1, 1, 2, 1, 0, 2, 2, 1, 1, 1, 1, 0, 1, 0,\n",
       "       2, 2, 1, 1, 2, 0, 1, 1, 1, 2, 2, 2, 0, 1, 2, 2, 1, 1, 1, 2, 1, 2,\n",
       "       2, 0, 2, 0, 1, 2, 0, 1, 1, 0, 2, 0, 0, 1, 1, 0, 0, 0, 0, 2, 1, 0,\n",
       "       0, 1, 2, 0, 0, 1, 2, 2, 2, 1, 0, 2, 2, 1, 2, 0, 1, 1, 0, 2, 2, 1,\n",
       "       0, 2, 2, 1, 2, 0, 2, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 0, 0, 1, 2, 1,\n",
       "       0, 0, 0, 0, 2, 2, 2, 0, 0, 2, 1, 2, 0, 1, 0, 1, 1, 0, 2, 0, 2, 1,\n",
       "       0, 0, 0, 0, 2, 1, 2, 0, 1, 0, 0, 2, 1, 2, 1, 0, 1, 2, 1, 1, 1, 0,\n",
       "       2, 0, 1, 2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 1, 1, 0, 1, 2, 2, 0, 1, 1,\n",
       "       1, 0, 0, 0, 1, 2, 0, 2, 1, 0, 0, 2, 0, 2, 0, 1, 1, 2, 0, 1, 0, 0,\n",
       "       1, 1, 2, 1, 0, 2, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 2, 2, 2, 1, 1,\n",
       "       1, 2, 2, 1, 1, 0, 0, 0, 2, 0, 1, 1, 1, 2, 0, 1, 2, 0, 2, 0, 2, 2,\n",
       "       1, 1, 0, 1, 1, 0, 1, 2, 0, 2, 2, 0, 1, 2, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "       2, 2, 2, 0, 1, 1, 2, 2, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0, 1, 1, 2, 0,\n",
       "       1, 1, 2, 2, 1, 0, 1, 2, 0, 1, 2, 2, 2, 0, 1, 1, 2, 2, 0, 0, 0, 0,\n",
       "       0, 2, 2, 0, 2, 1, 1, 1, 0, 2, 1, 2, 2, 0, 2, 0, 0, 2, 2, 0, 1, 2,\n",
       "       0, 0, 0, 2, 0, 1, 0, 1, 1, 0, 2, 1, 2, 0, 0, 2, 2, 0, 2, 1, 2, 2,\n",
       "       0, 2, 2, 1, 2, 2, 1, 0, 0, 0, 0, 2, 0, 1, 2, 0, 1, 0, 2, 1, 2, 1,\n",
       "       0, 2, 0, 1, 0, 0, 2, 1, 2, 2, 2, 2, 0, 2, 2, 0, 2, 1, 0, 2, 0, 0,\n",
       "       2, 1, 1, 1, 0, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 0, 2, 1, 1, 2, 1, 0,\n",
       "       0, 1, 0, 1, 1, 2, 0, 2, 1, 2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.10,random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logestic = LogisticRegression()\n",
    "logestic.fit(X_train, y_train)\n",
    "y_pred = logestic.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59\n"
     ]
    }
   ],
   "source": [
    "score = accuracy_score(y_pred, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.62      0.64        37\n",
      "           1       0.34      0.38      0.36        29\n",
      "           2       0.76      0.74      0.75        34\n",
      "\n",
      "    accuracy                           0.59       100\n",
      "   macro avg       0.59      0.58      0.58       100\n",
      "weighted avg       0.60      0.59      0.59       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23 13  1]\n",
      " [11 11  7]\n",
      " [ 1  8 25]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logestic Regression for imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate and plot a synthetic imbalanced classification dataset\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imbalanced dataset\n",
    "X, y = make_classification(n_samples=1000,n_features=2,n_clusters_per_class=1, n_redundant=0, weights=[0.99], random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.45122049, -1.08670474],\n",
       "       [ 2.08029047, -0.97808443],\n",
       "       [ 1.91805213, -1.0431487 ],\n",
       "       ...,\n",
       "       [ 0.83675119, -0.54161851],\n",
       "       [ 0.45782986, -1.05177133],\n",
       "       [ 0.27891721, -1.16309231]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 985, 1: 15})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.45122049, -1.08670474],\n",
       "       [ 2.08029047, -0.97808443],\n",
       "       [ 1.91805213, -1.0431487 ],\n",
       "       ...,\n",
       "       [ 0.83675119, -0.54161851],\n",
       "       [ 0.45782986, -1.05177133],\n",
       "       [ 0.27891721, -1.16309231]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight=[{0:w,1:y} for w in [1,10,50,100] for y in [1,10,50,100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0: 1, 1: 1},\n",
       " {0: 1, 1: 10},\n",
       " {0: 1, 1: 50},\n",
       " {0: 1, 1: 100},\n",
       " {0: 10, 1: 1},\n",
       " {0: 10, 1: 10},\n",
       " {0: 10, 1: 50},\n",
       " {0: 10, 1: 100},\n",
       " {0: 50, 1: 1},\n",
       " {0: 50, 1: 10},\n",
       " {0: 50, 1: 50},\n",
       " {0: 50, 1: 100},\n",
       " {0: 100, 1: 1},\n",
       " {0: 100, 1: 10},\n",
       " {0: 100, 1: 50},\n",
       " {0: 100, 1: 100}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'penalty': ['l2'],\n",
       "  'C': [100, 10, 1.0, 0.1, 0.01],\n",
       "  'solver': ['newton-cg', 'lbfgs', 'sag'],\n",
       "  'class_weight': [{0: 1, 1: 1},\n",
       "   {0: 1, 1: 10},\n",
       "   {0: 1, 1: 50},\n",
       "   {0: 1, 1: 100},\n",
       "   {0: 10, 1: 1},\n",
       "   {0: 10, 1: 10},\n",
       "   {0: 10, 1: 50},\n",
       "   {0: 10, 1: 100},\n",
       "   {0: 50, 1: 1},\n",
       "   {0: 50, 1: 10},\n",
       "   {0: 50, 1: 50},\n",
       "   {0: 50, 1: 100},\n",
       "   {0: 100, 1: 1},\n",
       "   {0: 100, 1: 10},\n",
       "   {0: 100, 1: 50},\n",
       "   {0: 100, 1: 100}]},\n",
       " {'penalty': ['l1'],\n",
       "  'C': [100, 10, 1.0, 0.1, 0.01],\n",
       "  'solver': ['liblinear'],\n",
       "  'class_weight': [{0: 1, 1: 1},\n",
       "   {0: 1, 1: 10},\n",
       "   {0: 1, 1: 50},\n",
       "   {0: 1, 1: 100},\n",
       "   {0: 10, 1: 1},\n",
       "   {0: 10, 1: 10},\n",
       "   {0: 10, 1: 50},\n",
       "   {0: 10, 1: 100},\n",
       "   {0: 50, 1: 1},\n",
       "   {0: 50, 1: 10},\n",
       "   {0: 50, 1: 50},\n",
       "   {0: 50, 1: 100},\n",
       "   {0: 100, 1: 1},\n",
       "   {0: 100, 1: 10},\n",
       "   {0: 100, 1: 50},\n",
       "   {0: 100, 1: 100}]},\n",
       " {'penalty': ['elasticnet'],\n",
       "  'C': [100, 10, 1.0, 0.1, 0.01],\n",
       "  'solver': ['saga'],\n",
       "  'class_weight': [{0: 1, 1: 1},\n",
       "   {0: 1, 1: 10},\n",
       "   {0: 1, 1: 50},\n",
       "   {0: 1, 1: 100},\n",
       "   {0: 10, 1: 1},\n",
       "   {0: 10, 1: 10},\n",
       "   {0: 10, 1: 50},\n",
       "   {0: 10, 1: 100},\n",
       "   {0: 50, 1: 1},\n",
       "   {0: 50, 1: 10},\n",
       "   {0: 50, 1: 50},\n",
       "   {0: 50, 1: 100},\n",
       "   {0: 100, 1: 1},\n",
       "   {0: 100, 1: 10},\n",
       "   {0: 100, 1: 50},\n",
       "   {0: 100, 1: 100}],\n",
       "  'l1_ratio': [0.5]}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correct the typo and ensure compatible combinations\n",
    "penalty = ['l1', 'l2', 'elasticnet']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "solver = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "class_weight = [{0: w, 1: y} for w in [1, 10, 50, 100] for y in [1, 10, 50, 100]]\n",
    "\n",
    "# Define the parameter grid with compatible combinations\n",
    "params = [\n",
    "    {'penalty': ['l2'], 'C': c_values, 'solver': ['newton-cg', 'lbfgs', 'sag'], 'class_weight': class_weight},\n",
    "    {'penalty': ['l1'], 'C': c_values, 'solver': ['liblinear'], 'class_weight': class_weight},\n",
    "    {'penalty': ['elasticnet'], 'C': c_values, 'solver': ['saga'], 'class_weight': class_weight, 'l1_ratio': [0.5]}\n",
    "]\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "cv = StratifiedKFold()\n",
    "grid=GridSearchCV(estimator=model, param_grid=params, scoring='accuracy',cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=LogisticRegression(),\n",
       "             param_grid=[{&#x27;C&#x27;: [100, 10, 1.0, 0.1, 0.01],\n",
       "                          &#x27;class_weight&#x27;: [{0: 1, 1: 1}, {0: 1, 1: 10},\n",
       "                                           {0: 1, 1: 50}, {0: 1, 1: 100},\n",
       "                                           {0: 10, 1: 1}, {0: 10, 1: 10},\n",
       "                                           {0: 10, 1: 50}, {0: 10, 1: 100},\n",
       "                                           {0: 50, 1: 1}, {0: 50, 1: 10},\n",
       "                                           {0: 50, 1: 50}, {0: 50, 1: 100},\n",
       "                                           {0: 100, 1: 1}, {0: 100, 1: 10},\n",
       "                                           {0: 100, 1: 50}, {0: 100, 1: 1...\n",
       "                          &#x27;penalty&#x27;: [&#x27;l1&#x27;], &#x27;solver&#x27;: [&#x27;liblinear&#x27;]},\n",
       "                         {&#x27;C&#x27;: [100, 10, 1.0, 0.1, 0.01],\n",
       "                          &#x27;class_weight&#x27;: [{0: 1, 1: 1}, {0: 1, 1: 10},\n",
       "                                           {0: 1, 1: 50}, {0: 1, 1: 100},\n",
       "                                           {0: 10, 1: 1}, {0: 10, 1: 10},\n",
       "                                           {0: 10, 1: 50}, {0: 10, 1: 100},\n",
       "                                           {0: 50, 1: 1}, {0: 50, 1: 10},\n",
       "                                           {0: 50, 1: 50}, {0: 50, 1: 100},\n",
       "                                           {0: 100, 1: 1}, {0: 100, 1: 10},\n",
       "                                           {0: 100, 1: 50}, {0: 100, 1: 100}],\n",
       "                          &#x27;l1_ratio&#x27;: [0.5], &#x27;penalty&#x27;: [&#x27;elasticnet&#x27;],\n",
       "                          &#x27;solver&#x27;: [&#x27;saga&#x27;]}],\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=LogisticRegression(),\n",
       "             param_grid=[{&#x27;C&#x27;: [100, 10, 1.0, 0.1, 0.01],\n",
       "                          &#x27;class_weight&#x27;: [{0: 1, 1: 1}, {0: 1, 1: 10},\n",
       "                                           {0: 1, 1: 50}, {0: 1, 1: 100},\n",
       "                                           {0: 10, 1: 1}, {0: 10, 1: 10},\n",
       "                                           {0: 10, 1: 50}, {0: 10, 1: 100},\n",
       "                                           {0: 50, 1: 1}, {0: 50, 1: 10},\n",
       "                                           {0: 50, 1: 50}, {0: 50, 1: 100},\n",
       "                                           {0: 100, 1: 1}, {0: 100, 1: 10},\n",
       "                                           {0: 100, 1: 50}, {0: 100, 1: 1...\n",
       "                          &#x27;penalty&#x27;: [&#x27;l1&#x27;], &#x27;solver&#x27;: [&#x27;liblinear&#x27;]},\n",
       "                         {&#x27;C&#x27;: [100, 10, 1.0, 0.1, 0.01],\n",
       "                          &#x27;class_weight&#x27;: [{0: 1, 1: 1}, {0: 1, 1: 10},\n",
       "                                           {0: 1, 1: 50}, {0: 1, 1: 100},\n",
       "                                           {0: 10, 1: 1}, {0: 10, 1: 10},\n",
       "                                           {0: 10, 1: 50}, {0: 10, 1: 100},\n",
       "                                           {0: 50, 1: 1}, {0: 50, 1: 10},\n",
       "                                           {0: 50, 1: 50}, {0: 50, 1: 100},\n",
       "                                           {0: 100, 1: 1}, {0: 100, 1: 10},\n",
       "                                           {0: 100, 1: 50}, {0: 100, 1: 100}],\n",
       "                          &#x27;l1_ratio&#x27;: [0.5], &#x27;penalty&#x27;: [&#x27;elasticnet&#x27;],\n",
       "                          &#x27;solver&#x27;: [&#x27;saga&#x27;]}],\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=LogisticRegression(),\n",
       "             param_grid=[{'C': [100, 10, 1.0, 0.1, 0.01],\n",
       "                          'class_weight': [{0: 1, 1: 1}, {0: 1, 1: 10},\n",
       "                                           {0: 1, 1: 50}, {0: 1, 1: 100},\n",
       "                                           {0: 10, 1: 1}, {0: 10, 1: 10},\n",
       "                                           {0: 10, 1: 50}, {0: 10, 1: 100},\n",
       "                                           {0: 50, 1: 1}, {0: 50, 1: 10},\n",
       "                                           {0: 50, 1: 50}, {0: 50, 1: 100},\n",
       "                                           {0: 100, 1: 1}, {0: 100, 1: 10},\n",
       "                                           {0: 100, 1: 50}, {0: 100, 1: 1...\n",
       "                          'penalty': ['l1'], 'solver': ['liblinear']},\n",
       "                         {'C': [100, 10, 1.0, 0.1, 0.01],\n",
       "                          'class_weight': [{0: 1, 1: 1}, {0: 1, 1: 10},\n",
       "                                           {0: 1, 1: 50}, {0: 1, 1: 100},\n",
       "                                           {0: 10, 1: 1}, {0: 10, 1: 10},\n",
       "                                           {0: 10, 1: 50}, {0: 10, 1: 100},\n",
       "                                           {0: 50, 1: 1}, {0: 50, 1: 10},\n",
       "                                           {0: 50, 1: 50}, {0: 50, 1: 100},\n",
       "                                           {0: 100, 1: 1}, {0: 100, 1: 10},\n",
       "                                           {0: 100, 1: 50}, {0: 100, 1: 100}],\n",
       "                          'l1_ratio': [0.5], 'penalty': ['elasticnet'],\n",
       "                          'solver': ['saga']}],\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=100, class_weight={0: 1, 1: 1}, solver=&#x27;newton-cg&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" checked><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=100, class_weight={0: 1, 1: 1}, solver=&#x27;newton-cg&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=100, class_weight={0: 1, 1: 1}, solver='newton-cg')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9825000000000002"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.01663489, 0.00519667, 0.00143876, 0.00929074, 0.00462894,\n",
       "        0.00466123, 0.01044159, 0.00461698, 0.00420036, 0.00815816,\n",
       "        0.00463591, 0.00441504, 0.01506872, 0.00655012, 0.00200319,\n",
       "        0.01309872, 0.00543871, 0.0038219 , 0.0103035 , 0.0052103 ,\n",
       "        0.00399904, 0.00969849, 0.00583987, 0.00523224, 0.01863642,\n",
       "        0.00683017, 0.00280919, 0.01227121, 0.00529094, 0.00419745,\n",
       "        0.01184254, 0.00492439, 0.00419121, 0.01217332, 0.00505977,\n",
       "        0.00419855, 0.01551676, 0.00636663, 0.00371895, 0.01462746,\n",
       "        0.00660558, 0.00399461, 0.01310873, 0.00519934, 0.00381179,\n",
       "        0.01303916, 0.00500417, 0.00440626, 0.0090693 , 0.0055316 ,\n",
       "        0.00120001, 0.00836978, 0.00357952, 0.00399857, 0.00714517,\n",
       "        0.00425787, 0.00492463, 0.00746398, 0.00390267, 0.00421886,\n",
       "        0.01277633, 0.00615888, 0.00220308, 0.01102791, 0.00472617,\n",
       "        0.00380354, 0.00979509, 0.00497627, 0.00482235, 0.00994763,\n",
       "        0.00383458, 0.00398498, 0.02045927, 0.00767169, 0.0028089 ,\n",
       "        0.01391883, 0.00617323, 0.00398784, 0.01935787, 0.00620685,\n",
       "        0.00436616, 0.01845093, 0.00579271, 0.004178  , 0.01641035,\n",
       "        0.00678635, 0.00380588, 0.0150806 , 0.00580392, 0.00380044,\n",
       "        0.01408148, 0.00579667, 0.00384126, 0.01189251, 0.00715642,\n",
       "        0.00410361, 0.0143723 , 0.00540643, 0.0015976 , 0.00895858,\n",
       "        0.00423002, 0.00379992, 0.00739799, 0.00399375, 0.00463152,\n",
       "        0.00949426, 0.00495481, 0.00392084, 0.01426172, 0.00920448,\n",
       "        0.00159969, 0.01679859, 0.00639358, 0.0034224 , 0.01105175,\n",
       "        0.00559816, 0.00414281, 0.00934548, 0.00399518, 0.00432863,\n",
       "        0.01409249, 0.00671649, 0.00220332, 0.01525855, 0.00598564,\n",
       "        0.00401554, 0.01362281, 0.00520062, 0.00382495, 0.01216507,\n",
       "        0.00580182, 0.00491405, 0.01832628, 0.00798264, 0.00279822,\n",
       "        0.01354251, 0.00619197, 0.00400386, 0.01221762, 0.00726027,\n",
       "        0.00379868, 0.01301332, 0.00611057, 0.00446286, 0.00878005,\n",
       "        0.00479918, 0.00100064, 0.00938196, 0.004003  , 0.00179987,\n",
       "        0.0072588 , 0.00364833, 0.0049943 , 0.00808258, 0.00359802,\n",
       "        0.00390382, 0.0114964 , 0.0047966 , 0.00131283, 0.01076417,\n",
       "        0.00480185, 0.00190163, 0.00911403, 0.00459328, 0.00444055,\n",
       "        0.00829463, 0.00399141, 0.00424213, 0.01244364, 0.00566163,\n",
       "        0.00141535, 0.02367439, 0.00699267, 0.0022162 , 0.01154394,\n",
       "        0.00498519, 0.00360084, 0.01263165, 0.00546317, 0.00433979,\n",
       "        0.0154047 , 0.00739079, 0.00139222, 0.01354094, 0.00599799,\n",
       "        0.00139723, 0.01275039, 0.00579987, 0.00490613, 0.01831307,\n",
       "        0.00689378, 0.00550632, 0.01390371, 0.00543308, 0.00140228,\n",
       "        0.01200094, 0.00460043, 0.00179958, 0.00956092, 0.00419788,\n",
       "        0.00362091, 0.00947609, 0.00458803, 0.00438938, 0.015097  ,\n",
       "        0.00619526, 0.00120559, 0.02268066, 0.00658913, 0.0023963 ,\n",
       "        0.01180053, 0.00534883, 0.0022069 , 0.00985584, 0.00501437,\n",
       "        0.00380268, 0.01777453, 0.00677199, 0.00139456, 0.01199389,\n",
       "        0.00531783, 0.00200353, 0.01271195, 0.00632806, 0.00200524,\n",
       "        0.01272879, 0.00559597, 0.00279884, 0.01697063, 0.00779915,\n",
       "        0.00142503, 0.01699328, 0.00643754, 0.00160093, 0.01364608,\n",
       "        0.00724998, 0.00120091, 0.01279826, 0.00599685, 0.00199962,\n",
       "        0.00140266, 0.00120692, 0.00100155, 0.00100408, 0.00117955,\n",
       "        0.0012177 , 0.00177937, 0.00139999, 0.00120106, 0.00160065,\n",
       "        0.00176525, 0.00099816, 0.00150714, 0.001401  , 0.00155301,\n",
       "        0.00127063, 0.00130353, 0.00140209, 0.00143309, 0.00137625,\n",
       "        0.00107474, 0.00138822, 0.001472  , 0.00107064, 0.00149131,\n",
       "        0.00145488, 0.00140061, 0.0017982 , 0.00160174, 0.00160184,\n",
       "        0.00139866, 0.00099974, 0.00100036, 0.00120478, 0.00100045,\n",
       "        0.00110984, 0.0011971 , 0.001405  , 0.00110416, 0.00100355,\n",
       "        0.00119934, 0.00100026, 0.00119967, 0.00100017, 0.00120535,\n",
       "        0.00099936, 0.00120096, 0.00120244, 0.00080371, 0.00120015,\n",
       "        0.00100265, 0.00099874, 0.0008007 , 0.00118923, 0.00121984,\n",
       "        0.0010046 , 0.00119143, 0.00139852, 0.00100274, 0.00100307,\n",
       "        0.00119987, 0.0008018 , 0.00120025, 0.00099936, 0.00080485,\n",
       "        0.00080371, 0.00080404, 0.00079331, 0.0011992 , 0.00080261,\n",
       "        0.00119982, 0.00120039, 0.00099359, 0.00079203, 0.00079727,\n",
       "        0.0012001 , 0.0010005 , 0.00119958, 0.00102005, 0.0011982 ,\n",
       "        0.0046021 , 0.00470195, 0.0059988 , 0.00519719, 0.00470686,\n",
       "        0.00272779, 0.00542989, 0.00580273, 0.005302  , 0.00538583,\n",
       "        0.00583324, 0.00520492, 0.00539937, 0.00575919, 0.00500317,\n",
       "        0.00559764, 0.00379691, 0.00394583, 0.00631247, 0.00650034,\n",
       "        0.00469704, 0.0026135 , 0.00599747, 0.00664244, 0.00559692,\n",
       "        0.00559711, 0.00529833, 0.00559511, 0.00519247, 0.00520263,\n",
       "        0.00539937, 0.00499616, 0.00419998, 0.00359888, 0.00530291,\n",
       "        0.00522246, 0.00439949, 0.00240006, 0.00545716, 0.00525999,\n",
       "        0.004     , 0.00580111, 0.00630074, 0.00520082, 0.00522156,\n",
       "        0.00489373, 0.00579939, 0.00580039, 0.00240078, 0.00292339,\n",
       "        0.0053771 , 0.00497775, 0.00239959, 0.00220141, 0.0069747 ,\n",
       "        0.00578704, 0.00263271, 0.00413895, 0.00551033, 0.0059423 ,\n",
       "        0.00300021, 0.00450459, 0.00499315, 0.00520682, 0.00120058,\n",
       "        0.0010015 , 0.00521841, 0.00517945, 0.00100169, 0.00139918,\n",
       "        0.00450182, 0.00502462, 0.00099993, 0.00319591, 0.00544281,\n",
       "        0.00534258, 0.0010004 , 0.00218735, 0.00587821, 0.00570569]),\n",
       " 'std_fit_time': array([1.15157201e-02, 3.93359460e-04, 5.37588263e-04, 7.79549618e-04,\n",
       "        5.39284684e-04, 3.36770327e-04, 1.01421895e-03, 8.07788282e-04,\n",
       "        7.22437496e-04, 9.12619911e-04, 4.95732363e-04, 8.38059965e-04,\n",
       "        2.67754921e-03, 5.70714090e-04, 7.21112748e-06, 2.95593887e-03,\n",
       "        1.09379508e-03, 3.72272740e-04, 2.14985190e-03, 3.95051991e-04,\n",
       "        1.23794126e-06, 2.43824601e-03, 1.03758868e-03, 1.01896790e-03,\n",
       "        1.84710912e-03, 6.93582741e-04, 7.37627808e-04, 1.46012790e-03,\n",
       "        4.06184698e-04, 4.00735224e-04, 8.87307233e-04, 5.23058383e-04,\n",
       "        7.45684367e-04, 2.35603403e-03, 7.42568318e-04, 4.09673029e-04,\n",
       "        1.35602333e-03, 3.78982290e-04, 8.80021851e-04, 1.91609325e-03,\n",
       "        6.68912808e-04, 4.02807466e-06, 2.52860356e-03, 4.00115239e-04,\n",
       "        4.09018935e-04, 1.63748246e-03, 1.74991181e-04, 4.85609833e-04,\n",
       "        6.63777476e-04, 5.32441043e-04, 3.99578255e-04, 4.72121058e-04,\n",
       "        4.84636551e-04, 8.94844119e-04, 1.81907711e-03, 5.31504810e-04,\n",
       "        1.14886292e-03, 1.21558425e-03, 5.08328360e-04, 3.93737565e-04,\n",
       "        1.36413808e-03, 1.04454485e-03, 3.99828358e-04, 1.14011800e-03,\n",
       "        3.93523309e-04, 4.01157391e-04, 1.62279321e-03, 6.55900752e-04,\n",
       "        3.73160056e-04, 3.76831341e-03, 7.10058140e-04, 3.07718502e-05,\n",
       "        2.88908897e-03, 1.23216616e-03, 7.28513592e-04, 1.58927777e-03,\n",
       "        1.00776499e-03, 2.04509226e-05, 2.93045540e-03, 9.37572720e-04,\n",
       "        5.17806184e-04, 4.65269081e-03, 3.96844544e-04, 4.22666999e-04,\n",
       "        2.45715271e-03, 5.37057100e-04, 4.00518632e-04, 2.89471356e-03,\n",
       "        7.51551826e-04, 4.00873213e-04, 2.16368533e-03, 4.07815771e-04,\n",
       "        3.18236975e-04, 1.80812026e-03, 1.92673929e-03, 2.30812848e-04,\n",
       "        4.18626278e-03, 5.14062777e-04, 4.82225548e-04, 7.00832182e-04,\n",
       "        4.59318476e-04, 7.49157355e-04, 1.42371235e-03, 8.82275482e-04,\n",
       "        3.75608083e-04, 2.79434590e-03, 5.44106029e-04, 1.32884635e-04,\n",
       "        2.85111076e-03, 2.03263284e-03, 4.88187401e-04, 3.23984414e-03,\n",
       "        4.95349094e-04, 8.20062399e-04, 2.56068407e-03, 7.97494111e-04,\n",
       "        4.41388647e-04, 1.15131805e-03, 6.38839244e-04, 8.68362644e-04,\n",
       "        2.59462582e-03, 8.40709861e-04, 3.97855523e-04, 2.64889391e-03,\n",
       "        6.38458969e-04, 6.45046858e-04, 1.42469045e-03, 4.01020929e-04,\n",
       "        3.73073839e-04, 3.82495628e-03, 7.48883324e-04, 1.26833744e-03,\n",
       "        8.84318981e-04, 6.80510037e-04, 3.97282555e-04, 1.17251113e-03,\n",
       "        4.04911657e-04, 6.28646268e-04, 1.89484613e-03, 1.87926663e-03,\n",
       "        3.98022417e-04, 1.52990824e-03, 1.18197272e-03, 7.72289878e-04,\n",
       "        1.02844953e-03, 9.79628077e-04, 1.16800773e-06, 1.68816806e-03,\n",
       "        5.46368277e-04, 7.47131011e-04, 7.27124081e-04, 8.34593401e-04,\n",
       "        5.77083502e-04, 1.49113473e-03, 4.92355353e-04, 4.96146678e-04,\n",
       "        4.63490859e-04, 7.48173709e-04, 3.99357699e-04, 1.31595459e-03,\n",
       "        3.94873069e-04, 4.84501493e-04, 1.36817076e-03, 4.91718991e-04,\n",
       "        4.65769791e-04, 9.13923473e-04, 1.66438711e-05, 4.33640688e-04,\n",
       "        1.02125487e-03, 5.93069071e-04, 5.05110917e-04, 9.14983985e-03,\n",
       "        8.90459319e-04, 3.94851504e-04, 6.68946153e-04, 7.17255657e-06,\n",
       "        8.01335464e-04, 2.28961140e-03, 4.53293687e-04, 4.27210292e-04,\n",
       "        3.08441937e-03, 6.62282001e-04, 4.99526960e-04, 1.25539735e-03,\n",
       "        1.48203566e-06, 4.85932079e-04, 1.48124799e-03, 7.48355498e-04,\n",
       "        4.87895167e-04, 6.35465555e-03, 7.15812079e-04, 1.08859084e-03,\n",
       "        3.29639107e-03, 8.69487003e-04, 4.93281128e-04, 3.16547370e-03,\n",
       "        8.01403908e-04, 7.46735561e-04, 3.74820364e-03, 7.49402964e-04,\n",
       "        7.85262917e-04, 3.45648464e-03, 1.02129255e-03, 7.99830517e-04,\n",
       "        4.08306453e-03, 1.16914712e-03, 3.99400142e-04, 9.01417369e-03,\n",
       "        1.85839702e-03, 4.98809284e-04, 1.16954216e-03, 7.96571089e-04,\n",
       "        7.78546422e-04, 1.89974789e-03, 8.66523952e-04, 9.97643855e-04,\n",
       "        6.11491156e-03, 7.58013055e-04, 4.81387561e-04, 6.68268045e-04,\n",
       "        1.10167963e-03, 6.30146224e-04, 1.34487877e-03, 8.61672484e-04,\n",
       "        6.72023875e-04, 1.94492516e-03, 7.94447618e-04, 7.49745189e-04,\n",
       "        1.01832018e-03, 1.83503066e-03, 5.12131987e-04, 3.09986774e-03,\n",
       "        1.22629848e-03, 4.82055303e-04, 1.44497322e-03, 8.08762274e-04,\n",
       "        4.00624437e-04, 6.94307969e-04, 8.93674606e-04, 6.31581121e-04,\n",
       "        4.92490252e-04, 3.91115412e-04, 6.32795237e-04, 4.70983819e-06,\n",
       "        3.60103040e-04, 4.40222784e-04, 3.91712900e-04, 4.87937261e-04,\n",
       "        3.97266329e-04, 8.00622192e-04, 6.93201139e-04, 3.14351077e-06,\n",
       "        7.75647960e-04, 4.89558063e-04, 2.45639014e-04, 2.35015113e-04,\n",
       "        2.77989708e-04, 7.45482986e-04, 2.81625023e-04, 1.68791639e-04,\n",
       "        4.14586023e-04, 2.72105009e-04, 4.74462437e-04, 1.05551823e-04,\n",
       "        3.20020903e-04, 5.53853336e-04, 4.90836113e-04, 3.99647017e-04,\n",
       "        4.91297508e-04, 8.03129970e-04, 4.87179057e-04, 7.41539275e-06,\n",
       "        7.57165758e-06, 4.04942440e-04, 5.49308053e-04, 6.73208218e-04,\n",
       "        3.96970614e-04, 5.01822732e-04, 2.17170792e-04, 1.78526966e-05,\n",
       "        4.01331053e-04, 4.42200589e-07, 4.00213248e-04, 1.27948836e-06,\n",
       "        3.96889375e-04, 9.31798861e-06, 4.07456261e-04, 3.98966279e-04,\n",
       "        4.02103999e-04, 3.93803969e-04, 5.13923400e-06, 1.89539022e-06,\n",
       "        4.00358810e-04, 3.77656384e-04, 3.92400090e-04, 1.21069696e-05,\n",
       "        3.97979770e-04, 4.99642694e-04, 3.99064559e-06, 4.41531640e-06,\n",
       "        3.99161539e-04, 7.48401229e-04, 3.99566390e-04, 6.31806690e-04,\n",
       "        4.02576909e-04, 4.02059354e-04, 4.02108511e-04, 3.96736728e-04,\n",
       "        7.47321846e-04, 4.01339721e-04, 3.99315252e-04, 4.01191397e-04,\n",
       "        1.29227269e-05, 3.96207866e-04, 3.98706899e-04, 4.01188512e-04,\n",
       "        8.99694551e-07, 4.00502271e-04, 6.61539024e-04, 3.98691661e-04,\n",
       "        8.00504936e-04, 1.53962714e-03, 6.38021329e-04, 3.99889344e-04,\n",
       "        6.11922953e-04, 6.49649976e-04, 5.19387344e-04, 4.03692647e-04,\n",
       "        1.72094147e-03, 5.04356534e-04, 4.20675313e-04, 3.77940091e-04,\n",
       "        5.33859685e-04, 9.70721324e-04, 6.86387835e-05, 7.99980058e-04,\n",
       "        7.52568728e-04, 1.46665913e-03, 8.78123865e-04, 7.74640119e-04,\n",
       "        8.67405448e-04, 4.74013345e-04, 1.09493208e-03, 9.87245046e-04,\n",
       "        9.71198033e-04, 4.89308745e-04, 4.13242345e-04, 7.96039293e-04,\n",
       "        9.79231346e-04, 3.98647631e-04, 9.62781227e-04, 2.62736841e-06,\n",
       "        4.00185596e-04, 1.49578380e-03, 3.90741357e-04, 4.01253202e-04,\n",
       "        1.36053763e-03, 4.89380338e-04, 6.51316359e-04, 3.89695908e-04,\n",
       "        6.10649513e-07, 9.78967503e-04, 8.70174529e-04, 4.02124140e-04,\n",
       "        7.74824412e-04, 1.27309530e-03, 1.16846076e-03, 7.47298528e-04,\n",
       "        4.91134231e-04, 4.56121415e-04, 4.66226770e-04, 4.10884246e-05,\n",
       "        1.02005769e-03, 4.00502583e-04, 6.69213525e-04, 6.36011715e-04,\n",
       "        7.46618348e-04, 2.17099022e-03, 3.91123917e-04, 5.49051379e-04,\n",
       "        6.33619206e-04, 7.79008131e-04, 6.26715032e-04, 4.05350543e-04,\n",
       "        3.99533219e-04, 1.90643050e-05, 3.90686905e-04, 4.14848750e-04,\n",
       "        2.60127672e-06, 4.90253941e-04, 9.96832122e-04, 5.24825095e-05,\n",
       "        6.57274664e-07, 7.48313553e-04, 5.04101486e-04, 5.54294054e-04,\n",
       "        6.00322643e-06, 9.50338114e-04, 7.63732275e-04, 7.45641847e-04]),\n",
       " 'mean_score_time': array([0.0004045 , 0.00020013, 0.00040011, 0.00021358, 0.        ,\n",
       "        0.00032315, 0.00031185, 0.00060029, 0.00040026, 0.00060487,\n",
       "        0.00076418, 0.00019917, 0.00063086, 0.0004055 , 0.00059648,\n",
       "        0.00020375, 0.00040278, 0.        , 0.00020418, 0.        ,\n",
       "        0.00060096, 0.00040874, 0.00073881, 0.00069246, 0.00062461,\n",
       "        0.00050659, 0.00059671, 0.00059958, 0.00040412, 0.00080161,\n",
       "        0.        , 0.00020008, 0.00020018, 0.00020018, 0.0006228 ,\n",
       "        0.00040383, 0.0004046 , 0.00080667, 0.00070305, 0.00059962,\n",
       "        0.00040512, 0.00060062, 0.00042458, 0.00020056, 0.00039988,\n",
       "        0.00020103, 0.00080566, 0.00039859, 0.00020494, 0.        ,\n",
       "        0.00079908, 0.00040393, 0.00080152, 0.00020022, 0.0004035 ,\n",
       "        0.00010142, 0.00052185, 0.00039983, 0.00020418, 0.00019679,\n",
       "        0.00020385, 0.00040507, 0.        , 0.        , 0.00020471,\n",
       "        0.00039754, 0.00080876, 0.00060568, 0.00019908, 0.00081134,\n",
       "        0.00061054, 0.0008316 , 0.00060506, 0.00020037, 0.00061712,\n",
       "        0.00040355, 0.00040054, 0.00020609, 0.00063405, 0.00066528,\n",
       "        0.00047259, 0.00039992, 0.00040431, 0.00020051, 0.        ,\n",
       "        0.00040808, 0.00040102, 0.00040035, 0.00083075, 0.00059977,\n",
       "        0.0004003 , 0.00020089, 0.00019846, 0.00040035, 0.00020323,\n",
       "        0.00022697, 0.00080791, 0.00080686, 0.00019994, 0.00019994,\n",
       "        0.00020032, 0.00040002, 0.00020475, 0.0004005 , 0.        ,\n",
       "        0.00040479, 0.00058208, 0.00060673, 0.00040417, 0.00040359,\n",
       "        0.00059938, 0.00060697, 0.00039988, 0.0003274 , 0.00017881,\n",
       "        0.00042748, 0.00019736, 0.00065241, 0.00040445, 0.00070553,\n",
       "        0.00040369, 0.00030103, 0.00019841, 0.00040798, 0.00040712,\n",
       "        0.00020299, 0.00019975, 0.00059953, 0.00039983, 0.00060372,\n",
       "        0.00019975, 0.00064139, 0.00043612, 0.00050821, 0.00020084,\n",
       "        0.00040407, 0.00040412, 0.00019937, 0.00020018, 0.00049243,\n",
       "        0.00062165, 0.00040026, 0.00040998, 0.00042257, 0.00040011,\n",
       "        0.00040035, 0.00060019, 0.        , 0.00020013, 0.00059967,\n",
       "        0.00020475, 0.0004004 , 0.00019855, 0.00020013, 0.        ,\n",
       "        0.00040059, 0.        , 0.00020337, 0.00040002, 0.00040016,\n",
       "        0.00060062, 0.00020118, 0.00019889, 0.        , 0.00039988,\n",
       "        0.00060639, 0.00020275, 0.00020137, 0.00020037, 0.        ,\n",
       "        0.00064893, 0.00019999, 0.00040321, 0.        , 0.00060782,\n",
       "        0.00040731, 0.0007988 , 0.00044379, 0.        , 0.00060353,\n",
       "        0.00019999, 0.00060701, 0.        , 0.00020032, 0.00060167,\n",
       "        0.00059938, 0.00020027, 0.00040054, 0.00039825, 0.00069094,\n",
       "        0.00022492, 0.00100074, 0.0004046 , 0.0007998 , 0.00060096,\n",
       "        0.00020123, 0.00019994, 0.00039968, 0.00080171, 0.0003993 ,\n",
       "        0.00019994, 0.00080738, 0.00040774, 0.00041108, 0.00019989,\n",
       "        0.00020361, 0.00080147, 0.00030599, 0.00080757, 0.00020099,\n",
       "        0.00102386, 0.00040841, 0.00039926, 0.00020051, 0.00039763,\n",
       "        0.00040693, 0.00022936, 0.00057154, 0.00060506, 0.00060492,\n",
       "        0.        , 0.00020018, 0.00073185, 0.00041137, 0.00059843,\n",
       "        0.00080924, 0.00040426, 0.00040035, 0.00060878, 0.00080004,\n",
       "        0.00080853, 0.00040355, 0.00040407, 0.00059915, 0.00039988,\n",
       "        0.00020256, 0.0004003 , 0.0002008 , 0.00080247, 0.0004003 ,\n",
       "        0.00040011, 0.00021095, 0.00049057, 0.        , 0.00019941,\n",
       "        0.00060153, 0.00060048, 0.00060029, 0.00059891, 0.00039997,\n",
       "        0.00059953, 0.0004003 , 0.00039959, 0.00099926, 0.00056982,\n",
       "        0.0003583 , 0.00068002, 0.00075912, 0.00055208, 0.00044012,\n",
       "        0.00050154, 0.00034695, 0.00067611, 0.00033011, 0.00034232,\n",
       "        0.00060225, 0.00040021, 0.00080247, 0.00110335, 0.00060015,\n",
       "        0.00040526, 0.00039663, 0.00020032, 0.00020037, 0.00050354,\n",
       "        0.00020218, 0.00080571, 0.00019898, 0.0003993 , 0.00059905,\n",
       "        0.00040202, 0.00039973, 0.00040035, 0.00039978, 0.00059762,\n",
       "        0.00059943, 0.0003993 , 0.00039916, 0.00039864, 0.00019989,\n",
       "        0.00059977, 0.00040045, 0.00019999, 0.00041213, 0.00038052,\n",
       "        0.00079679, 0.00020013, 0.00040026, 0.00020046, 0.0001996 ,\n",
       "        0.0004004 , 0.00050092, 0.00080099, 0.0004003 , 0.00059676,\n",
       "        0.00059671, 0.0006001 , 0.00040312, 0.00079975, 0.00059743,\n",
       "        0.00040002, 0.00039835, 0.0008193 , 0.00040221, 0.00040355,\n",
       "        0.00019999, 0.00079999, 0.00079985, 0.00039926, 0.00040054,\n",
       "        0.00079961, 0.0006052 , 0.00060115, 0.0008038 , 0.00020056,\n",
       "        0.00020065, 0.00078869, 0.00020041, 0.0004005 , 0.00081344,\n",
       "        0.0002007 , 0.00042162, 0.00060029, 0.00019927, 0.00073328,\n",
       "        0.00040321, 0.00060072, 0.00101409, 0.00040116, 0.00040007,\n",
       "        0.00079985, 0.00060143, 0.00100088, 0.00047483, 0.00040073,\n",
       "        0.00040021, 0.00040164, 0.0002059 , 0.00080605, 0.00020084,\n",
       "        0.00060239, 0.00060282, 0.        , 0.0004004 , 0.00020027,\n",
       "        0.00104594, 0.00040107, 0.0001996 , 0.000495  , 0.00039992,\n",
       "        0.0004005 , 0.00059996, 0.        , 0.00059967, 0.00040116,\n",
       "        0.00100684, 0.00040026, 0.0004003 , 0.        , 0.        ,\n",
       "        0.00020022, 0.00020084, 0.00020008, 0.00060606, 0.00060382,\n",
       "        0.00046768, 0.00039029, 0.00063562, 0.00058918, 0.00060358,\n",
       "        0.00019965, 0.00020108, 0.00059943, 0.00040588, 0.        ,\n",
       "        0.00090766, 0.00020056, 0.00039935, 0.00019994, 0.00039959,\n",
       "        0.00060186, 0.00097461, 0.00019999, 0.00040326, 0.0006021 ,\n",
       "        0.00060196, 0.00040078, 0.00041122, 0.00020132, 0.00099907]),\n",
       " 'std_score_time': array([4.95462252e-04, 4.00257111e-04, 4.90037694e-04, 4.27150726e-04,\n",
       "        0.00000000e+00, 6.46305084e-04, 3.97193303e-04, 4.90869993e-04,\n",
       "        4.90213052e-04, 4.94022673e-04, 3.84062994e-04, 3.98349762e-04,\n",
       "        5.16133769e-04, 4.96769089e-04, 4.87052533e-04, 4.07505035e-04,\n",
       "        4.93328119e-04, 0.00000000e+00, 4.08363342e-04, 0.00000000e+00,\n",
       "        4.90680765e-04, 5.00637543e-04, 6.08298369e-04, 1.71165452e-04,\n",
       "        3.74507868e-04, 4.48429828e-04, 4.87242125e-04, 4.89554603e-04,\n",
       "        4.94961932e-04, 4.00806080e-04, 0.00000000e+00, 4.00161743e-04,\n",
       "        4.00352478e-04, 4.00352478e-04, 5.10218709e-04, 4.94617151e-04,\n",
       "        4.95579040e-04, 4.03392670e-04, 4.00931857e-04, 4.89590494e-04,\n",
       "        4.96224694e-04, 4.90422819e-04, 5.21416743e-04, 4.01115417e-04,\n",
       "        4.89746476e-04, 4.02069092e-04, 4.02918325e-04, 4.88177685e-04,\n",
       "        4.09889221e-04, 0.00000000e+00, 3.99548727e-04, 4.94751180e-04,\n",
       "        4.00758226e-04, 4.00447845e-04, 4.94213883e-04, 2.02846527e-04,\n",
       "        4.50072582e-04, 4.89687246e-04, 4.08363342e-04, 3.93581390e-04,\n",
       "        4.07695770e-04, 4.96172057e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.09412384e-04, 4.86886129e-04, 4.04450523e-04, 4.94566011e-04,\n",
       "        3.98159027e-04, 4.05861650e-04, 4.98543280e-04, 4.17755970e-04,\n",
       "        4.94091438e-04, 4.00733948e-04, 5.04694878e-04, 4.94269848e-04,\n",
       "        4.90563269e-04, 4.12178040e-04, 3.67188812e-04, 1.74161768e-04,\n",
       "        2.37169733e-04, 4.89804325e-04, 4.95207462e-04, 4.01020050e-04,\n",
       "        0.00000000e+00, 5.00091244e-04, 4.91151786e-04, 4.90329853e-04,\n",
       "        4.19207764e-04, 4.89707666e-04, 4.90271528e-04, 4.01782990e-04,\n",
       "        3.96919250e-04, 4.90329667e-04, 4.06455994e-04, 4.53948975e-04,\n",
       "        4.04025185e-04, 4.03496589e-04, 3.99875641e-04, 3.99875641e-04,\n",
       "        4.00638580e-04, 4.89921311e-04, 4.09507751e-04, 4.90504851e-04,\n",
       "        0.00000000e+00, 4.95770517e-04, 4.90432537e-04, 4.95667086e-04,\n",
       "        4.95016029e-04, 4.94327457e-04, 4.89396098e-04, 4.95624148e-04,\n",
       "        4.89745664e-04, 4.22552777e-04, 3.57627869e-04, 5.23562336e-04,\n",
       "        3.94725800e-04, 5.41020406e-04, 4.95392561e-04, 6.06794636e-04,\n",
       "        4.94445838e-04, 6.02054596e-04, 3.96823883e-04, 4.99678165e-04,\n",
       "        4.98622522e-04, 4.05979156e-04, 3.99494171e-04, 4.89512480e-04,\n",
       "        4.89687292e-04, 4.92985664e-04, 3.99494171e-04, 4.10025626e-04,\n",
       "        2.18964454e-04, 4.48025105e-04, 4.01687622e-04, 4.94937800e-04,\n",
       "        4.94975575e-04, 3.98731232e-04, 4.00352478e-04, 6.20282884e-04,\n",
       "        5.08944465e-04, 4.90212936e-04, 5.02127088e-04, 5.18745633e-04,\n",
       "        4.90037787e-04, 4.90329737e-04, 4.90057952e-04, 0.00000000e+00,\n",
       "        4.00257111e-04, 4.89629141e-04, 4.09507751e-04, 4.90389024e-04,\n",
       "        3.97109985e-04, 4.00257111e-04, 0.00000000e+00, 4.90621791e-04,\n",
       "        0.00000000e+00, 4.06742096e-04, 4.89920894e-04, 4.90096066e-04,\n",
       "        4.90408655e-04, 4.02355194e-04, 3.97777557e-04, 0.00000000e+00,\n",
       "        4.89745849e-04, 4.95154937e-04, 4.05502319e-04, 4.02736664e-04,\n",
       "        4.00733948e-04, 0.00000000e+00, 5.36651452e-04, 3.99971008e-04,\n",
       "        4.93861868e-04, 0.00000000e+00, 4.96315094e-04, 4.98856465e-04,\n",
       "        3.99423396e-04, 5.47667926e-04, 0.00000000e+00, 4.92804123e-04,\n",
       "        3.99971008e-04, 4.95792155e-04, 0.00000000e+00, 4.00638580e-04,\n",
       "        4.91265254e-04, 4.89397189e-04, 4.00543213e-04, 4.90563338e-04,\n",
       "        4.87770306e-04, 2.07456060e-04, 2.76334995e-04, 1.10395783e-06,\n",
       "        4.95603127e-04, 3.99900631e-04, 4.90684425e-04, 4.02450562e-04,\n",
       "        3.99875641e-04, 4.89512248e-04, 4.00857167e-04, 4.89044929e-04,\n",
       "        3.99875641e-04, 4.03759075e-04, 4.99385853e-04, 5.03750381e-04,\n",
       "        3.99780273e-04, 4.07218933e-04, 4.00921115e-04, 4.07329071e-04,\n",
       "        4.03839366e-04, 4.01973724e-04, 3.67318158e-05, 5.00513004e-04,\n",
       "        4.89034565e-04, 4.01020050e-04, 4.87065161e-04, 4.98497488e-04,\n",
       "        4.58717346e-04, 4.68357637e-04, 4.94944071e-04, 4.93977483e-04,\n",
       "        0.00000000e+00, 4.00352478e-04, 4.00687277e-04, 5.03977437e-04,\n",
       "        4.88731174e-04, 4.04687131e-04, 4.95146603e-04, 4.90329644e-04,\n",
       "        4.97118360e-04, 4.00019161e-04, 4.04500919e-04, 4.94271458e-04,\n",
       "        4.94904194e-04, 4.89303982e-04, 4.89745849e-04, 4.05120850e-04,\n",
       "        4.90393114e-04, 4.01592255e-04, 4.01240928e-04, 4.90273337e-04,\n",
       "        4.90038112e-04, 4.21905518e-04, 6.35936421e-04, 0.00000000e+00,\n",
       "        3.98826599e-04, 4.91209951e-04, 4.90292202e-04, 4.90135726e-04,\n",
       "        4.89024322e-04, 4.89862464e-04, 8.00146698e-04, 4.90273800e-04,\n",
       "        4.89395610e-04, 2.06421478e-06, 5.36394117e-05, 2.44337027e-04,\n",
       "        4.20396184e-04, 4.44184652e-04, 3.24951841e-05, 2.24791779e-04,\n",
       "        4.48205638e-04, 2.28108520e-04, 2.09186804e-04, 4.18609516e-04,\n",
       "        2.82697300e-04, 4.91783571e-04, 4.90154588e-04, 7.52979751e-04,\n",
       "        6.66431925e-04, 4.90026744e-04, 4.96431508e-04, 4.85795477e-04,\n",
       "        4.00638580e-04, 4.00733948e-04, 4.49492135e-04, 4.04357910e-04,\n",
       "        4.02885006e-04, 3.97968292e-04, 4.89044929e-04, 4.89277624e-04,\n",
       "        4.92379944e-04, 4.89570724e-04, 4.90329737e-04, 4.89628932e-04,\n",
       "        4.87971338e-04, 4.89434412e-04, 4.89045045e-04, 4.88870617e-04,\n",
       "        4.88229559e-04, 3.99780273e-04, 4.89706714e-04, 4.90449783e-04,\n",
       "        3.99971008e-04, 5.05114833e-04, 4.67029415e-04, 3.98426965e-04,\n",
       "        4.00257111e-04, 4.90213215e-04, 4.00924683e-04, 3.99208069e-04,\n",
       "        4.90388514e-04, 6.34338319e-04, 4.00501164e-04, 4.90271250e-04,\n",
       "        4.87290528e-04, 4.87243478e-04, 4.89979544e-04, 4.93741942e-04,\n",
       "        3.99877290e-04, 4.87818475e-04, 4.89920987e-04, 4.87877970e-04,\n",
       "        4.09936924e-04, 4.92678342e-04, 4.94256277e-04, 3.99971008e-04,\n",
       "        3.99998897e-04, 3.99930787e-04, 4.88986906e-04, 4.90563617e-04,\n",
       "        3.99806827e-04, 4.94170635e-04, 4.90838429e-04, 4.01918418e-04,\n",
       "        4.01115417e-04, 4.01306152e-04, 3.94440349e-04, 4.00829315e-04,\n",
       "        4.90506937e-04, 4.09869051e-04, 4.01401520e-04, 5.17273789e-04,\n",
       "        4.90136283e-04, 3.98540497e-04, 4.12797920e-04, 4.93838847e-04,\n",
       "        4.90490254e-04, 6.53167818e-04, 4.91322457e-04, 4.89979335e-04,\n",
       "        3.99934653e-04, 4.91078234e-04, 2.01177817e-06, 5.93087454e-04,\n",
       "        4.90797218e-04, 4.90155145e-04, 4.91906507e-04, 4.11796570e-04,\n",
       "        4.03059407e-04, 4.01687622e-04, 4.91854249e-04, 4.92201159e-04,\n",
       "        0.00000000e+00, 4.90390137e-04, 4.00543213e-04, 9.45932435e-05,\n",
       "        4.91206119e-04, 3.99208069e-04, 4.82005161e-04, 4.89804093e-04,\n",
       "        4.90505129e-04, 4.89862534e-04, 0.00000000e+00, 4.89629049e-04,\n",
       "        4.91322734e-04, 8.63193799e-06, 4.90212843e-04, 4.90271528e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.00447845e-04, 4.01687622e-04,\n",
       "        4.00161743e-04, 4.95053339e-04, 4.93052679e-04, 2.35956333e-04,\n",
       "        3.35203770e-04, 6.56284546e-05, 2.85419818e-04, 4.92847284e-04,\n",
       "        3.99303436e-04, 4.02164459e-04, 4.89437106e-04, 4.97186414e-04,\n",
       "        0.00000000e+00, 4.96455430e-04, 4.01115417e-04, 4.89107472e-04,\n",
       "        3.99875641e-04, 4.89406481e-04, 4.91422129e-04, 7.14096720e-05,\n",
       "        3.99971008e-04, 8.06522369e-04, 4.91642780e-04, 4.91503967e-04,\n",
       "        4.90881239e-04, 5.04312284e-04, 4.02641296e-04, 9.02797694e-06]),\n",
       " 'param_C': masked_array(data=[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_class_weight': masked_array(data=[{0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1},\n",
       "                    {0: 1, 1: 10}, {0: 1, 1: 10}, {0: 1, 1: 10},\n",
       "                    {0: 1, 1: 50}, {0: 1, 1: 50}, {0: 1, 1: 50},\n",
       "                    {0: 1, 1: 100}, {0: 1, 1: 100}, {0: 1, 1: 100},\n",
       "                    {0: 10, 1: 1}, {0: 10, 1: 1}, {0: 10, 1: 1},\n",
       "                    {0: 10, 1: 10}, {0: 10, 1: 10}, {0: 10, 1: 10},\n",
       "                    {0: 10, 1: 50}, {0: 10, 1: 50}, {0: 10, 1: 50},\n",
       "                    {0: 10, 1: 100}, {0: 10, 1: 100}, {0: 10, 1: 100},\n",
       "                    {0: 50, 1: 1}, {0: 50, 1: 1}, {0: 50, 1: 1},\n",
       "                    {0: 50, 1: 10}, {0: 50, 1: 10}, {0: 50, 1: 10},\n",
       "                    {0: 50, 1: 50}, {0: 50, 1: 50}, {0: 50, 1: 50},\n",
       "                    {0: 50, 1: 100}, {0: 50, 1: 100}, {0: 50, 1: 100},\n",
       "                    {0: 100, 1: 1}, {0: 100, 1: 1}, {0: 100, 1: 1},\n",
       "                    {0: 100, 1: 10}, {0: 100, 1: 10}, {0: 100, 1: 10},\n",
       "                    {0: 100, 1: 50}, {0: 100, 1: 50}, {0: 100, 1: 50},\n",
       "                    {0: 100, 1: 100}, {0: 100, 1: 100}, {0: 100, 1: 100},\n",
       "                    {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1},\n",
       "                    {0: 1, 1: 10}, {0: 1, 1: 10}, {0: 1, 1: 10},\n",
       "                    {0: 1, 1: 50}, {0: 1, 1: 50}, {0: 1, 1: 50},\n",
       "                    {0: 1, 1: 100}, {0: 1, 1: 100}, {0: 1, 1: 100},\n",
       "                    {0: 10, 1: 1}, {0: 10, 1: 1}, {0: 10, 1: 1},\n",
       "                    {0: 10, 1: 10}, {0: 10, 1: 10}, {0: 10, 1: 10},\n",
       "                    {0: 10, 1: 50}, {0: 10, 1: 50}, {0: 10, 1: 50},\n",
       "                    {0: 10, 1: 100}, {0: 10, 1: 100}, {0: 10, 1: 100},\n",
       "                    {0: 50, 1: 1}, {0: 50, 1: 1}, {0: 50, 1: 1},\n",
       "                    {0: 50, 1: 10}, {0: 50, 1: 10}, {0: 50, 1: 10},\n",
       "                    {0: 50, 1: 50}, {0: 50, 1: 50}, {0: 50, 1: 50},\n",
       "                    {0: 50, 1: 100}, {0: 50, 1: 100}, {0: 50, 1: 100},\n",
       "                    {0: 100, 1: 1}, {0: 100, 1: 1}, {0: 100, 1: 1},\n",
       "                    {0: 100, 1: 10}, {0: 100, 1: 10}, {0: 100, 1: 10},\n",
       "                    {0: 100, 1: 50}, {0: 100, 1: 50}, {0: 100, 1: 50},\n",
       "                    {0: 100, 1: 100}, {0: 100, 1: 100}, {0: 100, 1: 100},\n",
       "                    {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1},\n",
       "                    {0: 1, 1: 10}, {0: 1, 1: 10}, {0: 1, 1: 10},\n",
       "                    {0: 1, 1: 50}, {0: 1, 1: 50}, {0: 1, 1: 50},\n",
       "                    {0: 1, 1: 100}, {0: 1, 1: 100}, {0: 1, 1: 100},\n",
       "                    {0: 10, 1: 1}, {0: 10, 1: 1}, {0: 10, 1: 1},\n",
       "                    {0: 10, 1: 10}, {0: 10, 1: 10}, {0: 10, 1: 10},\n",
       "                    {0: 10, 1: 50}, {0: 10, 1: 50}, {0: 10, 1: 50},\n",
       "                    {0: 10, 1: 100}, {0: 10, 1: 100}, {0: 10, 1: 100},\n",
       "                    {0: 50, 1: 1}, {0: 50, 1: 1}, {0: 50, 1: 1},\n",
       "                    {0: 50, 1: 10}, {0: 50, 1: 10}, {0: 50, 1: 10},\n",
       "                    {0: 50, 1: 50}, {0: 50, 1: 50}, {0: 50, 1: 50},\n",
       "                    {0: 50, 1: 100}, {0: 50, 1: 100}, {0: 50, 1: 100},\n",
       "                    {0: 100, 1: 1}, {0: 100, 1: 1}, {0: 100, 1: 1},\n",
       "                    {0: 100, 1: 10}, {0: 100, 1: 10}, {0: 100, 1: 10},\n",
       "                    {0: 100, 1: 50}, {0: 100, 1: 50}, {0: 100, 1: 50},\n",
       "                    {0: 100, 1: 100}, {0: 100, 1: 100}, {0: 100, 1: 100},\n",
       "                    {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1},\n",
       "                    {0: 1, 1: 10}, {0: 1, 1: 10}, {0: 1, 1: 10},\n",
       "                    {0: 1, 1: 50}, {0: 1, 1: 50}, {0: 1, 1: 50},\n",
       "                    {0: 1, 1: 100}, {0: 1, 1: 100}, {0: 1, 1: 100},\n",
       "                    {0: 10, 1: 1}, {0: 10, 1: 1}, {0: 10, 1: 1},\n",
       "                    {0: 10, 1: 10}, {0: 10, 1: 10}, {0: 10, 1: 10},\n",
       "                    {0: 10, 1: 50}, {0: 10, 1: 50}, {0: 10, 1: 50},\n",
       "                    {0: 10, 1: 100}, {0: 10, 1: 100}, {0: 10, 1: 100},\n",
       "                    {0: 50, 1: 1}, {0: 50, 1: 1}, {0: 50, 1: 1},\n",
       "                    {0: 50, 1: 10}, {0: 50, 1: 10}, {0: 50, 1: 10},\n",
       "                    {0: 50, 1: 50}, {0: 50, 1: 50}, {0: 50, 1: 50},\n",
       "                    {0: 50, 1: 100}, {0: 50, 1: 100}, {0: 50, 1: 100},\n",
       "                    {0: 100, 1: 1}, {0: 100, 1: 1}, {0: 100, 1: 1},\n",
       "                    {0: 100, 1: 10}, {0: 100, 1: 10}, {0: 100, 1: 10},\n",
       "                    {0: 100, 1: 50}, {0: 100, 1: 50}, {0: 100, 1: 50},\n",
       "                    {0: 100, 1: 100}, {0: 100, 1: 100}, {0: 100, 1: 100},\n",
       "                    {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1},\n",
       "                    {0: 1, 1: 10}, {0: 1, 1: 10}, {0: 1, 1: 10},\n",
       "                    {0: 1, 1: 50}, {0: 1, 1: 50}, {0: 1, 1: 50},\n",
       "                    {0: 1, 1: 100}, {0: 1, 1: 100}, {0: 1, 1: 100},\n",
       "                    {0: 10, 1: 1}, {0: 10, 1: 1}, {0: 10, 1: 1},\n",
       "                    {0: 10, 1: 10}, {0: 10, 1: 10}, {0: 10, 1: 10},\n",
       "                    {0: 10, 1: 50}, {0: 10, 1: 50}, {0: 10, 1: 50},\n",
       "                    {0: 10, 1: 100}, {0: 10, 1: 100}, {0: 10, 1: 100},\n",
       "                    {0: 50, 1: 1}, {0: 50, 1: 1}, {0: 50, 1: 1},\n",
       "                    {0: 50, 1: 10}, {0: 50, 1: 10}, {0: 50, 1: 10},\n",
       "                    {0: 50, 1: 50}, {0: 50, 1: 50}, {0: 50, 1: 50},\n",
       "                    {0: 50, 1: 100}, {0: 50, 1: 100}, {0: 50, 1: 100},\n",
       "                    {0: 100, 1: 1}, {0: 100, 1: 1}, {0: 100, 1: 1},\n",
       "                    {0: 100, 1: 10}, {0: 100, 1: 10}, {0: 100, 1: 10},\n",
       "                    {0: 100, 1: 50}, {0: 100, 1: 50}, {0: 100, 1: 50},\n",
       "                    {0: 100, 1: 100}, {0: 100, 1: 100}, {0: 100, 1: 100},\n",
       "                    {0: 1, 1: 1}, {0: 1, 1: 10}, {0: 1, 1: 50},\n",
       "                    {0: 1, 1: 100}, {0: 10, 1: 1}, {0: 10, 1: 10},\n",
       "                    {0: 10, 1: 50}, {0: 10, 1: 100}, {0: 50, 1: 1},\n",
       "                    {0: 50, 1: 10}, {0: 50, 1: 50}, {0: 50, 1: 100},\n",
       "                    {0: 100, 1: 1}, {0: 100, 1: 10}, {0: 100, 1: 50},\n",
       "                    {0: 100, 1: 100}, {0: 1, 1: 1}, {0: 1, 1: 10},\n",
       "                    {0: 1, 1: 50}, {0: 1, 1: 100}, {0: 10, 1: 1},\n",
       "                    {0: 10, 1: 10}, {0: 10, 1: 50}, {0: 10, 1: 100},\n",
       "                    {0: 50, 1: 1}, {0: 50, 1: 10}, {0: 50, 1: 50},\n",
       "                    {0: 50, 1: 100}, {0: 100, 1: 1}, {0: 100, 1: 10},\n",
       "                    {0: 100, 1: 50}, {0: 100, 1: 100}, {0: 1, 1: 1},\n",
       "                    {0: 1, 1: 10}, {0: 1, 1: 50}, {0: 1, 1: 100},\n",
       "                    {0: 10, 1: 1}, {0: 10, 1: 10}, {0: 10, 1: 50},\n",
       "                    {0: 10, 1: 100}, {0: 50, 1: 1}, {0: 50, 1: 10},\n",
       "                    {0: 50, 1: 50}, {0: 50, 1: 100}, {0: 100, 1: 1},\n",
       "                    {0: 100, 1: 10}, {0: 100, 1: 50}, {0: 100, 1: 100},\n",
       "                    {0: 1, 1: 1}, {0: 1, 1: 10}, {0: 1, 1: 50},\n",
       "                    {0: 1, 1: 100}, {0: 10, 1: 1}, {0: 10, 1: 10},\n",
       "                    {0: 10, 1: 50}, {0: 10, 1: 100}, {0: 50, 1: 1},\n",
       "                    {0: 50, 1: 10}, {0: 50, 1: 50}, {0: 50, 1: 100},\n",
       "                    {0: 100, 1: 1}, {0: 100, 1: 10}, {0: 100, 1: 50},\n",
       "                    {0: 100, 1: 100}, {0: 1, 1: 1}, {0: 1, 1: 10},\n",
       "                    {0: 1, 1: 50}, {0: 1, 1: 100}, {0: 10, 1: 1},\n",
       "                    {0: 10, 1: 10}, {0: 10, 1: 50}, {0: 10, 1: 100},\n",
       "                    {0: 50, 1: 1}, {0: 50, 1: 10}, {0: 50, 1: 50},\n",
       "                    {0: 50, 1: 100}, {0: 100, 1: 1}, {0: 100, 1: 10},\n",
       "                    {0: 100, 1: 50}, {0: 100, 1: 100}, {0: 1, 1: 1},\n",
       "                    {0: 1, 1: 10}, {0: 1, 1: 50}, {0: 1, 1: 100},\n",
       "                    {0: 10, 1: 1}, {0: 10, 1: 10}, {0: 10, 1: 50},\n",
       "                    {0: 10, 1: 100}, {0: 50, 1: 1}, {0: 50, 1: 10},\n",
       "                    {0: 50, 1: 50}, {0: 50, 1: 100}, {0: 100, 1: 1},\n",
       "                    {0: 100, 1: 10}, {0: 100, 1: 50}, {0: 100, 1: 100},\n",
       "                    {0: 1, 1: 1}, {0: 1, 1: 10}, {0: 1, 1: 50},\n",
       "                    {0: 1, 1: 100}, {0: 10, 1: 1}, {0: 10, 1: 10},\n",
       "                    {0: 10, 1: 50}, {0: 10, 1: 100}, {0: 50, 1: 1},\n",
       "                    {0: 50, 1: 10}, {0: 50, 1: 50}, {0: 50, 1: 100},\n",
       "                    {0: 100, 1: 1}, {0: 100, 1: 10}, {0: 100, 1: 50},\n",
       "                    {0: 100, 1: 100}, {0: 1, 1: 1}, {0: 1, 1: 10},\n",
       "                    {0: 1, 1: 50}, {0: 1, 1: 100}, {0: 10, 1: 1},\n",
       "                    {0: 10, 1: 10}, {0: 10, 1: 50}, {0: 10, 1: 100},\n",
       "                    {0: 50, 1: 1}, {0: 50, 1: 10}, {0: 50, 1: 50},\n",
       "                    {0: 50, 1: 100}, {0: 100, 1: 1}, {0: 100, 1: 10},\n",
       "                    {0: 100, 1: 50}, {0: 100, 1: 100}, {0: 1, 1: 1},\n",
       "                    {0: 1, 1: 10}, {0: 1, 1: 50}, {0: 1, 1: 100},\n",
       "                    {0: 10, 1: 1}, {0: 10, 1: 10}, {0: 10, 1: 50},\n",
       "                    {0: 10, 1: 100}, {0: 50, 1: 1}, {0: 50, 1: 10},\n",
       "                    {0: 50, 1: 50}, {0: 50, 1: 100}, {0: 100, 1: 1},\n",
       "                    {0: 100, 1: 10}, {0: 100, 1: 50}, {0: 100, 1: 100},\n",
       "                    {0: 1, 1: 1}, {0: 1, 1: 10}, {0: 1, 1: 50},\n",
       "                    {0: 1, 1: 100}, {0: 10, 1: 1}, {0: 10, 1: 10},\n",
       "                    {0: 10, 1: 50}, {0: 10, 1: 100}, {0: 50, 1: 1},\n",
       "                    {0: 50, 1: 10}, {0: 50, 1: 50}, {0: 50, 1: 100},\n",
       "                    {0: 100, 1: 1}, {0: 100, 1: 10}, {0: 100, 1: 50},\n",
       "                    {0: 100, 1: 100}],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_penalty': masked_array(data=['l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l1', 'l1', 'l1',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l1', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_solver': masked_array(data=['newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_l1_ratio': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5],\n",
       "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 100,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100, 'class_weight': {0: 1, 1: 1}, 'penalty': 'l2', 'solver': 'lbfgs'},\n",
       "  {'C': 100, 'class_weight': {0: 1, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 100, 'class_weight': {0: 1, 1: 10}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 100, 'class_weight': {0: 1, 1: 50}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 100, 'class_weight': {0: 1, 1: 100}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 100, 'class_weight': {0: 10, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 100, 'class_weight': {0: 10, 1: 10}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 100, 'class_weight': {0: 10, 1: 50}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 100, 'class_weight': {0: 50, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 100, 'class_weight': {0: 50, 1: 10}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 100, 'class_weight': {0: 50, 1: 50}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 100, 'class_weight': {0: 100, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10, 'class_weight': {0: 1, 1: 1}, 'penalty': 'l2', 'solver': 'lbfgs'},\n",
       "  {'C': 10, 'class_weight': {0: 1, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10, 'class_weight': {0: 1, 1: 10}, 'penalty': 'l2', 'solver': 'lbfgs'},\n",
       "  {'C': 10, 'class_weight': {0: 1, 1: 10}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10, 'class_weight': {0: 1, 1: 50}, 'penalty': 'l2', 'solver': 'lbfgs'},\n",
       "  {'C': 10, 'class_weight': {0: 1, 1: 50}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 10, 'class_weight': {0: 1, 1: 100}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10, 'class_weight': {0: 10, 1: 1}, 'penalty': 'l2', 'solver': 'lbfgs'},\n",
       "  {'C': 10, 'class_weight': {0: 10, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 10, 'class_weight': {0: 10, 1: 10}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 10, 'class_weight': {0: 10, 1: 50}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 10, 'class_weight': {0: 10, 1: 100}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10, 'class_weight': {0: 50, 1: 1}, 'penalty': 'l2', 'solver': 'lbfgs'},\n",
       "  {'C': 10, 'class_weight': {0: 50, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 10, 'class_weight': {0: 50, 1: 10}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 10, 'class_weight': {0: 50, 1: 50}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 10, 'class_weight': {0: 50, 1: 100}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 10, 'class_weight': {0: 100, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 10, 'class_weight': {0: 100, 1: 10}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 10, 'class_weight': {0: 100, 1: 50}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0, 'class_weight': {0: 1, 1: 1}, 'penalty': 'l2', 'solver': 'lbfgs'},\n",
       "  {'C': 1.0, 'class_weight': {0: 1, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 1.0, 'class_weight': {0: 1, 1: 10}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 1.0, 'class_weight': {0: 1, 1: 50}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 1.0, 'class_weight': {0: 1, 1: 100}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 1.0, 'class_weight': {0: 10, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 1.0, 'class_weight': {0: 10, 1: 10}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 1.0, 'class_weight': {0: 10, 1: 50}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 1.0, 'class_weight': {0: 50, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 1.0, 'class_weight': {0: 50, 1: 10}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 1.0, 'class_weight': {0: 50, 1: 50}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 1.0, 'class_weight': {0: 100, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1, 'class_weight': {0: 1, 1: 1}, 'penalty': 'l2', 'solver': 'lbfgs'},\n",
       "  {'C': 0.1, 'class_weight': {0: 1, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.1, 'class_weight': {0: 1, 1: 10}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.1, 'class_weight': {0: 1, 1: 50}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.1, 'class_weight': {0: 1, 1: 100}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.1, 'class_weight': {0: 10, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.1, 'class_weight': {0: 10, 1: 10}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.1, 'class_weight': {0: 10, 1: 50}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.1, 'class_weight': {0: 50, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.1, 'class_weight': {0: 50, 1: 10}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.1, 'class_weight': {0: 50, 1: 50}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.1, 'class_weight': {0: 100, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01, 'class_weight': {0: 1, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01, 'class_weight': {0: 1, 1: 10}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01, 'class_weight': {0: 1, 1: 50}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01, 'class_weight': {0: 10, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01, 'class_weight': {0: 50, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'}],\n",
       " 'split0_test_score': array([0.9875 , 0.9875 , 0.9875 , 0.98125, 0.98125, 0.9625 , 0.70625,\n",
       "        0.70625, 0.31875, 0.25625, 0.25625, 0.03125, 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.74375,\n",
       "        0.98125, 0.98125, 0.58125, 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9375 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.98125, 0.98125, 0.96875, 0.70625, 0.70625,\n",
       "        0.61875, 0.25625, 0.25625, 0.49375, 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.98125,\n",
       "        0.98125, 0.98125, 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.98125, 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.98125,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.98125, 0.9875 ,\n",
       "        0.9875 , 0.925  , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.98125, 0.98125, 0.98125, 0.70625, 0.70625, 0.475  ,\n",
       "        0.25625, 0.25625, 0.0125 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.79375, 0.98125, 0.98125,\n",
       "        0.81875, 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9375 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.975  , 0.9875 , 0.9875 ,\n",
       "        0.98125, 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.98125, 0.98125, 0.98125, 0.7125 , 0.7125 , 0.9875 , 0.25   ,\n",
       "        0.25   , 0.98125, 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.98125, 0.98125, 0.98125, 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.93125, 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9625 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.725  , 0.725  , 0.725  , 0.18125, 0.18125,\n",
       "        0.5    , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.98125, 0.98125, 0.98125, 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.98125, 0.70625, 0.25625, 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.98125, 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.98125, 0.70625,\n",
       "        0.25625, 0.9875 , 0.9875 , 0.9875 , 0.98125, 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.98125, 0.7125 , 0.25   , 0.9875 , 0.9875 , 0.9875 , 0.98125,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.98125, 0.7125 , 0.21875, 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.98125, 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.675  , 0.1375 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.98125, 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.98125,\n",
       "        0.35625, 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.95   , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.98125, 0.55   , 0.025  , 0.9875 , 0.9875 , 0.975  ,\n",
       "        0.96875, 0.9875 , 0.9875 , 0.9625 , 0.85   , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.94375, 0.9875 , 0.98125, 0.25   , 0.08125, 0.9875 ,\n",
       "        0.9875 , 0.975  , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.95   , 0.9875 , 0.9875 , 0.98125, 0.25   ,\n",
       "        0.00625, 0.9875 , 0.9875 , 0.9875 , 0.95625, 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.96875, 0.9875 , 0.9875 , 0.9875 , 0.98125, 0.9875 ,\n",
       "        0.9875 , 0.4625 , 0.21875, 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 ]),\n",
       " 'split1_test_score': array([0.98125, 0.98125, 0.98125, 0.95625, 0.95625, 0.91875, 0.7875 ,\n",
       "        0.7875 , 0.41875, 0.49375, 0.49375, 0.66875, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.975  , 0.975  , 0.88125,\n",
       "        0.95625, 0.95625, 0.96875, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.975  , 0.98125, 0.98125, 0.925  , 0.98125, 0.98125,\n",
       "        0.9625 , 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.95625, 0.95625, 0.86875, 0.7875 , 0.7875 ,\n",
       "        0.1875 , 0.49375, 0.49375, 0.925  , 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.975  , 0.975  , 0.775  , 0.95625,\n",
       "        0.95625, 0.91875, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.89375, 0.98125, 0.98125, 0.93125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.95625, 0.95625, 0.93125, 0.79375, 0.79375, 0.8    ,\n",
       "        0.49375, 0.49375, 0.35625, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.975  , 0.975  , 0.975  , 0.95625, 0.95625,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.9375 , 0.98125, 0.98125, 0.925  , 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.975  , 0.975  , 0.975  , 0.8    , 0.8    , 0.4375 , 0.48125,\n",
       "        0.48125, 0.125  , 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.975  , 0.975  , 0.9625 , 0.95625, 0.95625, 0.925  ,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.975  , 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.9625 ,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.8    , 0.8    , 0.8    , 0.44375, 0.44375,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.975  , 0.975  , 0.975  , 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.95625, 0.7875 , 0.49375, 0.98125,\n",
       "        0.98125, 0.975  , 0.95625, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.95625, 0.7875 ,\n",
       "        0.49375, 0.98125, 0.98125, 0.975  , 0.95625, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.95625, 0.7875 , 0.48125, 0.98125, 0.98125, 0.975  , 0.95625,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.96875, 0.78125, 0.46875, 0.98125, 0.98125,\n",
       "        0.975  , 0.95625, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.96875, 0.825  , 0.45625,\n",
       "        0.98125, 0.98125, 0.98125, 0.96875, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.93125,\n",
       "        0.70625, 0.25   , 0.98125, 0.98125, 0.9625 , 0.80625, 0.98125,\n",
       "        0.98125, 0.98125, 0.81875, 0.98125, 0.98125, 0.85   , 0.95625,\n",
       "        0.98125, 0.95625, 0.475  , 0.2    , 0.98125, 0.98125, 0.89375,\n",
       "        0.81875, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.975  , 0.98125, 0.95625, 0.6375 , 0.075  , 0.98125,\n",
       "        0.98125, 0.98125, 0.94375, 0.98125, 0.98125, 0.98125, 0.94375,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.975  , 0.60625,\n",
       "        0.01875, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.95625, 0.96875, 0.98125, 0.98125, 0.975  , 0.98125, 0.98125,\n",
       "        0.98125, 0.49375, 0.15   , 0.98125, 0.98125, 0.98125, 0.9    ,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125]),\n",
       " 'split2_test_score': array([0.98125, 0.98125, 0.98125, 0.975  , 0.975  , 0.98125, 0.73125,\n",
       "        0.73125, 0.71875, 0.3625 , 0.3625 , 0.8625 , 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.975  , 0.975  , 0.7875 , 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.8625 , 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.975  , 0.975  , 0.94375, 0.73125, 0.73125,\n",
       "        0.94375, 0.3625 , 0.3625 , 0.88125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.9625 , 0.975  ,\n",
       "        0.975  , 0.9375 , 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.95625, 0.98125, 0.98125, 0.91875,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.975  , 0.98125,\n",
       "        0.98125, 0.975  , 0.98125, 0.98125, 0.975  , 0.98125, 0.98125,\n",
       "        0.98125, 0.975  , 0.975  , 0.975  , 0.73125, 0.73125, 0.01875,\n",
       "        0.3625 , 0.3625 , 0.0875 , 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.96875, 0.975  , 0.975  ,\n",
       "        0.9625 , 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.975  , 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.975  , 0.975  , 0.975  , 0.73125, 0.73125, 0.275  , 0.35   ,\n",
       "        0.35   , 0.33125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.975  , 0.975  , 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.95625,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.7625 , 0.7625 , 0.7625 , 0.29375, 0.29375,\n",
       "        0.03125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.975  , 0.975  , 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.975  , 0.73125, 0.3625 , 0.98125,\n",
       "        0.98125, 0.98125, 0.975  , 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.975  , 0.73125,\n",
       "        0.3625 , 0.98125, 0.98125, 0.98125, 0.975  , 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.975  , 0.73125, 0.3625 , 0.98125, 0.98125, 0.98125, 0.975  ,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.975  , 0.70625, 0.35   , 0.98125, 0.98125,\n",
       "        0.98125, 0.975  , 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.73125, 0.29375,\n",
       "        0.98125, 0.98125, 0.98125, 0.975  , 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.975  ,\n",
       "        0.975  , 0.16875, 0.98125, 0.98125, 0.98125, 0.86875, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.975  , 0.98125, 0.98125,\n",
       "        0.98125, 0.975  , 0.675  , 0.275  , 0.98125, 0.98125, 0.94375,\n",
       "        0.9375 , 0.98125, 0.98125, 0.98125, 0.975  , 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.975  , 0.85625, 0.975  , 0.98125,\n",
       "        0.98125, 0.975  , 0.73125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.975  , 0.975  , 0.98125, 0.98125, 0.975  , 0.76875,\n",
       "        0.43125, 0.98125, 0.98125, 0.98125, 0.975  , 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.9625 , 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.975  , 0.96875, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125]),\n",
       " 'split3_test_score': array([0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.76875,\n",
       "        0.76875, 0.6625 , 0.43125, 0.43125, 0.30625, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.96875,\n",
       "        0.98125, 0.98125, 0.9    , 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.9625 , 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.9    , 0.98125, 0.98125, 0.95   , 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.76875, 0.76875,\n",
       "        0.9625 , 0.43125, 0.43125, 0.025  , 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.96875, 0.98125,\n",
       "        0.98125, 0.91875, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.94375, 0.98125, 0.98125, 0.925  ,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.9625 , 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.76875, 0.76875, 0.525  ,\n",
       "        0.43125, 0.43125, 0.125  , 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.94375, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.96875, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.76875, 0.76875, 0.53125, 0.425  ,\n",
       "        0.425  , 0.375  , 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.9    ,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.9625 , 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.775  , 0.775  , 0.775  , 0.38125, 0.38125,\n",
       "        0.55625, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.76875, 0.43125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.76875,\n",
       "        0.43125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.76875, 0.43125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.7875 , 0.4375 , 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.96875, 0.84375, 0.5125 ,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.91875, 0.925  , 0.98125, 0.98125, 0.95625, 0.83125, 0.98125,\n",
       "        0.98125, 0.98125, 0.95   , 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.58125, 0.45625, 0.98125, 0.98125, 0.98125,\n",
       "        0.95625, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.925  , 0.98125, 0.98125, 0.75625, 0.4    , 0.98125,\n",
       "        0.98125, 0.98125, 0.94375, 0.98125, 0.98125, 0.98125, 0.94375,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.575  ,\n",
       "        0.30625, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.7125 , 0.28125, 0.98125, 0.98125, 0.98125, 0.76875,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125]),\n",
       " 'split4_test_score': array([0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.9625 , 0.7    ,\n",
       "        0.7    , 0.13125, 0.2625 , 0.2625 , 0.84375, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.89375,\n",
       "        0.98125, 0.98125, 0.7125 , 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.975  , 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.7    , 0.7    ,\n",
       "        0.04375, 0.2625 , 0.2625 , 0.25   , 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.91875, 0.98125,\n",
       "        0.98125, 0.81875, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.9875 , 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.94375, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.7    , 0.7    , 0.3625 ,\n",
       "        0.2625 , 0.2625 , 0.01875, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.93125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.975  , 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.70625, 0.70625, 0.46875, 0.25625,\n",
       "        0.25625, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.8875 , 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.7375 , 0.7375 , 0.7375 , 0.18125, 0.18125,\n",
       "        0.725  , 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.7    , 0.2625 , 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.7    ,\n",
       "        0.2625 , 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.7    , 0.2625 , 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.7    , 0.21875, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.7125 , 0.08125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.18125, 0.23125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.64375, 0.0125 , 0.98125, 0.98125, 0.9125 ,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.975  , 0.98125, 0.98125, 0.98125, 0.65   , 0.30625, 0.98125,\n",
       "        0.98125, 0.79375, 0.81875, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.975  , 0.98125, 0.98125, 0.83125,\n",
       "        0.68125, 0.98125, 0.98125, 0.98125, 0.9875 , 0.98125, 0.98125,\n",
       "        0.9625 , 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.8875 , 0.01875, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.975  , 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125]),\n",
       " 'mean_test_score': array([0.9825 , 0.9825 , 0.9825 , 0.975  , 0.975  , 0.96125, 0.73875,\n",
       "        0.73875, 0.45   , 0.36125, 0.36125, 0.5425 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.98125, 0.98125, 0.89375,\n",
       "        0.975  , 0.975  , 0.79   , 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.98125, 0.9825 , 0.9825 , 0.9325 , 0.9825 , 0.9825 ,\n",
       "        0.97875, 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.96625, 0.9825 , 0.9825 , 0.97625, 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.975  , 0.975  , 0.94875, 0.73875, 0.73875,\n",
       "        0.55125, 0.36125, 0.36125, 0.515  , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.98125, 0.98125, 0.9225 , 0.975  ,\n",
       "        0.975  , 0.915  , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9525 , 0.9825 , 0.9825 , 0.9475 ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.98   , 0.9825 ,\n",
       "        0.9825 , 0.96125, 0.9825 , 0.9825 , 0.9775 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.975  , 0.975  , 0.97   , 0.74   , 0.74   , 0.43625,\n",
       "        0.36125, 0.36125, 0.12   , 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.98125, 0.98125, 0.9325 , 0.975  , 0.975  ,\n",
       "        0.935  , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9625 , 0.9825 , 0.9825 , 0.97   , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.98   , 0.9825 , 0.9825 ,\n",
       "        0.98125, 0.9825 , 0.9825 , 0.98   , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.97875, 0.97875, 0.97875, 0.74375, 0.74375, 0.54   , 0.3525 ,\n",
       "        0.3525 , 0.55875, 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.98125, 0.98125, 0.95875, 0.975  , 0.975  , 0.955  ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.97125, 0.9825 , 0.9825 , 0.98125, 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.96875,\n",
       "        0.9825 , 0.9825 , 0.97875, 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.76   , 0.76   , 0.76   , 0.29625, 0.29625,\n",
       "        0.55875, 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.97875, 0.97875, 0.98   , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.975  , 0.73875, 0.36125, 0.9825 ,\n",
       "        0.9825 , 0.98125, 0.975  , 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.975  , 0.73875,\n",
       "        0.36125, 0.9825 , 0.9825 , 0.98125, 0.975  , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.975  , 0.74   , 0.3575 , 0.9825 , 0.9825 , 0.98125, 0.975  ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9775 , 0.7375 , 0.33875, 0.9825 , 0.9825 ,\n",
       "        0.98125, 0.975  , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9775 , 0.7575 , 0.29625,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9775 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.97   ,\n",
       "        0.6275 , 0.5125 , 0.9825 , 0.9825 , 0.97375, 0.895  , 0.9825 ,\n",
       "        0.975  , 0.9825 , 0.94375, 0.9825 , 0.98125, 0.95625, 0.9775 ,\n",
       "        0.9825 , 0.975  , 0.585  , 0.19375, 0.9825 , 0.9825 , 0.94125,\n",
       "        0.9325 , 0.9825 , 0.9825 , 0.9775 , 0.95375, 0.9825 , 0.9825 ,\n",
       "        0.98125, 0.96125, 0.9825 , 0.975  , 0.63   , 0.3675 , 0.9825 ,\n",
       "        0.9825 , 0.94125, 0.885  , 0.9825 , 0.9825 , 0.9825 , 0.9675 ,\n",
       "        0.9825 , 0.98125, 0.97375, 0.98125, 0.9825 , 0.97875, 0.60625,\n",
       "        0.28875, 0.9825 , 0.9825 , 0.9825 , 0.97625, 0.9825 , 0.9825 ,\n",
       "        0.97375, 0.97625, 0.9825 , 0.97875, 0.98125, 0.98125, 0.9825 ,\n",
       "        0.9825 , 0.70625, 0.3275 , 0.9825 , 0.9825 , 0.9825 , 0.92375,\n",
       "        0.9825 , 0.9825 , 0.98125, 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 ]),\n",
       " 'std_test_score': array([0.0025    , 0.0025    , 0.0025    , 0.00968246, 0.00968246,\n",
       "        0.02284458, 0.03432383, 0.03432383, 0.21780152, 0.09298857,\n",
       "        0.09298857, 0.3244322 , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.00395285, 0.00395285,\n",
       "        0.08477912, 0.00968246, 0.00968246, 0.13685074, 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.00395285,\n",
       "        0.0025    , 0.0025    , 0.03921097, 0.0025    , 0.0025    ,\n",
       "        0.00847791, 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.03321333,\n",
       "        0.0025    , 0.0025    , 0.01334635, 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.00968246, 0.00968246, 0.04227884, 0.03432383,\n",
       "        0.03432383, 0.37884281, 0.09298857, 0.09298857, 0.35014729,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.00395285, 0.00395285, 0.07711598, 0.00968246,\n",
       "        0.00968246, 0.05326819, 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.03344772, 0.0025    , 0.0025    , 0.02783882, 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.02284458, 0.0025    , 0.0025    ,\n",
       "        0.00847791, 0.0025    , 0.0025    , 0.0025    , 0.00968246,\n",
       "        0.00968246, 0.01952562, 0.03614208, 0.03614208, 0.2535498 ,\n",
       "        0.09298857, 0.09298857, 0.12546165, 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.00395285,\n",
       "        0.00395285, 0.07053368, 0.00968246, 0.00968246, 0.06093029,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0205396 , 0.0025    ,\n",
       "        0.0025    , 0.02284458, 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.        , 0.0025    , 0.0025    , 0.00612372, 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.00306186, 0.00306186, 0.00306186,\n",
       "        0.03557562, 0.03557562, 0.23922401, 0.09122431, 0.09122431,\n",
       "        0.35515402, 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.00395285, 0.00395285, 0.0363576 ,\n",
       "        0.00968246, 0.00968246, 0.03566336, 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.02      , 0.0025    , 0.0025    , 0.00395285,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.01045825, 0.0025    ,\n",
       "        0.0025    , 0.00847791, 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0266927 , 0.0266927 ,\n",
       "        0.0266927 , 0.1052972 , 0.1052972 , 0.31236497, 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.00306186, 0.00306186,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.00968246, 0.03432383, 0.09298857, 0.0025    ,\n",
       "        0.0025    , 0.00395285, 0.00968246, 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.00968246, 0.03432383, 0.09298857,\n",
       "        0.0025    , 0.0025    , 0.00395285, 0.00968246, 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.00968246, 0.03321333,\n",
       "        0.09094985, 0.0025    , 0.0025    , 0.00395285, 0.00968246,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.005     ,\n",
       "        0.03852759, 0.10543066, 0.0025    , 0.0025    , 0.00395285,\n",
       "        0.00968246, 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0075    , 0.06559821, 0.16958589, 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.005     , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.01952562, 0.31128765, 0.36385523, 0.0025    ,\n",
       "        0.0025    , 0.0121192 , 0.07566373, 0.0025    , 0.0125    ,\n",
       "        0.0025    , 0.0638602 , 0.0025    , 0.00395285, 0.05318012,\n",
       "        0.01089725, 0.0025    , 0.00968246, 0.07055583, 0.16545392,\n",
       "        0.0025    , 0.0025    , 0.03414125, 0.05868347, 0.0025    ,\n",
       "        0.0025    , 0.0075    , 0.05193145, 0.0025    , 0.0025    ,\n",
       "        0.00395285, 0.02284458, 0.0025    , 0.00968246, 0.20590501,\n",
       "        0.32904692, 0.0025    , 0.0025    , 0.07380295, 0.09532838,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.01952562, 0.0025    ,\n",
       "        0.00395285, 0.0121192 , 0.00395285, 0.0025    , 0.00306186,\n",
       "        0.20244598, 0.25588083, 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.01075291, 0.0025    , 0.0025    , 0.0121192 , 0.00612372,\n",
       "        0.0025    , 0.00847791, 0.00395285, 0.        , 0.0025    ,\n",
       "        0.0025    , 0.20478647, 0.33226495, 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.08398289, 0.0025    , 0.0025    , 0.00395285,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ]),\n",
       " 'rank_test_score': array([  1,   1,   1, 292, 292, 327, 360, 360, 379, 382, 382, 375,   1,\n",
       "          1,   1,   1,   1,   1, 248, 248, 348, 292, 292, 350,   1,   1,\n",
       "          1,   1,   1, 248,   1,   1, 342,   1,   1, 274,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1, 325,   1,   1, 289,   1,   1,   1, 292,\n",
       "        292, 335, 360, 360, 374, 382, 382, 377,   1,   1,   1,   1,   1,\n",
       "          1, 248, 248, 345, 292, 292, 346,   1,   1,   1,   1,   1, 247,\n",
       "          1,   1, 334,   1,   1, 336,   1,   1,   1,   1,   1, 270,   1,\n",
       "          1, 329,   1,   1, 283,   1,   1,   1, 292, 292, 320, 357, 357,\n",
       "        380, 382, 382, 400,   1,   1,   1,   1,   1,   1, 248, 248, 342,\n",
       "        292, 292, 340,   1,   1,   1,   1,   1,   1,   1,   1, 326,   1,\n",
       "          1, 320,   1,   1,   1,   1,   1, 270,   1,   1, 248,   1,   1,\n",
       "        270,   1,   1,   1, 275, 275, 275, 355, 355, 376, 391, 391, 372,\n",
       "          1,   1,   1,   1,   1,   1, 248, 248, 330, 292, 292, 332,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1, 319,   1,   1, 248,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1, 323,   1,   1, 275,   1,   1,   1,\n",
       "          1,   1,   1, 351, 351, 351, 396, 396, 372,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1, 275, 275, 270,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1, 292, 360, 382,   1,   1, 248,\n",
       "        292,   1,   1,   1,   1,   1,   1,   1,   1,   1, 292, 360, 382,\n",
       "          1,   1, 248, 292,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "        292, 357, 390,   1,   1, 248, 292,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1, 283, 366, 393,   1,   1, 248, 292,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1, 283, 354, 395,   1,   1,   1, 283,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1, 320, 369, 378,   1,\n",
       "          1, 316, 347,   1, 292,   1, 337,   1, 248, 331, 283,   1, 292,\n",
       "        371, 399,   1,   1, 338, 341,   1,   1, 283, 333,   1,   1, 248,\n",
       "        327,   1, 292, 368, 381,   1,   1, 338, 349,   1,   1,   1, 324,\n",
       "          1, 248, 316, 248,   1, 275, 370, 398,   1,   1,   1, 291,   1,\n",
       "          1, 316, 289,   1, 275, 248, 248,   1,   1, 367, 394,   1,   1,\n",
       "          1, 344,   1,   1, 248,   1,   1,   1,   1,   1])}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = grid.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.995\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       200\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99       200\n",
      "   macro avg       0.50      0.50      0.50       200\n",
      "weighted avg       1.00      0.99      1.00       200\n",
      "\n",
      "[[199   1]\n",
      " [  0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "score = accuracy_score(y_pred, y_test)\n",
    "print(score)\n",
    "print(classification_report(y_pred, y_test))\n",
    "print(confusion_matrix(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logestic Regression with ROC curvw and ROC AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc curve and auc\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 2 class dataset\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,y,test_size=0.10,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Created a dummy model with default 0 as output \n",
    "dummy_model_prob = [0 for _ in range(len(y_test))]\n",
    "dummy_model_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" checked><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prob = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99816799, 0.00183201],\n",
       "       [0.98734262, 0.01265738],\n",
       "       [0.36096379, 0.63903621],\n",
       "       [0.36425511, 0.63574489],\n",
       "       [0.22610271, 0.77389729],\n",
       "       [0.8844746 , 0.1155254 ],\n",
       "       [0.74477363, 0.25522637],\n",
       "       [0.95061624, 0.04938376],\n",
       "       [0.35988183, 0.64011817],\n",
       "       [0.54634991, 0.45365009],\n",
       "       [0.07597853, 0.92402147],\n",
       "       [0.71301873, 0.28698127],\n",
       "       [0.21791542, 0.78208458],\n",
       "       [0.21712424, 0.78287576],\n",
       "       [0.92675981, 0.07324019],\n",
       "       [0.9928822 , 0.0071178 ],\n",
       "       [0.7835632 , 0.2164368 ],\n",
       "       [0.48799541, 0.51200459],\n",
       "       [0.15009711, 0.84990289],\n",
       "       [0.01457321, 0.98542679],\n",
       "       [0.05268202, 0.94731798],\n",
       "       [0.92843353, 0.07156647],\n",
       "       [0.95386124, 0.04613876],\n",
       "       [0.27667368, 0.72332632],\n",
       "       [0.08454816, 0.91545184],\n",
       "       [0.30400332, 0.69599668],\n",
       "       [0.18623319, 0.81376681],\n",
       "       [0.99685816, 0.00314184],\n",
       "       [0.29257466, 0.70742534],\n",
       "       [0.26849299, 0.73150701],\n",
       "       [0.01738002, 0.98261998],\n",
       "       [0.64895944, 0.35104056],\n",
       "       [0.01968748, 0.98031252],\n",
       "       [0.79117036, 0.20882964],\n",
       "       [0.99495306, 0.00504694],\n",
       "       [0.95701703, 0.04298297],\n",
       "       [0.00151535, 0.99848465],\n",
       "       [0.71059536, 0.28940464],\n",
       "       [0.15573224, 0.84426776],\n",
       "       [0.15826821, 0.84173179],\n",
       "       [0.28035624, 0.71964376],\n",
       "       [0.60792132, 0.39207868],\n",
       "       [0.99552268, 0.00447732],\n",
       "       [0.96776637, 0.03223363],\n",
       "       [0.76018604, 0.23981396],\n",
       "       [0.90527131, 0.09472869],\n",
       "       [0.96690599, 0.03309401],\n",
       "       [0.40125381, 0.59874619],\n",
       "       [0.98624051, 0.01375949],\n",
       "       [0.21421474, 0.78578526],\n",
       "       [0.99413727, 0.00586273],\n",
       "       [0.92258072, 0.07741928],\n",
       "       [0.34120074, 0.65879926],\n",
       "       [0.11850741, 0.88149259],\n",
       "       [0.57205622, 0.42794378],\n",
       "       [0.99886904, 0.00113096],\n",
       "       [0.91801341, 0.08198659],\n",
       "       [0.13446449, 0.86553551],\n",
       "       [0.7722632 , 0.2277368 ],\n",
       "       [0.03822819, 0.96177181],\n",
       "       [0.7095021 , 0.2904979 ],\n",
       "       [0.82355309, 0.17644691],\n",
       "       [0.21853391, 0.78146609],\n",
       "       [0.96292992, 0.03707008],\n",
       "       [0.58771342, 0.41228658],\n",
       "       [0.95008493, 0.04991507],\n",
       "       [0.93469348, 0.06530652],\n",
       "       [0.47954625, 0.52045375],\n",
       "       [0.8252984 , 0.1747016 ],\n",
       "       [0.25387834, 0.74612166],\n",
       "       [0.01638893, 0.98361107],\n",
       "       [0.76745731, 0.23254269],\n",
       "       [0.00211978, 0.99788022],\n",
       "       [0.17744743, 0.82255257],\n",
       "       [0.12328052, 0.87671948],\n",
       "       [0.09054133, 0.90945867],\n",
       "       [0.99732531, 0.00267469],\n",
       "       [0.95727532, 0.04272468],\n",
       "       [0.55089646, 0.44910354],\n",
       "       [0.97223817, 0.02776183],\n",
       "       [0.34356137, 0.65643863],\n",
       "       [0.95748322, 0.04251678],\n",
       "       [0.71678005, 0.28321995],\n",
       "       [0.13548662, 0.86451338],\n",
       "       [0.97338137, 0.02661863],\n",
       "       [0.03421868, 0.96578132],\n",
       "       [0.95998174, 0.04001826],\n",
       "       [0.02263212, 0.97736788],\n",
       "       [0.69993285, 0.30006715],\n",
       "       [0.00604386, 0.99395614],\n",
       "       [0.92577555, 0.07422445],\n",
       "       [0.06678638, 0.93321362],\n",
       "       [0.02102623, 0.97897377],\n",
       "       [0.08900545, 0.91099455],\n",
       "       [0.00720395, 0.99279605],\n",
       "       [0.15727942, 0.84272058],\n",
       "       [0.78006609, 0.21993391],\n",
       "       [0.9586498 , 0.0413502 ],\n",
       "       [0.92416694, 0.07583306],\n",
       "       [0.09990044, 0.90009956]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets focus on the postieve outcome\n",
    "model_prob = model_prob[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.9017857142857144\n"
     ]
    }
   ],
   "source": [
    "#Lets calculate the score\n",
    "dummy_model_auc=roc_auc_score(y_test,dummy_model_prob)\n",
    "model_auc=roc_auc_score(y_test,model_prob)\n",
    "print(dummy_model_auc)\n",
    "print(model_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate ROC Curves\n",
    "dummy_fpr, dummy_tpr, _ = roc_curve(y_test, dummy_model_prob)\n",
    "model_fpr, model_tpr, thresholds = roc_curve(y_test, model_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.99848465e+00, 9.98484650e-01, 9.85426791e-01, 9.83611075e-01,\n",
       "       8.65535511e-01, 8.64513382e-01, 8.13766815e-01, 7.85785262e-01,\n",
       "       7.81466094e-01, 7.31507007e-01, 7.07425343e-01, 6.95996680e-01,\n",
       "       6.35744891e-01, 5.98746190e-01, 5.20453749e-01, 4.49103540e-01,\n",
       "       4.27943778e-01, 2.19933909e-01, 2.16436802e-01, 2.08829643e-01,\n",
       "       1.76446911e-01, 7.58330569e-02, 7.42244496e-02, 4.47731532e-03,\n",
       "       3.14184433e-03, 1.13095978e-03])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.        , 0.        , 0.        , 0.01785714, 0.01785714,\n",
       "        0.03571429, 0.03571429, 0.05357143, 0.05357143, 0.10714286,\n",
       "        0.10714286, 0.125     , 0.125     , 0.14285714, 0.14285714,\n",
       "        0.19642857, 0.19642857, 0.42857143, 0.42857143, 0.44642857,\n",
       "        0.44642857, 0.55357143, 0.55357143, 0.94642857, 0.94642857,\n",
       "        1.        ]),\n",
       " array([0.        , 0.02272727, 0.11363636, 0.11363636, 0.47727273,\n",
       "        0.47727273, 0.61363636, 0.61363636, 0.68181818, 0.68181818,\n",
       "        0.75      , 0.75      , 0.86363636, 0.86363636, 0.88636364,\n",
       "        0.88636364, 0.90909091, 0.90909091, 0.93181818, 0.93181818,\n",
       "        0.95454545, 0.95454545, 0.97727273, 0.97727273, 1.        ,\n",
       "        1.        ]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fpr,model_tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(dummy_fpr, dummy_tpr, linestyle='--', label='Dummy Model')\n",
    "pyplot.plot(model_fpr, model_tpr, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x5000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the roc curve for the model\n",
    "import numpy as np\n",
    "fig = pyplot.figure(figsize=(20,50))\n",
    "pyplot.plot(dummy_fpr, dummy_tpr, linestyle='--', label='Dummy Model')\n",
    "pyplot.plot(model_fpr, model_tpr, marker='.', label='Logistic')\n",
    "ax = fig.add_subplot(111)\n",
    "for xyz in zip(model_fpr, model_tpr,thresholds):   \n",
    "    ax.annotate('%s' % np.round(xyz[2],2), xy=(xyz[0],xyz[1]))\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
