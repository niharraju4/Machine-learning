{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nihar Muniraju"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Load Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3779567 ,  1.04389498,  1.04349443, ..., -0.0671922 ,\n",
       "         0.17547148, -1.04964564],\n",
       "       [-0.32525851,  1.27626282, -0.68612327, ...,  1.00663329,\n",
       "        -0.83369182,  0.95774417],\n",
       "       [ 0.73901891, -0.60090284, -0.17729436, ..., -0.21898072,\n",
       "         0.87864296, -1.25774001],\n",
       "       ...,\n",
       "       [ 0.67556288, -0.53841971, -1.29950008, ...,  2.04333597,\n",
       "         0.94738793,  0.79035376],\n",
       "       [ 2.62971021, -2.45289885, -1.35978523, ...,  0.37889809,\n",
       "        -1.97189411, -0.2522504 ],\n",
       "       [-1.79149103, -0.12190773,  0.53515332, ..., -1.94135733,\n",
       "         0.58900166, -1.00748218]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting the data to data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.377957</td>\n",
       "      <td>1.043895</td>\n",
       "      <td>1.043494</td>\n",
       "      <td>-0.101838</td>\n",
       "      <td>-1.617442</td>\n",
       "      <td>0.402713</td>\n",
       "      <td>0.913601</td>\n",
       "      <td>-0.067192</td>\n",
       "      <td>0.175471</td>\n",
       "      <td>-1.049646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.325259</td>\n",
       "      <td>1.276263</td>\n",
       "      <td>-0.686123</td>\n",
       "      <td>-2.463205</td>\n",
       "      <td>-0.489426</td>\n",
       "      <td>-0.240715</td>\n",
       "      <td>-1.469496</td>\n",
       "      <td>1.006633</td>\n",
       "      <td>-0.833692</td>\n",
       "      <td>0.957744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.739019</td>\n",
       "      <td>-0.600903</td>\n",
       "      <td>-0.177294</td>\n",
       "      <td>1.335714</td>\n",
       "      <td>-0.817332</td>\n",
       "      <td>-0.790047</td>\n",
       "      <td>1.457365</td>\n",
       "      <td>-0.218981</td>\n",
       "      <td>0.878643</td>\n",
       "      <td>-1.257740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.474312</td>\n",
       "      <td>-1.103002</td>\n",
       "      <td>1.189936</td>\n",
       "      <td>-0.800186</td>\n",
       "      <td>0.912377</td>\n",
       "      <td>-0.406451</td>\n",
       "      <td>-1.130950</td>\n",
       "      <td>1.985111</td>\n",
       "      <td>1.379029</td>\n",
       "      <td>1.041768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.927365</td>\n",
       "      <td>1.114796</td>\n",
       "      <td>0.080284</td>\n",
       "      <td>1.261064</td>\n",
       "      <td>0.761179</td>\n",
       "      <td>0.921563</td>\n",
       "      <td>0.440832</td>\n",
       "      <td>0.184645</td>\n",
       "      <td>-1.567739</td>\n",
       "      <td>-0.142107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1.538272</td>\n",
       "      <td>0.171629</td>\n",
       "      <td>0.075371</td>\n",
       "      <td>-0.957658</td>\n",
       "      <td>-1.066219</td>\n",
       "      <td>1.158096</td>\n",
       "      <td>-0.036964</td>\n",
       "      <td>0.123689</td>\n",
       "      <td>0.927871</td>\n",
       "      <td>-0.225003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-0.060266</td>\n",
       "      <td>0.095018</td>\n",
       "      <td>-0.271685</td>\n",
       "      <td>1.830560</td>\n",
       "      <td>0.219445</td>\n",
       "      <td>-0.341269</td>\n",
       "      <td>1.180088</td>\n",
       "      <td>-0.216876</td>\n",
       "      <td>-1.752938</td>\n",
       "      <td>-0.810152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.675563</td>\n",
       "      <td>-0.538420</td>\n",
       "      <td>-1.299500</td>\n",
       "      <td>0.747835</td>\n",
       "      <td>1.733898</td>\n",
       "      <td>-0.268044</td>\n",
       "      <td>-0.520953</td>\n",
       "      <td>2.043336</td>\n",
       "      <td>0.947388</td>\n",
       "      <td>0.790354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>2.629710</td>\n",
       "      <td>-2.452899</td>\n",
       "      <td>-1.359785</td>\n",
       "      <td>1.592065</td>\n",
       "      <td>0.854157</td>\n",
       "      <td>1.618828</td>\n",
       "      <td>0.621701</td>\n",
       "      <td>0.378898</td>\n",
       "      <td>-1.971894</td>\n",
       "      <td>-0.252250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-1.791491</td>\n",
       "      <td>-0.121908</td>\n",
       "      <td>0.535153</td>\n",
       "      <td>-0.588085</td>\n",
       "      <td>-1.929461</td>\n",
       "      <td>-0.659900</td>\n",
       "      <td>0.754921</td>\n",
       "      <td>-1.941357</td>\n",
       "      <td>0.589002</td>\n",
       "      <td>-1.007482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2  ...         7         8         9\n",
       "0   -0.377957  1.043895  1.043494  ... -0.067192  0.175471 -1.049646\n",
       "1   -0.325259  1.276263 -0.686123  ...  1.006633 -0.833692  0.957744\n",
       "2    0.739019 -0.600903 -0.177294  ... -0.218981  0.878643 -1.257740\n",
       "3    0.474312 -1.103002  1.189936  ...  1.985111  1.379029  1.041768\n",
       "4    0.927365  1.114796  0.080284  ...  0.184645 -1.567739 -0.142107\n",
       "..        ...       ...       ...  ...       ...       ...       ...\n",
       "995  1.538272  0.171629  0.075371  ...  0.123689  0.927871 -0.225003\n",
       "996 -0.060266  0.095018 -0.271685  ... -0.216876 -1.752938 -0.810152\n",
       "997  0.675563 -0.538420 -1.299500  ...  2.043336  0.947388  0.790354\n",
       "998  2.629710 -2.452899 -1.359785  ...  0.378898 -1.971894 -0.252250\n",
       "999 -1.791491 -0.121908  0.535153  ... -1.941357  0.589002 -1.007482\n",
       "\n",
       "[1000 rows x 10 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 1, 0, 0, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test  = train_test_split(X,y, test_size=0.30,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implement logestic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = LogisticRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" checked><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "       0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = log.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.33104089e-01, 6.68959106e-02],\n",
       "       [6.59976832e-01, 3.40023168e-01],\n",
       "       [8.65439561e-01, 1.34560439e-01],\n",
       "       [9.59081747e-02, 9.04091825e-01],\n",
       "       [2.43108150e-01, 7.56891850e-01],\n",
       "       [5.76396931e-01, 4.23603069e-01],\n",
       "       [8.25188888e-01, 1.74811112e-01],\n",
       "       [2.29999952e-04, 9.99770000e-01],\n",
       "       [1.25363187e-02, 9.87463681e-01],\n",
       "       [2.73346597e-03, 9.97266534e-01],\n",
       "       [7.06227890e-03, 9.92937721e-01],\n",
       "       [5.68134296e-01, 4.31865704e-01],\n",
       "       [8.38796778e-02, 9.16120322e-01],\n",
       "       [9.73835584e-01, 2.61644162e-02],\n",
       "       [1.27501162e-02, 9.87249884e-01],\n",
       "       [8.06120072e-01, 1.93879928e-01],\n",
       "       [9.95125747e-01, 4.87425270e-03],\n",
       "       [1.78699988e-02, 9.82130001e-01],\n",
       "       [9.78717701e-01, 2.12822989e-02],\n",
       "       [3.90303620e-02, 9.60969638e-01],\n",
       "       [3.39139480e-02, 9.66086052e-01],\n",
       "       [3.13743686e-02, 9.68625631e-01],\n",
       "       [5.00274883e-02, 9.49972512e-01],\n",
       "       [8.18573651e-01, 1.81426349e-01],\n",
       "       [8.59038470e-01, 1.40961530e-01],\n",
       "       [9.12460521e-01, 8.75394788e-02],\n",
       "       [8.53285450e-01, 1.46714550e-01],\n",
       "       [8.63538338e-01, 1.36461662e-01],\n",
       "       [7.14069794e-01, 2.85930206e-01],\n",
       "       [1.50110810e-01, 8.49889190e-01],\n",
       "       [4.25748098e-04, 9.99574252e-01],\n",
       "       [6.18630454e-01, 3.81369546e-01],\n",
       "       [2.02970365e-02, 9.79702963e-01],\n",
       "       [4.31674190e-01, 5.68325810e-01],\n",
       "       [1.32602897e-04, 9.99867397e-01],\n",
       "       [9.79277212e-01, 2.07227884e-02],\n",
       "       [7.91505067e-05, 9.99920849e-01],\n",
       "       [2.79028629e-01, 7.20971371e-01],\n",
       "       [7.50191140e-01, 2.49808860e-01],\n",
       "       [9.78914019e-01, 2.10859811e-02],\n",
       "       [9.29298742e-01, 7.07012581e-02],\n",
       "       [5.07627084e-01, 4.92372916e-01],\n",
       "       [7.86334536e-01, 2.13665464e-01],\n",
       "       [1.02170326e-02, 9.89782967e-01],\n",
       "       [6.71001381e-01, 3.28998619e-01],\n",
       "       [2.28316835e-02, 9.77168316e-01],\n",
       "       [9.56906175e-04, 9.99043094e-01],\n",
       "       [2.80866655e-04, 9.99719133e-01],\n",
       "       [2.35383593e-04, 9.99764616e-01],\n",
       "       [5.95639599e-01, 4.04360401e-01],\n",
       "       [2.24986821e-01, 7.75013179e-01],\n",
       "       [1.87963545e-06, 9.99998120e-01],\n",
       "       [3.76191758e-03, 9.96238082e-01],\n",
       "       [1.47229167e-03, 9.98527708e-01],\n",
       "       [2.71064330e-02, 9.72893567e-01],\n",
       "       [1.06485998e-02, 9.89351400e-01],\n",
       "       [4.95368676e-01, 5.04631324e-01],\n",
       "       [9.70585084e-01, 2.94149162e-02],\n",
       "       [4.88523232e-02, 9.51147677e-01],\n",
       "       [5.89167160e-01, 4.10832840e-01],\n",
       "       [3.61611389e-01, 6.38388611e-01],\n",
       "       [9.39498551e-01, 6.05014489e-02],\n",
       "       [9.84757455e-01, 1.52425455e-02],\n",
       "       [6.89700912e-01, 3.10299088e-01],\n",
       "       [1.37200997e-03, 9.98627990e-01],\n",
       "       [1.09949522e-05, 9.99989005e-01],\n",
       "       [7.17407766e-01, 2.82592234e-01],\n",
       "       [9.33068344e-01, 6.69316556e-02],\n",
       "       [1.27246555e-02, 9.87275344e-01],\n",
       "       [9.95912281e-01, 4.08771888e-03],\n",
       "       [9.45014768e-01, 5.49852320e-02],\n",
       "       [9.72505405e-01, 2.74945952e-02],\n",
       "       [1.57767118e-02, 9.84223288e-01],\n",
       "       [7.02089863e-01, 2.97910137e-01],\n",
       "       [3.78336641e-03, 9.96216634e-01],\n",
       "       [7.88166769e-01, 2.11833231e-01],\n",
       "       [9.76950585e-03, 9.90230494e-01],\n",
       "       [9.08319915e-01, 9.16800853e-02],\n",
       "       [9.97668337e-01, 2.33166339e-03],\n",
       "       [9.78676555e-01, 2.13234453e-02],\n",
       "       [9.97859196e-01, 2.14080396e-03],\n",
       "       [9.62738467e-01, 3.72615329e-02],\n",
       "       [6.87123257e-01, 3.12876743e-01],\n",
       "       [9.65470028e-01, 3.45299723e-02],\n",
       "       [4.09424875e-03, 9.95905751e-01],\n",
       "       [6.80206407e-01, 3.19793593e-01],\n",
       "       [2.00265911e-02, 9.79973409e-01],\n",
       "       [9.70587240e-01, 2.94127604e-02],\n",
       "       [7.63313709e-01, 2.36686291e-01],\n",
       "       [5.93516932e-05, 9.99940648e-01],\n",
       "       [2.71967499e-03, 9.97280325e-01],\n",
       "       [1.16719400e-02, 9.88328060e-01],\n",
       "       [9.76638433e-01, 2.33615675e-02],\n",
       "       [1.90773686e-04, 9.99809226e-01],\n",
       "       [2.75472277e-01, 7.24527723e-01],\n",
       "       [2.66885863e-01, 7.33114137e-01],\n",
       "       [6.31794396e-01, 3.68205604e-01],\n",
       "       [2.23108158e-01, 7.76891842e-01],\n",
       "       [1.49891897e-04, 9.99850108e-01],\n",
       "       [5.41255917e-03, 9.94587441e-01],\n",
       "       [1.99221339e-01, 8.00778661e-01],\n",
       "       [8.53091208e-01, 1.46908792e-01],\n",
       "       [9.63745661e-01, 3.62543392e-02],\n",
       "       [9.68858604e-01, 3.11413963e-02],\n",
       "       [9.62889671e-01, 3.71103286e-02],\n",
       "       [9.90852701e-01, 9.14729857e-03],\n",
       "       [1.83724160e-02, 9.81627584e-01],\n",
       "       [9.52934925e-01, 4.70650748e-02],\n",
       "       [9.79501310e-01, 2.04986896e-02],\n",
       "       [9.74838735e-01, 2.51612650e-02],\n",
       "       [8.64561278e-04, 9.99135439e-01],\n",
       "       [2.63635565e-04, 9.99736364e-01],\n",
       "       [7.15816922e-01, 2.84183078e-01],\n",
       "       [5.20102473e-01, 4.79897527e-01],\n",
       "       [8.22118465e-01, 1.77881535e-01],\n",
       "       [9.93687723e-01, 6.31227662e-03],\n",
       "       [7.01689887e-01, 2.98310113e-01],\n",
       "       [4.08054578e-03, 9.95919454e-01],\n",
       "       [8.82580348e-01, 1.17419652e-01],\n",
       "       [9.18167868e-01, 8.18321316e-02],\n",
       "       [2.04191995e-01, 7.95808005e-01],\n",
       "       [8.30915134e-01, 1.69084866e-01],\n",
       "       [9.50759759e-01, 4.92402408e-02],\n",
       "       [9.95188524e-01, 4.81147615e-03],\n",
       "       [9.98081563e-01, 1.91843692e-03],\n",
       "       [9.94720644e-01, 5.27935599e-03],\n",
       "       [7.86910251e-01, 2.13089749e-01],\n",
       "       [9.59328264e-01, 4.06717360e-02],\n",
       "       [9.60584999e-01, 3.94150010e-02],\n",
       "       [2.78162831e-02, 9.72183717e-01],\n",
       "       [4.83504710e-01, 5.16495290e-01],\n",
       "       [9.76313056e-01, 2.36869436e-02],\n",
       "       [5.29536484e-03, 9.94704635e-01],\n",
       "       [8.95190218e-01, 1.04809782e-01],\n",
       "       [9.78725579e-01, 2.12744207e-02],\n",
       "       [5.70424328e-04, 9.99429576e-01],\n",
       "       [6.06597095e-04, 9.99393403e-01],\n",
       "       [9.97063519e-01, 2.93648136e-03],\n",
       "       [9.96770635e-01, 3.22936499e-03],\n",
       "       [2.80932364e-04, 9.99719068e-01],\n",
       "       [4.36681854e-02, 9.56331815e-01],\n",
       "       [3.67577620e-02, 9.63242238e-01],\n",
       "       [7.64929290e-02, 9.23507071e-01],\n",
       "       [9.99111854e-01, 8.88145827e-04],\n",
       "       [9.21724541e-01, 7.82754591e-02],\n",
       "       [8.47470908e-01, 1.52529092e-01],\n",
       "       [3.52211622e-03, 9.96477884e-01],\n",
       "       [1.99963628e-02, 9.80003637e-01],\n",
       "       [8.87815624e-01, 1.12184376e-01],\n",
       "       [2.04900489e-04, 9.99795100e-01],\n",
       "       [9.69272062e-01, 3.07279383e-02],\n",
       "       [5.93415564e-01, 4.06584436e-01],\n",
       "       [4.12434381e-01, 5.87565619e-01],\n",
       "       [9.62232914e-01, 3.77670862e-02],\n",
       "       [9.06571259e-01, 9.34287406e-02],\n",
       "       [9.68115138e-01, 3.18848620e-02],\n",
       "       [5.57913049e-02, 9.44208695e-01],\n",
       "       [1.71187994e-04, 9.99828812e-01],\n",
       "       [9.59561695e-01, 4.04383046e-02],\n",
       "       [4.27143830e-02, 9.57285617e-01],\n",
       "       [7.42046755e-01, 2.57953245e-01],\n",
       "       [9.97281089e-01, 2.71891147e-03],\n",
       "       [7.56358109e-01, 2.43641891e-01],\n",
       "       [1.43801088e-02, 9.85619891e-01],\n",
       "       [5.35312556e-01, 4.64687444e-01],\n",
       "       [4.45378258e-03, 9.95546217e-01],\n",
       "       [1.60029587e-01, 8.39970413e-01],\n",
       "       [9.38429814e-01, 6.15701856e-02],\n",
       "       [9.30104103e-01, 6.98958975e-02],\n",
       "       [7.97155690e-02, 9.20284431e-01],\n",
       "       [9.80799507e-01, 1.92004933e-02],\n",
       "       [3.40113855e-03, 9.96598861e-01],\n",
       "       [3.85767933e-03, 9.96142321e-01],\n",
       "       [9.23872609e-01, 7.61273909e-02],\n",
       "       [2.48244000e-03, 9.97517560e-01],\n",
       "       [1.42590198e-01, 8.57409802e-01],\n",
       "       [7.71857139e-01, 2.28142861e-01],\n",
       "       [7.15361824e-01, 2.84638176e-01],\n",
       "       [7.95802216e-03, 9.92041978e-01],\n",
       "       [2.90672971e-03, 9.97093270e-01],\n",
       "       [9.62804022e-01, 3.71959777e-02],\n",
       "       [3.95173272e-06, 9.99996048e-01],\n",
       "       [7.46521232e-01, 2.53478768e-01],\n",
       "       [2.68377139e-01, 7.31622861e-01],\n",
       "       [9.08001425e-01, 9.19985751e-02],\n",
       "       [6.29221189e-01, 3.70778811e-01],\n",
       "       [1.73092334e-04, 9.99826908e-01],\n",
       "       [2.88878674e-01, 7.11121326e-01],\n",
       "       [8.70348991e-01, 1.29651009e-01],\n",
       "       [4.65501083e-02, 9.53449892e-01],\n",
       "       [2.69925625e-04, 9.99730074e-01],\n",
       "       [9.18821208e-03, 9.90811788e-01],\n",
       "       [1.84058415e-05, 9.99981594e-01],\n",
       "       [9.21585305e-01, 7.84146951e-02],\n",
       "       [1.66148452e-01, 8.33851548e-01],\n",
       "       [4.28792808e-02, 9.57120719e-01],\n",
       "       [3.24218883e-01, 6.75781117e-01],\n",
       "       [2.88812742e-03, 9.97111873e-01],\n",
       "       [2.34274548e-03, 9.97657255e-01],\n",
       "       [5.66510205e-01, 4.33489795e-01],\n",
       "       [8.64460483e-02, 9.13553952e-01],\n",
       "       [1.63989586e-01, 8.36010414e-01],\n",
       "       [9.73141580e-01, 2.68584199e-02],\n",
       "       [1.48492890e-03, 9.98515071e-01],\n",
       "       [9.76142249e-01, 2.38577513e-02],\n",
       "       [9.20183372e-01, 7.98166284e-02],\n",
       "       [1.27799670e-04, 9.99872200e-01],\n",
       "       [8.14616098e-01, 1.85383902e-01],\n",
       "       [6.57619511e-02, 9.34238049e-01],\n",
       "       [8.05506424e-01, 1.94493576e-01],\n",
       "       [3.16841231e-02, 9.68315877e-01],\n",
       "       [3.85311660e-02, 9.61468834e-01],\n",
       "       [9.69954697e-01, 3.00453026e-02],\n",
       "       [6.65986768e-03, 9.93340132e-01],\n",
       "       [6.58151073e-01, 3.41848927e-01],\n",
       "       [4.54368859e-01, 5.45631141e-01],\n",
       "       [9.73084127e-01, 2.69158732e-02],\n",
       "       [9.90303001e-01, 9.69699858e-03],\n",
       "       [8.65783205e-01, 1.34216795e-01],\n",
       "       [7.15540687e-01, 2.84459313e-01],\n",
       "       [6.21203632e-01, 3.78796368e-01],\n",
       "       [9.14608618e-01, 8.53913819e-02],\n",
       "       [7.63263535e-01, 2.36736465e-01],\n",
       "       [1.27328969e-03, 9.98726710e-01],\n",
       "       [9.56775002e-01, 4.32249985e-02],\n",
       "       [8.81796794e-01, 1.18203206e-01],\n",
       "       [9.54741311e-01, 4.52586890e-02],\n",
       "       [9.37675309e-01, 6.23246915e-02],\n",
       "       [9.04350122e-01, 9.56498784e-02],\n",
       "       [4.36825205e-02, 9.56317480e-01],\n",
       "       [9.37566377e-01, 6.24336232e-02],\n",
       "       [2.67935975e-01, 7.32064025e-01],\n",
       "       [9.88332119e-01, 1.16678810e-02],\n",
       "       [9.37357607e-01, 6.26423933e-02],\n",
       "       [1.98870091e-01, 8.01129909e-01],\n",
       "       [7.53193594e-01, 2.46806406e-01],\n",
       "       [5.64637682e-05, 9.99943536e-01],\n",
       "       [2.35669383e-03, 9.97643306e-01],\n",
       "       [9.63223386e-01, 3.67766138e-02],\n",
       "       [9.91344273e-01, 8.65572690e-03],\n",
       "       [2.79414842e-02, 9.72058516e-01],\n",
       "       [1.73663772e-02, 9.82633623e-01],\n",
       "       [1.97654982e-05, 9.99980235e-01],\n",
       "       [9.97873229e-01, 2.12677091e-03],\n",
       "       [5.38568985e-02, 9.46143101e-01],\n",
       "       [1.14208934e-02, 9.88579107e-01],\n",
       "       [9.96664447e-01, 3.33555343e-03],\n",
       "       [9.34325785e-01, 6.56742147e-02],\n",
       "       [8.33456753e-05, 9.99916654e-01],\n",
       "       [4.38586824e-02, 9.56141318e-01],\n",
       "       [8.78527383e-01, 1.21472617e-01],\n",
       "       [1.02262699e-01, 8.97737301e-01],\n",
       "       [1.39702455e-01, 8.60297545e-01],\n",
       "       [7.38438745e-01, 2.61561255e-01],\n",
       "       [9.96218361e-01, 3.78163937e-03],\n",
       "       [2.96903603e-01, 7.03096397e-01],\n",
       "       [8.92354714e-01, 1.07645286e-01],\n",
       "       [9.84488536e-01, 1.55114638e-02],\n",
       "       [1.16290641e-04, 9.99883709e-01],\n",
       "       [9.80018027e-01, 1.99819730e-02],\n",
       "       [8.03911966e-01, 1.96088034e-01],\n",
       "       [1.38054725e-03, 9.98619453e-01],\n",
       "       [7.84488886e-01, 2.15511114e-01],\n",
       "       [1.03611401e-03, 9.98963886e-01],\n",
       "       [7.02865763e-01, 2.97134237e-01],\n",
       "       [6.23112023e-03, 9.93768880e-01],\n",
       "       [8.62299841e-01, 1.37700159e-01],\n",
       "       [9.53531098e-01, 4.64689022e-02],\n",
       "       [2.24165759e-01, 7.75834241e-01],\n",
       "       [2.89736651e-01, 7.10263349e-01],\n",
       "       [7.70379972e-01, 2.29620028e-01],\n",
       "       [8.96941856e-01, 1.03058144e-01],\n",
       "       [1.01542270e-02, 9.89845773e-01],\n",
       "       [3.29284863e-01, 6.70715137e-01],\n",
       "       [9.79115114e-01, 2.08848863e-02],\n",
       "       [3.21286101e-04, 9.99678714e-01],\n",
       "       [9.85080267e-02, 9.01491973e-01],\n",
       "       [6.73824799e-02, 9.32617520e-01],\n",
       "       [9.95957151e-01, 4.04284935e-03],\n",
       "       [1.46535483e-02, 9.85346452e-01],\n",
       "       [2.73619909e-05, 9.99972638e-01],\n",
       "       [9.70016883e-01, 2.99831167e-02],\n",
       "       [8.50873664e-01, 1.49126336e-01],\n",
       "       [6.52726598e-01, 3.47273402e-01],\n",
       "       [3.71328572e-01, 6.28671428e-01],\n",
       "       [9.57376655e-01, 4.26233447e-02],\n",
       "       [8.32142898e-01, 1.67857102e-01],\n",
       "       [8.56322585e-01, 1.43677415e-01],\n",
       "       [5.75653149e-02, 9.42434685e-01],\n",
       "       [8.93439186e-01, 1.06560814e-01],\n",
       "       [8.56747667e-01, 1.43252333e-01],\n",
       "       [8.86252236e-03, 9.91137478e-01],\n",
       "       [7.59051924e-01, 2.40948076e-01],\n",
       "       [2.86240052e-01, 7.13759948e-01],\n",
       "       [9.96145674e-01, 3.85432624e-03],\n",
       "       [6.95980974e-03, 9.93040190e-01],\n",
       "       [9.79049347e-01, 2.09506530e-02],\n",
       "       [4.46258951e-03, 9.95537410e-01],\n",
       "       [8.80047667e-01, 1.19952333e-01],\n",
       "       [9.71816536e-01, 2.81834640e-02]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.predict_proba(X_test) # for probaBILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 0.9166666666666666\n",
      "confusion [[146  11]\n",
      " [ 14 129]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92       157\n",
      "           1       0.92      0.90      0.91       143\n",
      "\n",
      "    accuracy                           0.92       300\n",
      "   macro avg       0.92      0.92      0.92       300\n",
      "weighted avg       0.92      0.92      0.92       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score =accuracy_score (y_test,y_pred)\n",
    "print(\"score\", score)\n",
    "confusion = confusion_matrix(y_test,y_pred)\n",
    "print(\"confusion\", confusion)\n",
    "\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning And Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "penality =['l1','l2','elasticnet']\n",
    "c_values = [100,10,1.0,0.1,0.01]\n",
    "solver=['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to put all the above values in the key value pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'penalty': ['l1', 'l2', 'elasticnet'],\n",
       " 'C': [100, 10, 1.0, 0.1, 0.01],\n",
       " 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params=dict(penalty=penality,C=c_values,solver=solver)\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search  CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "cv=StratifiedKFold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = GridSearchCV(estimator=model,param_grid=params,scoring='accuracy',cv=cv,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={&#x27;C&#x27;: [100, 10, 1.0, 0.1, 0.01],\n",
       "                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;elasticnet&#x27;],\n",
       "                         &#x27;solver&#x27;: [&#x27;newton-cg&#x27;, &#x27;lbfgs&#x27;, &#x27;liblinear&#x27;, &#x27;sag&#x27;,\n",
       "                                    &#x27;saga&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={&#x27;C&#x27;: [100, 10, 1.0, 0.1, 0.01],\n",
       "                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;elasticnet&#x27;],\n",
       "                         &#x27;solver&#x27;: [&#x27;newton-cg&#x27;, &#x27;lbfgs&#x27;, &#x27;liblinear&#x27;, &#x27;sag&#x27;,\n",
       "                                    &#x27;saga&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={'C': [100, 10, 1.0, 0.1, 0.01],\n",
       "                         'penalty': ['l1', 'l2', 'elasticnet'],\n",
       "                         'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag',\n",
       "                                    'saga']},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "200 fits failed out of a total of 375.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "                   ^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "                         ~~^~~~~~~~~~\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.91285714        nan 0.91285714 0.91285714\n",
      " 0.91285714 0.91285714 0.91285714 0.91285714        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.91285714\n",
      "        nan 0.91285714 0.91285714 0.91285714 0.91285714 0.91285714\n",
      " 0.91285714        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.91142857        nan 0.91142857 0.91142857\n",
      " 0.91142857 0.91142857 0.91142857 0.91142857        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.92285714\n",
      "        nan 0.92142857 0.91285714 0.91285714 0.91857143 0.91285714\n",
      " 0.91285714        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.92142857        nan 0.92428571 0.91285714\n",
      " 0.91285714 0.92285714 0.91285714 0.91285714        nan        nan\n",
      "        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={&#x27;C&#x27;: [100, 10, 1.0, 0.1, 0.01],\n",
       "                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;elasticnet&#x27;],\n",
       "                         &#x27;solver&#x27;: [&#x27;newton-cg&#x27;, &#x27;lbfgs&#x27;, &#x27;liblinear&#x27;, &#x27;sag&#x27;,\n",
       "                                    &#x27;saga&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={&#x27;C&#x27;: [100, 10, 1.0, 0.1, 0.01],\n",
       "                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;elasticnet&#x27;],\n",
       "                         &#x27;solver&#x27;: [&#x27;newton-cg&#x27;, &#x27;lbfgs&#x27;, &#x27;liblinear&#x27;, &#x27;sag&#x27;,\n",
       "                                    &#x27;saga&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={'C': [100, 10, 1.0, 0.1, 0.01],\n",
       "                         'penalty': ['l1', 'l2', 'elasticnet'],\n",
       "                         'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag',\n",
       "                                    'saga']},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.01, 'penalty': 'l1', 'solver': 'saga'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9242857142857142"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "       0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = grid.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.93       165\n",
      "           1       0.89      0.94      0.91       135\n",
      "\n",
      "    accuracy                           0.92       300\n",
      "   macro avg       0.92      0.92      0.92       300\n",
      "weighted avg       0.92      0.92      0.92       300\n",
      "\n",
      "[[149  16]\n",
      " [  8 127]]\n"
     ]
    }
   ],
   "source": [
    "score = accuracy_score(y_pred, y_test)\n",
    "print(score)\n",
    "print(classification_report(y_pred,y_test))\n",
    "print(confusion_matrix(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomized Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "randomcv = RandomizedSearchCV(estimator=model, param_distributions=params, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "30 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.91285714        nan        nan 0.91285714        nan\n",
      "        nan        nan 0.91285714 0.91285714]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5, estimator=LogisticRegression(),\n",
       "                   param_distributions={&#x27;C&#x27;: [100, 10, 1.0, 0.1, 0.01],\n",
       "                                        &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;elasticnet&#x27;],\n",
       "                                        &#x27;solver&#x27;: [&#x27;newton-cg&#x27;, &#x27;lbfgs&#x27;,\n",
       "                                                   &#x27;liblinear&#x27;, &#x27;sag&#x27;,\n",
       "                                                   &#x27;saga&#x27;]},\n",
       "                   scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5, estimator=LogisticRegression(),\n",
       "                   param_distributions={&#x27;C&#x27;: [100, 10, 1.0, 0.1, 0.01],\n",
       "                                        &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;elasticnet&#x27;],\n",
       "                                        &#x27;solver&#x27;: [&#x27;newton-cg&#x27;, &#x27;lbfgs&#x27;,\n",
       "                                                   &#x27;liblinear&#x27;, &#x27;sag&#x27;,\n",
       "                                                   &#x27;saga&#x27;]},\n",
       "                   scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=LogisticRegression(),\n",
       "                   param_distributions={'C': [100, 10, 1.0, 0.1, 0.01],\n",
       "                                        'penalty': ['l1', 'l2', 'elasticnet'],\n",
       "                                        'solver': ['newton-cg', 'lbfgs',\n",
       "                                                   'liblinear', 'sag',\n",
       "                                                   'saga']},\n",
       "                   scoring='accuracy')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomcv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9128571428571428"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomcv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'solver': 'lbfgs', 'penalty': 'l2', 'C': 0.01}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomcv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "       0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = randomcv.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9166666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92       162\n",
      "           1       0.90      0.93      0.91       138\n",
      "\n",
      "    accuracy                           0.92       300\n",
      "   macro avg       0.92      0.92      0.92       300\n",
      "weighted avg       0.92      0.92      0.92       300\n",
      "\n",
      "[[147  15]\n",
      " [ 10 128]]\n"
     ]
    }
   ],
   "source": [
    "score = accuracy_score(y_pred,y_test)\n",
    "print(score)\n",
    "print(classification_report(y_pred, y_test))\n",
    "print(confusion_matrix(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logestic regrssion fo multiclass classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=3, n_classes=3, random_state=42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 1, 1, 2, 0, 1, 2, 0, 1, 2, 0, 0, 0, 2, 0, 0, 1, 1, 2, 1,\n",
       "       0, 1, 1, 1, 0, 2, 1, 1, 0, 0, 1, 0, 1, 0, 2, 2, 0, 2, 0, 0, 0, 1,\n",
       "       2, 2, 1, 2, 1, 2, 0, 0, 1, 2, 2, 1, 0, 2, 2, 1, 0, 0, 1, 2, 2, 0,\n",
       "       2, 2, 1, 2, 2, 2, 1, 2, 1, 0, 1, 1, 2, 1, 1, 0, 1, 2, 2, 1, 2, 1,\n",
       "       1, 2, 2, 2, 2, 2, 2, 0, 0, 2, 1, 1, 1, 0, 1, 1, 1, 2, 2, 0, 0, 0,\n",
       "       0, 0, 1, 2, 0, 0, 0, 1, 0, 1, 1, 2, 1, 0, 2, 1, 0, 2, 1, 0, 1, 0,\n",
       "       1, 2, 2, 0, 0, 1, 2, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 2, 2, 1, 1, 2,\n",
       "       0, 0, 1, 0, 0, 2, 2, 2, 0, 0, 2, 1, 2, 1, 1, 1, 2, 2, 0, 0, 2, 0,\n",
       "       1, 0, 1, 2, 2, 2, 2, 1, 0, 0, 2, 2, 0, 1, 0, 1, 0, 1, 2, 1, 2, 0,\n",
       "       0, 2, 2, 1, 1, 0, 1, 0, 1, 0, 1, 2, 1, 1, 0, 2, 0, 2, 1, 0, 2, 2,\n",
       "       2, 0, 0, 1, 1, 2, 1, 1, 1, 0, 0, 2, 0, 1, 2, 1, 1, 1, 1, 0, 0, 2,\n",
       "       2, 0, 2, 0, 0, 2, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 2, 1, 1, 1, 1, 1,\n",
       "       2, 2, 0, 2, 2, 0, 1, 0, 2, 0, 1, 2, 2, 1, 1, 0, 1, 2, 2, 0, 2, 0,\n",
       "       2, 0, 0, 1, 1, 0, 1, 1, 1, 2, 0, 0, 0, 0, 1, 2, 2, 2, 0, 2, 1, 1,\n",
       "       2, 0, 1, 1, 0, 1, 2, 1, 0, 2, 1, 1, 0, 1, 2, 0, 2, 1, 2, 0, 2, 2,\n",
       "       1, 1, 1, 0, 2, 0, 2, 0, 0, 0, 1, 0, 2, 1, 2, 1, 1, 1, 0, 1, 2, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 2, 2, 2, 2, 0, 1, 0, 1, 0, 1, 2, 0,\n",
       "       2, 2, 2, 2, 1, 1, 0, 0, 2, 2, 0, 0, 2, 0, 2, 1, 2, 1, 0, 1, 2, 1,\n",
       "       1, 1, 1, 0, 0, 0, 1, 2, 1, 1, 2, 0, 0, 1, 1, 2, 2, 0, 2, 0, 2, 2,\n",
       "       0, 0, 1, 1, 1, 1, 2, 1, 1, 1, 2, 0, 1, 1, 2, 1, 2, 0, 0, 2, 1, 1,\n",
       "       1, 2, 2, 1, 2, 2, 0, 0, 0, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 0, 0, 1,\n",
       "       1, 2, 0, 0, 2, 2, 0, 0, 1, 0, 0, 1, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0,\n",
       "       0, 2, 2, 1, 2, 1, 1, 0, 2, 2, 2, 1, 2, 1, 1, 0, 0, 2, 2, 2, 2, 2,\n",
       "       2, 0, 1, 1, 1, 1, 0, 1, 2, 1, 1, 0, 0, 2, 1, 1, 1, 0, 2, 2, 2, 2,\n",
       "       2, 0, 1, 0, 0, 0, 1, 0, 2, 0, 1, 2, 0, 2, 0, 1, 1, 2, 0, 2, 0, 0,\n",
       "       0, 2, 1, 0, 1, 0, 2, 2, 1, 0, 1, 0, 0, 2, 2, 2, 1, 0, 1, 1, 0, 1,\n",
       "       2, 2, 0, 2, 2, 2, 0, 0, 1, 1, 2, 1, 0, 2, 2, 1, 1, 1, 1, 0, 1, 0,\n",
       "       2, 2, 1, 1, 2, 0, 1, 1, 1, 2, 2, 2, 0, 1, 2, 2, 1, 1, 1, 2, 1, 2,\n",
       "       2, 0, 2, 0, 1, 2, 0, 1, 1, 0, 2, 0, 0, 1, 1, 0, 0, 0, 0, 2, 1, 0,\n",
       "       0, 1, 2, 0, 0, 1, 2, 2, 2, 1, 0, 2, 2, 1, 2, 0, 1, 1, 0, 2, 2, 1,\n",
       "       0, 2, 2, 1, 2, 0, 2, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 0, 0, 1, 2, 1,\n",
       "       0, 0, 0, 0, 2, 2, 2, 0, 0, 2, 1, 2, 0, 1, 0, 1, 1, 0, 2, 0, 2, 1,\n",
       "       0, 0, 0, 0, 2, 1, 2, 0, 1, 0, 0, 2, 1, 2, 1, 0, 1, 2, 1, 1, 1, 0,\n",
       "       2, 0, 1, 2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 1, 1, 0, 1, 2, 2, 0, 1, 1,\n",
       "       1, 0, 0, 0, 1, 2, 0, 2, 1, 0, 0, 2, 0, 2, 0, 1, 1, 2, 0, 1, 0, 0,\n",
       "       1, 1, 2, 1, 0, 2, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 2, 2, 2, 1, 1,\n",
       "       1, 2, 2, 1, 1, 0, 0, 0, 2, 0, 1, 1, 1, 2, 0, 1, 2, 0, 2, 0, 2, 2,\n",
       "       1, 1, 0, 1, 1, 0, 1, 2, 0, 2, 2, 0, 1, 2, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "       2, 2, 2, 0, 1, 1, 2, 2, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0, 1, 1, 2, 0,\n",
       "       1, 1, 2, 2, 1, 0, 1, 2, 0, 1, 2, 2, 2, 0, 1, 1, 2, 2, 0, 0, 0, 0,\n",
       "       0, 2, 2, 0, 2, 1, 1, 1, 0, 2, 1, 2, 2, 0, 2, 0, 0, 2, 2, 0, 1, 2,\n",
       "       0, 0, 0, 2, 0, 1, 0, 1, 1, 0, 2, 1, 2, 0, 0, 2, 2, 0, 2, 1, 2, 2,\n",
       "       0, 2, 2, 1, 2, 2, 1, 0, 0, 0, 0, 2, 0, 1, 2, 0, 1, 0, 2, 1, 2, 1,\n",
       "       0, 2, 0, 1, 0, 0, 2, 1, 2, 2, 2, 2, 0, 2, 2, 0, 2, 1, 0, 2, 0, 0,\n",
       "       2, 1, 1, 1, 0, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 0, 2, 1, 1, 2, 1, 0,\n",
       "       0, 1, 0, 1, 1, 2, 0, 2, 1, 2])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.10,random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logestic = LogisticRegression()\n",
    "logestic.fit(X_train, y_train)\n",
    "y_pred = logestic.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59\n"
     ]
    }
   ],
   "source": [
    "score = accuracy_score(y_pred, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.62      0.64        37\n",
      "           1       0.34      0.38      0.36        29\n",
      "           2       0.76      0.74      0.75        34\n",
      "\n",
      "    accuracy                           0.59       100\n",
      "   macro avg       0.59      0.58      0.58       100\n",
      "weighted avg       0.60      0.59      0.59       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23 13  1]\n",
      " [11 11  7]\n",
      " [ 1  8 25]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logestic Regression for imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate and plot a synthetic imbalanced classification dataset\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imbalanced dataset\n",
    "X, y = make_classification(n_samples=1000,n_features=2,n_clusters_per_class=1, n_redundant=0, weights=[0.99], random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.45122049, -1.08670474],\n",
       "       [ 2.08029047, -0.97808443],\n",
       "       [ 1.91805213, -1.0431487 ],\n",
       "       ...,\n",
       "       [ 0.83675119, -0.54161851],\n",
       "       [ 0.45782986, -1.05177133],\n",
       "       [ 0.27891721, -1.16309231]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 985, 1: 15})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.45122049, -1.08670474],\n",
       "       [ 2.08029047, -0.97808443],\n",
       "       [ 1.91805213, -1.0431487 ],\n",
       "       ...,\n",
       "       [ 0.83675119, -0.54161851],\n",
       "       [ 0.45782986, -1.05177133],\n",
       "       [ 0.27891721, -1.16309231]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The reason why we are taking the class weight for ther imbalanced dataset because we can make the data understand easily if we do so by using weight for that we are considering weights. And the weights can be dict or balanced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight=[{0:w,1:y} for w in [1,10,50,100] for y in [1,10,50,100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0: 1, 1: 1},\n",
       " {0: 1, 1: 10},\n",
       " {0: 1, 1: 50},\n",
       " {0: 1, 1: 100},\n",
       " {0: 10, 1: 1},\n",
       " {0: 10, 1: 10},\n",
       " {0: 10, 1: 50},\n",
       " {0: 10, 1: 100},\n",
       " {0: 50, 1: 1},\n",
       " {0: 50, 1: 10},\n",
       " {0: 50, 1: 50},\n",
       " {0: 50, 1: 100},\n",
       " {0: 100, 1: 1},\n",
       " {0: 100, 1: 10},\n",
       " {0: 100, 1: 50},\n",
       " {0: 100, 1: 100}]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'penalty': ['l2'],\n",
       "  'C': [100, 10, 1.0, 0.1, 0.01],\n",
       "  'solver': ['newton-cg', 'lbfgs', 'sag'],\n",
       "  'class_weight': [{0: 1, 1: 1},\n",
       "   {0: 1, 1: 10},\n",
       "   {0: 1, 1: 50},\n",
       "   {0: 1, 1: 100},\n",
       "   {0: 10, 1: 1},\n",
       "   {0: 10, 1: 10},\n",
       "   {0: 10, 1: 50},\n",
       "   {0: 10, 1: 100},\n",
       "   {0: 50, 1: 1},\n",
       "   {0: 50, 1: 10},\n",
       "   {0: 50, 1: 50},\n",
       "   {0: 50, 1: 100},\n",
       "   {0: 100, 1: 1},\n",
       "   {0: 100, 1: 10},\n",
       "   {0: 100, 1: 50},\n",
       "   {0: 100, 1: 100}]},\n",
       " {'penalty': ['l1'],\n",
       "  'C': [100, 10, 1.0, 0.1, 0.01],\n",
       "  'solver': ['liblinear'],\n",
       "  'class_weight': [{0: 1, 1: 1},\n",
       "   {0: 1, 1: 10},\n",
       "   {0: 1, 1: 50},\n",
       "   {0: 1, 1: 100},\n",
       "   {0: 10, 1: 1},\n",
       "   {0: 10, 1: 10},\n",
       "   {0: 10, 1: 50},\n",
       "   {0: 10, 1: 100},\n",
       "   {0: 50, 1: 1},\n",
       "   {0: 50, 1: 10},\n",
       "   {0: 50, 1: 50},\n",
       "   {0: 50, 1: 100},\n",
       "   {0: 100, 1: 1},\n",
       "   {0: 100, 1: 10},\n",
       "   {0: 100, 1: 50},\n",
       "   {0: 100, 1: 100}]},\n",
       " {'penalty': ['elasticnet'],\n",
       "  'C': [100, 10, 1.0, 0.1, 0.01],\n",
       "  'solver': ['saga'],\n",
       "  'class_weight': [{0: 1, 1: 1},\n",
       "   {0: 1, 1: 10},\n",
       "   {0: 1, 1: 50},\n",
       "   {0: 1, 1: 100},\n",
       "   {0: 10, 1: 1},\n",
       "   {0: 10, 1: 10},\n",
       "   {0: 10, 1: 50},\n",
       "   {0: 10, 1: 100},\n",
       "   {0: 50, 1: 1},\n",
       "   {0: 50, 1: 10},\n",
       "   {0: 50, 1: 50},\n",
       "   {0: 50, 1: 100},\n",
       "   {0: 100, 1: 1},\n",
       "   {0: 100, 1: 10},\n",
       "   {0: 100, 1: 50},\n",
       "   {0: 100, 1: 100}],\n",
       "  'l1_ratio': [0.5]}]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correct the typo and ensure compatible combinations\n",
    "penalty = ['l1', 'l2', 'elasticnet']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "solver = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "class_weight = [{0: w, 1: y} for w in [1, 10, 50, 100] for y in [1, 10, 50, 100]]\n",
    "\n",
    "# Define the parameter grid with compatible combinations\n",
    "params = [\n",
    "    {'penalty': ['l2'], 'C': c_values, 'solver': ['newton-cg', 'lbfgs', 'sag'], 'class_weight': class_weight},\n",
    "    {'penalty': ['l1'], 'C': c_values, 'solver': ['liblinear'], 'class_weight': class_weight},\n",
    "    {'penalty': ['elasticnet'], 'C': c_values, 'solver': ['saga'], 'class_weight': class_weight, 'l1_ratio': [0.5]}\n",
    "]\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "cv = StratifiedKFold()\n",
    "grid=GridSearchCV(estimator=model, param_grid=params, scoring='accuracy',cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=LogisticRegression(),\n",
       "             param_grid=[{&#x27;C&#x27;: [100, 10, 1.0, 0.1, 0.01],\n",
       "                          &#x27;class_weight&#x27;: [{0: 1, 1: 1}, {0: 1, 1: 10},\n",
       "                                           {0: 1, 1: 50}, {0: 1, 1: 100},\n",
       "                                           {0: 10, 1: 1}, {0: 10, 1: 10},\n",
       "                                           {0: 10, 1: 50}, {0: 10, 1: 100},\n",
       "                                           {0: 50, 1: 1}, {0: 50, 1: 10},\n",
       "                                           {0: 50, 1: 50}, {0: 50, 1: 100},\n",
       "                                           {0: 100, 1: 1}, {0: 100, 1: 10},\n",
       "                                           {0: 100, 1: 50}, {0: 100, 1: 1...\n",
       "                          &#x27;penalty&#x27;: [&#x27;l1&#x27;], &#x27;solver&#x27;: [&#x27;liblinear&#x27;]},\n",
       "                         {&#x27;C&#x27;: [100, 10, 1.0, 0.1, 0.01],\n",
       "                          &#x27;class_weight&#x27;: [{0: 1, 1: 1}, {0: 1, 1: 10},\n",
       "                                           {0: 1, 1: 50}, {0: 1, 1: 100},\n",
       "                                           {0: 10, 1: 1}, {0: 10, 1: 10},\n",
       "                                           {0: 10, 1: 50}, {0: 10, 1: 100},\n",
       "                                           {0: 50, 1: 1}, {0: 50, 1: 10},\n",
       "                                           {0: 50, 1: 50}, {0: 50, 1: 100},\n",
       "                                           {0: 100, 1: 1}, {0: 100, 1: 10},\n",
       "                                           {0: 100, 1: 50}, {0: 100, 1: 100}],\n",
       "                          &#x27;l1_ratio&#x27;: [0.5], &#x27;penalty&#x27;: [&#x27;elasticnet&#x27;],\n",
       "                          &#x27;solver&#x27;: [&#x27;saga&#x27;]}],\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=LogisticRegression(),\n",
       "             param_grid=[{&#x27;C&#x27;: [100, 10, 1.0, 0.1, 0.01],\n",
       "                          &#x27;class_weight&#x27;: [{0: 1, 1: 1}, {0: 1, 1: 10},\n",
       "                                           {0: 1, 1: 50}, {0: 1, 1: 100},\n",
       "                                           {0: 10, 1: 1}, {0: 10, 1: 10},\n",
       "                                           {0: 10, 1: 50}, {0: 10, 1: 100},\n",
       "                                           {0: 50, 1: 1}, {0: 50, 1: 10},\n",
       "                                           {0: 50, 1: 50}, {0: 50, 1: 100},\n",
       "                                           {0: 100, 1: 1}, {0: 100, 1: 10},\n",
       "                                           {0: 100, 1: 50}, {0: 100, 1: 1...\n",
       "                          &#x27;penalty&#x27;: [&#x27;l1&#x27;], &#x27;solver&#x27;: [&#x27;liblinear&#x27;]},\n",
       "                         {&#x27;C&#x27;: [100, 10, 1.0, 0.1, 0.01],\n",
       "                          &#x27;class_weight&#x27;: [{0: 1, 1: 1}, {0: 1, 1: 10},\n",
       "                                           {0: 1, 1: 50}, {0: 1, 1: 100},\n",
       "                                           {0: 10, 1: 1}, {0: 10, 1: 10},\n",
       "                                           {0: 10, 1: 50}, {0: 10, 1: 100},\n",
       "                                           {0: 50, 1: 1}, {0: 50, 1: 10},\n",
       "                                           {0: 50, 1: 50}, {0: 50, 1: 100},\n",
       "                                           {0: 100, 1: 1}, {0: 100, 1: 10},\n",
       "                                           {0: 100, 1: 50}, {0: 100, 1: 100}],\n",
       "                          &#x27;l1_ratio&#x27;: [0.5], &#x27;penalty&#x27;: [&#x27;elasticnet&#x27;],\n",
       "                          &#x27;solver&#x27;: [&#x27;saga&#x27;]}],\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=LogisticRegression(),\n",
       "             param_grid=[{'C': [100, 10, 1.0, 0.1, 0.01],\n",
       "                          'class_weight': [{0: 1, 1: 1}, {0: 1, 1: 10},\n",
       "                                           {0: 1, 1: 50}, {0: 1, 1: 100},\n",
       "                                           {0: 10, 1: 1}, {0: 10, 1: 10},\n",
       "                                           {0: 10, 1: 50}, {0: 10, 1: 100},\n",
       "                                           {0: 50, 1: 1}, {0: 50, 1: 10},\n",
       "                                           {0: 50, 1: 50}, {0: 50, 1: 100},\n",
       "                                           {0: 100, 1: 1}, {0: 100, 1: 10},\n",
       "                                           {0: 100, 1: 50}, {0: 100, 1: 1...\n",
       "                          'penalty': ['l1'], 'solver': ['liblinear']},\n",
       "                         {'C': [100, 10, 1.0, 0.1, 0.01],\n",
       "                          'class_weight': [{0: 1, 1: 1}, {0: 1, 1: 10},\n",
       "                                           {0: 1, 1: 50}, {0: 1, 1: 100},\n",
       "                                           {0: 10, 1: 1}, {0: 10, 1: 10},\n",
       "                                           {0: 10, 1: 50}, {0: 10, 1: 100},\n",
       "                                           {0: 50, 1: 1}, {0: 50, 1: 10},\n",
       "                                           {0: 50, 1: 50}, {0: 50, 1: 100},\n",
       "                                           {0: 100, 1: 1}, {0: 100, 1: 10},\n",
       "                                           {0: 100, 1: 50}, {0: 100, 1: 100}],\n",
       "                          'l1_ratio': [0.5], 'penalty': ['elasticnet'],\n",
       "                          'solver': ['saga']}],\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=100, class_weight={0: 1, 1: 1}, solver=&#x27;newton-cg&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" checked><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=100, class_weight={0: 1, 1: 1}, solver=&#x27;newton-cg&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=100, class_weight={0: 1, 1: 1}, solver='newton-cg')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9825000000000002"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.02779083, 0.00639563, 0.00140672, 0.00949945, 0.00479927,\n",
       "        0.00380931, 0.00960007, 0.00561695, 0.00530486, 0.01219893,\n",
       "        0.00710526, 0.0047998 , 0.01808124, 0.00739617, 0.00180888,\n",
       "        0.01890726, 0.00714765, 0.00376229, 0.0110137 , 0.00538721,\n",
       "        0.00400386, 0.00911961, 0.00399203, 0.00400014, 0.0142025 ,\n",
       "        0.00675144, 0.00259967, 0.01191201, 0.00620022, 0.00411944,\n",
       "        0.01109929, 0.00520134, 0.00552764, 0.01079297, 0.00511408,\n",
       "        0.00408902, 0.01469975, 0.00697632, 0.00420232, 0.0124773 ,\n",
       "        0.00653667, 0.00380764, 0.01169934, 0.00499315, 0.00450335,\n",
       "        0.03390455, 0.01307683, 0.00659232, 0.01273041, 0.00616183,\n",
       "        0.00200019, 0.00984674, 0.00667071, 0.00419979, 0.01113162,\n",
       "        0.01054993, 0.00874972, 0.01381431, 0.00634122, 0.00600057,\n",
       "        0.03494668, 0.01320019, 0.00257902, 0.0221899 , 0.00794086,\n",
       "        0.00417204, 0.01165004, 0.00576606, 0.00440593, 0.01055021,\n",
       "        0.00496955, 0.00393376, 0.01802764, 0.00937347, 0.00340781,\n",
       "        0.01797843, 0.00610118, 0.00400329, 0.01271381, 0.00591049,\n",
       "        0.00413952, 0.01603909, 0.00700035, 0.00581608, 0.02110825,\n",
       "        0.00916848, 0.00452671, 0.01666903, 0.00773153, 0.00423031,\n",
       "        0.01941648, 0.00733118, 0.00414543, 0.01201191, 0.00529628,\n",
       "        0.00361843, 0.01201496, 0.00656586, 0.00099049, 0.00975056,\n",
       "        0.00531888, 0.0040885 , 0.00934443, 0.0117517 , 0.01198812,\n",
       "        0.01681685, 0.00831165, 0.00564365, 0.01451325, 0.00748663,\n",
       "        0.0015933 , 0.01409612, 0.00620332, 0.0030045 , 0.01639938,\n",
       "        0.00669146, 0.00411801, 0.01018729, 0.00511456, 0.00514102,\n",
       "        0.01651669, 0.00760145, 0.00208683, 0.01374416, 0.00891948,\n",
       "        0.0050416 , 0.01926584, 0.00580907, 0.00411315, 0.01848526,\n",
       "        0.01244102, 0.00853744, 0.0266089 , 0.00820045, 0.00251131,\n",
       "        0.01799374, 0.00711837, 0.00408878, 0.01751752, 0.00611348,\n",
       "        0.00404787, 0.01522994, 0.00671272, 0.00421071, 0.01702142,\n",
       "        0.00910678, 0.00122375, 0.02199059, 0.01478987, 0.00271072,\n",
       "        0.02103343, 0.01157179, 0.0063839 , 0.02087717, 0.00729327,\n",
       "        0.00558305, 0.02415133, 0.00760598, 0.00150461, 0.01821361,\n",
       "        0.00614872, 0.00172448, 0.01171484, 0.00531411, 0.00414295,\n",
       "        0.01025071, 0.00538483, 0.00409946, 0.01844893, 0.00620947,\n",
       "        0.00111094, 0.01532207, 0.00721712, 0.00170727, 0.01190929,\n",
       "        0.00661488, 0.00510702, 0.01525345, 0.00602307, 0.00453196,\n",
       "        0.01757255, 0.00835452, 0.00141039, 0.02030702, 0.00720024,\n",
       "        0.00158839, 0.01423526, 0.00602455, 0.0046176 , 0.01444702,\n",
       "        0.00628743, 0.00434213, 0.01248651, 0.00428061, 0.00120606,\n",
       "        0.0095902 , 0.00564847, 0.00268264, 0.01739669, 0.00480852,\n",
       "        0.00409331, 0.01172395, 0.00489545, 0.00491838, 0.01738157,\n",
       "        0.00602989, 0.00132151, 0.01269665, 0.00494924, 0.00171371,\n",
       "        0.01031733, 0.00520525, 0.00240669, 0.0106637 , 0.00571699,\n",
       "        0.00403533, 0.01403246, 0.00661459, 0.00140052, 0.01040077,\n",
       "        0.00628176, 0.00150652, 0.01681342, 0.00626349, 0.0015152 ,\n",
       "        0.01311784, 0.00651479, 0.00261497, 0.01653118, 0.00628953,\n",
       "        0.00140643, 0.01530857, 0.00757861, 0.00080433, 0.01325436,\n",
       "        0.00832872, 0.00160265, 0.0165452 , 0.00628233, 0.00191007,\n",
       "        0.00130191, 0.00120707, 0.00132232, 0.0010016 , 0.00133247,\n",
       "        0.00133033, 0.00143209, 0.00059481, 0.00098205, 0.0013114 ,\n",
       "        0.00132732, 0.00160723, 0.00119276, 0.00222731, 0.00243731,\n",
       "        0.0022285 , 0.00230708, 0.00209846, 0.00240579, 0.00192089,\n",
       "        0.00152125, 0.00130939, 0.0013134 , 0.0015069 , 0.00198226,\n",
       "        0.00268259, 0.00210676, 0.00110874, 0.00090337, 0.00152106,\n",
       "        0.00111198, 0.0017951 , 0.00120158, 0.00132327, 0.0015986 ,\n",
       "        0.00121579, 0.00121984, 0.00103197, 0.0011374 , 0.00107441,\n",
       "        0.00130429, 0.00119534, 0.001547  , 0.00206146, 0.00115385,\n",
       "        0.00124168, 0.00153217, 0.00127397, 0.00099812, 0.00143638,\n",
       "        0.00169568, 0.00111947, 0.00182438, 0.00231719, 0.00170259,\n",
       "        0.00138726, 0.0012022 , 0.00130777, 0.0021173 , 0.00141468,\n",
       "        0.0012321 , 0.00238628, 0.00251756, 0.00200682, 0.00190835,\n",
       "        0.00153108, 0.00111217, 0.00071025, 0.00109301, 0.0013113 ,\n",
       "        0.00120053, 0.0012898 , 0.00100694, 0.00111356, 0.00092187,\n",
       "        0.00151758, 0.00221324, 0.00151181, 0.00234079, 0.00223689,\n",
       "        0.00452385, 0.00534563, 0.00975833, 0.00996432, 0.00573306,\n",
       "        0.00561557, 0.00987158, 0.00877233, 0.00584884, 0.00833983,\n",
       "        0.00836048, 0.01524963, 0.00542693, 0.0055347 , 0.00694399,\n",
       "        0.01748605, 0.00723686, 0.00867968, 0.00624881, 0.00547743,\n",
       "        0.00474343, 0.00262113, 0.00712924, 0.00706425, 0.00744238,\n",
       "        0.01153216, 0.01218457, 0.0107152 , 0.00979023, 0.00872788,\n",
       "        0.00613141, 0.00625243, 0.0046248 , 0.00403171, 0.00690618,\n",
       "        0.01104951, 0.0076293 , 0.00594654, 0.00925937, 0.00877399,\n",
       "        0.00526905, 0.00553131, 0.00724907, 0.01036053, 0.00986371,\n",
       "        0.00654769, 0.00705209, 0.00898809, 0.00416627, 0.00527639,\n",
       "        0.01045799, 0.01230803, 0.00587945, 0.00427103, 0.0098815 ,\n",
       "        0.00799079, 0.00625453, 0.00902529, 0.01512408, 0.0167942 ,\n",
       "        0.00880084, 0.01055388, 0.00746775, 0.01085501, 0.00198908,\n",
       "        0.00398302, 0.01015453, 0.00773506, 0.00099859, 0.00284367,\n",
       "        0.00801067, 0.01062889, 0.00118046, 0.00472784, 0.00639505,\n",
       "        0.00893803, 0.00282264, 0.00268559, 0.00769014, 0.0071651 ]),\n",
       " 'std_fit_time': array([5.57561687e-03, 4.94558467e-04, 4.84788227e-04, 6.28940703e-04,\n",
       "        1.47184124e-03, 7.43717347e-04, 1.35638894e-03, 4.72427516e-04,\n",
       "        1.46495361e-03, 3.42945167e-03, 1.73996722e-03, 7.48381411e-04,\n",
       "        2.06841146e-03, 8.05999449e-04, 3.97286287e-04, 4.62566993e-03,\n",
       "        8.11558865e-04, 9.23321910e-04, 5.26024190e-04, 8.07253828e-04,\n",
       "        6.38167482e-04, 8.97753129e-04, 8.78261041e-06, 2.42390919e-06,\n",
       "        1.31633637e-03, 1.04851605e-03, 8.00645761e-04, 7.53364918e-04,\n",
       "        4.08737363e-04, 4.86638904e-04, 1.27861469e-03, 3.99666349e-04,\n",
       "        6.20906976e-04, 7.50531811e-04, 4.97863779e-04, 1.79353320e-04,\n",
       "        1.07378929e-03, 1.45559489e-03, 4.01459180e-04, 4.74244427e-04,\n",
       "        5.95244257e-04, 4.04277846e-04, 1.73316757e-03, 6.48510340e-04,\n",
       "        4.45596826e-04, 8.39483878e-03, 1.42628883e-03, 1.13582823e-03,\n",
       "        2.72691725e-03, 6.00303716e-04, 4.21562883e-06, 1.25555636e-03,\n",
       "        2.17743991e-03, 7.46028648e-04, 1.56454396e-03, 1.83478703e-03,\n",
       "        3.48666280e-03, 2.55587029e-03, 1.95640165e-03, 1.41554315e-03,\n",
       "        4.61773278e-03, 2.85433571e-03, 7.30223346e-04, 6.68700604e-03,\n",
       "        1.35449740e-03, 9.63796652e-04, 8.39355070e-04, 1.82331873e-03,\n",
       "        7.80493135e-04, 1.32148463e-03, 7.25941475e-04, 3.64483750e-04,\n",
       "        1.49982038e-03, 2.07432179e-03, 1.15021323e-03, 2.55430442e-03,\n",
       "        3.17233508e-04, 3.14184997e-04, 9.30067774e-04, 6.57161509e-04,\n",
       "        7.42633584e-04, 5.13907600e-03, 1.02624654e-03, 1.88530721e-03,\n",
       "        1.94231809e-03, 1.79161070e-03, 1.05361821e-03, 1.14165361e-03,\n",
       "        1.42001237e-03, 5.13840868e-04, 4.37397116e-03, 1.21896344e-03,\n",
       "        2.01688275e-04, 1.42111646e-03, 7.63726252e-04, 3.76734986e-04,\n",
       "        1.61998097e-03, 1.29383173e-03, 1.94930190e-05, 7.81969505e-04,\n",
       "        1.58247741e-03, 3.49087973e-04, 2.03648366e-03, 3.67819388e-03,\n",
       "        2.27404941e-03, 1.86323122e-03, 8.08134446e-04, 8.25628130e-04,\n",
       "        6.05896414e-04, 5.65522244e-04, 5.03796666e-04, 1.73041568e-03,\n",
       "        1.26666838e-03, 7.07953778e-04, 4.51691672e-03, 1.70230828e-03,\n",
       "        4.88011614e-04, 1.71854468e-03, 7.29271295e-04, 1.32734847e-03,\n",
       "        8.13827604e-04, 9.27912961e-04, 2.12223201e-04, 5.84888104e-04,\n",
       "        1.59716868e-03, 1.40632130e-03, 5.92946084e-03, 2.38715797e-04,\n",
       "        3.75870573e-04, 1.73951440e-03, 2.47623032e-03, 4.01188744e-03,\n",
       "        5.96740558e-03, 5.30335479e-04, 4.50773245e-04, 1.83138645e-03,\n",
       "        1.19777099e-03, 2.18847353e-04, 1.27935634e-03, 3.74977641e-04,\n",
       "        4.21924180e-04, 1.86803475e-03, 5.14465119e-04, 3.51788888e-04,\n",
       "        2.93724773e-03, 1.59259701e-03, 5.20268989e-04, 5.48931532e-03,\n",
       "        1.22200382e-03, 6.06006144e-04, 7.10134339e-03, 4.64370508e-03,\n",
       "        1.75627027e-03, 4.87125521e-03, 1.00527434e-03, 1.99583237e-03,\n",
       "        5.42185864e-03, 1.40150215e-03, 6.42832331e-04, 2.70984553e-03,\n",
       "        4.87340280e-04, 3.91213810e-04, 2.01037430e-03, 2.70634221e-04,\n",
       "        8.65412091e-04, 1.63405196e-03, 1.37408803e-03, 6.67873597e-04,\n",
       "        4.13168274e-03, 8.22607184e-04, 1.99411715e-04, 1.13655661e-03,\n",
       "        1.26263598e-03, 3.97236503e-04, 9.75868772e-04, 1.36059623e-03,\n",
       "        6.46383092e-04, 2.21053747e-03, 5.61956781e-04, 6.54893344e-04,\n",
       "        1.68715372e-03, 1.84872162e-03, 5.95893318e-04, 7.64367385e-03,\n",
       "        7.58527996e-04, 5.02706315e-04, 1.98076158e-03, 7.27959265e-04,\n",
       "        6.02076481e-04, 1.87779429e-03, 8.62747805e-04, 4.86691161e-04,\n",
       "        1.73914878e-03, 4.68193786e-04, 4.11440113e-04, 2.63039677e-03,\n",
       "        1.99578381e-03, 1.33934422e-03, 5.66276473e-03, 9.47651988e-04,\n",
       "        1.09758288e-03, 2.15992569e-03, 6.62990722e-04, 8.68591042e-04,\n",
       "        2.49391517e-03, 8.69442172e-04, 4.14371090e-04, 1.73908059e-03,\n",
       "        4.00291705e-04, 4.01907267e-04, 7.45611527e-04, 1.33747233e-03,\n",
       "        8.48939852e-04, 1.85042031e-03, 1.79794521e-03, 8.35862796e-04,\n",
       "        1.01610021e-03, 1.02004336e-03, 4.80693988e-04, 1.27456752e-03,\n",
       "        1.28466108e-03, 6.44579705e-04, 1.54796408e-03, 1.13129082e-03,\n",
       "        6.26313607e-04, 1.41945953e-03, 1.06340585e-03, 5.82984433e-04,\n",
       "        1.34265121e-03, 1.90985717e-04, 4.84762374e-04, 1.62427206e-03,\n",
       "        1.40146202e-03, 4.02255672e-04, 1.16809897e-03, 2.57327641e-03,\n",
       "        4.91749310e-04, 2.60271065e-03, 1.01082243e-03, 4.99082235e-04,\n",
       "        3.99293130e-04, 4.09287735e-04, 4.11567551e-04, 6.75865160e-06,\n",
       "        8.63527480e-04, 4.22092428e-04, 3.88059021e-04, 4.85678049e-04,\n",
       "        2.19831756e-05, 6.15100021e-04, 4.18457016e-04, 4.96135564e-04,\n",
       "        3.95805243e-04, 2.78278219e-04, 3.92055859e-04, 2.75067060e-04,\n",
       "        4.06599987e-04, 2.09092773e-04, 1.59691372e-03, 1.69351749e-04,\n",
       "        4.44450003e-04, 4.11902663e-04, 4.08860473e-04, 6.18895848e-04,\n",
       "        6.40832266e-04, 8.62995805e-04, 2.12545027e-04, 2.37210340e-04,\n",
       "        1.80842968e-04, 4.46206837e-04, 2.14332866e-04, 3.88671878e-04,\n",
       "        9.82792147e-04, 4.03286683e-04, 8.07015399e-04, 3.95478908e-04,\n",
       "        4.29834107e-04, 6.44918687e-05, 2.94861293e-04, 2.76498441e-04,\n",
       "        4.75988243e-04, 9.33475530e-04, 4.16405774e-04, 7.67224906e-04,\n",
       "        3.47621732e-04, 3.61983541e-04, 7.32857955e-04, 3.71736560e-04,\n",
       "        8.92968104e-04, 3.83638641e-04, 3.91519326e-04, 2.41867140e-04,\n",
       "        5.37006018e-04, 3.94757612e-04, 3.92148612e-04, 5.08259083e-04,\n",
       "        3.96651469e-04, 3.94614754e-04, 6.74407510e-04, 7.39762401e-04,\n",
       "        3.88738708e-04, 4.98794378e-04, 4.49609870e-04, 6.31864355e-04,\n",
       "        5.00084556e-04, 4.74059584e-04, 2.38502602e-04, 6.14055667e-04,\n",
       "        2.18623914e-04, 3.97783462e-04, 4.04576325e-04, 3.76215223e-04,\n",
       "        1.59242195e-05, 2.43107586e-04, 5.17407794e-04, 6.51269724e-04,\n",
       "        2.82800859e-04, 7.77365099e-04, 9.18391424e-04, 3.14471752e-04,\n",
       "        4.47760288e-04, 2.14565715e-03, 1.47586760e-03, 1.40914538e-03,\n",
       "        6.71260562e-04, 7.49678561e-04, 1.82725131e-03, 2.09267970e-03,\n",
       "        6.06097345e-04, 1.15601562e-03, 2.27309359e-03, 5.90674338e-03,\n",
       "        5.69667624e-04, 7.06234071e-04, 1.17157016e-03, 6.23542298e-03,\n",
       "        1.93352053e-03, 3.01291627e-03, 8.99353570e-04, 8.54629480e-04,\n",
       "        8.28099791e-04, 8.67895835e-04, 1.25621094e-03, 8.59805264e-04,\n",
       "        1.20793032e-03, 1.10028924e-03, 1.06497668e-03, 1.75449023e-03,\n",
       "        1.53041300e-03, 1.94411021e-03, 7.09436234e-04, 5.88999685e-04,\n",
       "        5.91863016e-04, 1.93438181e-03, 1.05396579e-03, 1.09767219e-03,\n",
       "        1.67380237e-03, 3.32930210e-03, 2.94402085e-03, 2.28551772e-03,\n",
       "        1.66211247e-03, 9.97812465e-04, 2.00131654e-03, 2.17616175e-03,\n",
       "        2.19952627e-03, 1.06107072e-03, 1.41992772e-03, 1.05182057e-03,\n",
       "        9.37965787e-04, 9.74420045e-04, 2.40044506e-03, 1.76994451e-03,\n",
       "        1.66373157e-03, 6.76290041e-04, 3.99037331e-03, 2.70559017e-03,\n",
       "        1.79822047e-03, 4.52399301e-03, 1.38227749e-03, 2.75280758e-03,\n",
       "        1.16329731e-03, 2.83435862e-03, 1.65194203e-03, 1.94513339e-03,\n",
       "        5.41083628e-04, 1.82086003e-03, 2.33841336e-03, 1.37568241e-03,\n",
       "        6.33624144e-04, 9.44307021e-04, 2.45266127e-03, 4.11159115e-03,\n",
       "        7.32989442e-04, 1.99960651e-03, 7.42262515e-04, 1.49279404e-03,\n",
       "        5.44845935e-04, 1.35896816e-03, 2.03032159e-03, 2.47393736e-03]),\n",
       " 'mean_score_time': array([9.99450684e-04, 0.00000000e+00, 0.00000000e+00, 4.01115417e-04,\n",
       "        1.99747086e-04, 5.91993332e-04, 4.01258469e-04, 1.99937820e-04,\n",
       "        4.01115417e-04, 8.00561905e-04, 5.99670410e-04, 6.00767136e-04,\n",
       "        1.99937820e-04, 2.00891495e-04, 5.92327118e-04, 7.52830505e-04,\n",
       "        2.26163864e-04, 4.41646576e-04, 5.05208969e-04, 6.08444214e-04,\n",
       "        5.99193573e-04, 2.00176239e-04, 2.04229355e-04, 7.99703598e-04,\n",
       "        5.11264801e-04, 8.11433792e-04, 4.00257111e-04, 2.03227997e-04,\n",
       "        1.99985504e-04, 4.00400162e-04, 6.06203079e-04, 0.00000000e+00,\n",
       "        7.06863403e-04, 4.03356552e-04, 2.00319290e-04, 6.06536865e-04,\n",
       "        4.06694412e-04, 6.71386719e-04, 0.00000000e+00, 6.04486465e-04,\n",
       "        4.69684601e-04, 6.02340698e-04, 6.03771210e-04, 2.03323364e-04,\n",
       "        2.00080872e-04, 1.00097656e-03, 1.20401382e-03, 6.15358353e-04,\n",
       "        7.17067719e-04, 8.08906555e-04, 0.00000000e+00, 9.90533829e-04,\n",
       "        4.13131714e-04, 2.00176239e-04, 1.08718872e-03, 1.20296478e-03,\n",
       "        9.06658173e-04, 8.35418701e-04, 8.03947449e-04, 5.99336624e-04,\n",
       "        7.99608231e-04, 4.43029404e-04, 8.16679001e-04, 6.86645508e-04,\n",
       "        6.03294373e-04, 2.41565704e-04, 8.11719894e-04, 6.59418106e-04,\n",
       "        7.09533691e-04, 4.00018692e-04, 8.08048248e-04, 5.06353378e-04,\n",
       "        6.08921051e-04, 5.95140457e-04, 6.01816177e-04, 6.11543655e-04,\n",
       "        1.01227760e-03, 4.00257111e-04, 3.25393677e-04, 3.01647186e-04,\n",
       "        2.01988220e-04, 4.01592255e-04, 6.07109070e-04, 7.03954697e-04,\n",
       "        3.99827957e-04, 7.23791122e-04, 6.04581833e-04, 6.16168976e-04,\n",
       "        7.10296631e-04, 2.02846527e-04, 5.42163849e-04, 6.05058670e-04,\n",
       "        3.89385223e-04, 6.06060028e-04, 3.01122665e-04, 4.03690338e-04,\n",
       "        4.01067734e-04, 6.03532791e-04, 8.18586349e-04, 6.03342056e-04,\n",
       "        2.00366974e-04, 3.99494171e-04, 2.03371048e-04, 8.28075409e-04,\n",
       "        1.00088120e-03, 8.04519653e-04, 1.24974251e-03, 6.27422333e-04,\n",
       "        5.43785095e-04, 3.12280655e-04, 3.99684906e-04, 6.02865219e-04,\n",
       "        8.11862946e-04, 0.00000000e+00, 2.00986862e-04, 6.06107712e-04,\n",
       "        0.00000000e+00, 4.06789780e-04, 1.99985504e-04, 4.00400162e-04,\n",
       "        6.30474091e-04, 5.28430939e-04, 0.00000000e+00, 4.00161743e-04,\n",
       "        6.03294373e-04, 4.01020050e-04, 5.98907471e-04, 3.07559967e-04,\n",
       "        2.02226639e-04, 4.11033630e-04, 9.32836533e-04, 1.03120804e-03,\n",
       "        6.01339340e-04, 2.03895569e-04, 1.99651718e-04, 5.08975983e-04,\n",
       "        8.03947449e-04, 8.23020935e-04, 4.00686264e-04, 6.00910187e-04,\n",
       "        5.85603714e-04, 8.04901123e-04, 4.06455994e-04, 2.45666504e-04,\n",
       "        6.01720810e-04, 6.03008270e-04, 4.04167175e-04, 6.68478012e-04,\n",
       "        1.42068863e-03, 5.99479675e-04, 1.09467506e-03, 1.01299286e-03,\n",
       "        5.47027588e-04, 1.50184631e-03, 5.01251221e-04, 5.98955154e-04,\n",
       "        1.00693703e-03, 6.81877136e-04, 8.03470612e-04, 8.06188583e-04,\n",
       "        8.08382034e-04, 1.96266174e-04, 4.06837463e-04, 8.05807114e-04,\n",
       "        3.85999680e-04, 1.06940269e-03, 5.04541397e-04, 8.25119019e-04,\n",
       "        5.03206253e-04, 8.10146332e-04, 6.00147247e-04, 8.09144974e-04,\n",
       "        2.03514099e-04, 6.03723526e-04, 5.25569916e-04, 6.00290298e-04,\n",
       "        0.00000000e+00, 3.02219391e-04, 3.11136246e-04, 3.96537781e-04,\n",
       "        3.05509567e-04, 8.02993774e-04, 6.04677200e-04, 8.20875168e-04,\n",
       "        5.21326065e-04, 2.00748444e-04, 0.00000000e+00, 1.99985504e-04,\n",
       "        3.96919250e-04, 2.02655792e-04, 9.38367844e-04, 9.92631912e-04,\n",
       "        4.11272049e-04, 6.14833832e-04, 1.08289719e-04, 8.12816620e-04,\n",
       "        6.13737106e-04, 6.70528412e-04, 1.09024048e-03, 4.57239151e-04,\n",
       "        5.64050674e-04, 1.12500191e-03, 3.07369232e-04, 1.99937820e-04,\n",
       "        9.28831100e-04, 4.31585312e-04, 0.00000000e+00, 6.07109070e-04,\n",
       "        2.00223923e-04, 2.00557709e-04, 2.03752518e-04, 2.02035904e-04,\n",
       "        4.09126282e-04, 6.09159470e-04, 0.00000000e+00, 3.90863419e-04,\n",
       "        2.00653076e-04, 6.07156754e-04, 3.12948227e-04, 4.04167175e-04,\n",
       "        4.06360626e-04, 4.00972366e-04, 6.61516190e-04, 1.00431442e-03,\n",
       "        8.01515579e-04, 8.05091858e-04, 7.13872910e-04, 4.03404236e-04,\n",
       "        3.05223465e-04, 6.02865219e-04, 1.99270248e-04, 3.03268433e-04,\n",
       "        6.02531433e-04, 9.01842117e-04, 7.11774826e-04, 7.07626343e-04,\n",
       "        7.02285767e-04, 3.67546082e-04, 4.00972366e-04, 3.99827957e-04,\n",
       "        4.01020050e-04, 1.99031830e-04, 5.84983826e-04, 2.00557709e-04,\n",
       "        5.79309464e-04, 3.98015976e-04, 3.98540497e-04, 7.31229782e-04,\n",
       "        6.05726242e-04, 1.97696686e-04, 3.86524200e-04, 5.08308411e-04,\n",
       "        1.99747086e-04, 7.04908371e-04, 9.98926163e-04, 9.98878479e-04,\n",
       "        1.10163689e-03, 1.31244659e-03, 1.01494789e-03, 7.24935532e-04,\n",
       "        7.95269012e-04, 4.07171249e-04, 9.11378860e-04, 6.06203079e-04,\n",
       "        7.09581375e-04, 1.43079758e-03, 9.16624069e-04, 5.92851639e-04,\n",
       "        2.08711624e-04, 0.00000000e+00, 1.96647644e-04, 5.21612167e-04,\n",
       "        7.39765167e-04, 2.00843811e-04, 2.00223923e-04, 3.96776199e-04,\n",
       "        3.99875641e-04, 7.01189041e-04, 3.37362289e-04, 4.52947617e-04,\n",
       "        5.77592850e-04, 3.35168839e-04, 2.18915939e-04, 7.58457184e-04,\n",
       "        3.82804871e-04, 5.88226318e-04, 4.14276123e-04, 1.04808807e-04,\n",
       "        5.25331497e-04, 5.95378876e-04, 4.04787064e-04, 7.87639618e-04,\n",
       "        6.04772568e-04, 9.31692123e-04, 7.98559189e-04, 1.09677315e-03,\n",
       "        6.01816177e-04, 1.99985504e-04, 3.99732590e-04, 4.08601761e-04,\n",
       "        7.08818436e-04, 7.07387924e-04, 9.08613205e-04, 6.00576401e-04,\n",
       "        1.32031441e-03, 6.02436066e-04, 4.00590897e-04, 3.98111343e-04,\n",
       "        6.19173050e-04, 3.95965576e-04, 2.02798843e-04, 2.02178955e-05,\n",
       "        5.13219833e-04, 5.92756271e-04, 1.91116333e-04, 8.00514221e-04,\n",
       "        5.99670410e-04, 1.00088120e-03, 1.00736618e-03, 7.94363022e-04,\n",
       "        8.13770294e-04, 1.08528137e-04, 7.98225403e-04, 9.84048843e-04,\n",
       "        5.00869751e-04, 9.29737091e-04, 1.18994713e-03, 5.89704514e-04,\n",
       "        6.15024567e-04, 7.99512863e-04, 3.95917892e-04, 1.24473572e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 7.08484650e-04, 2.00548172e-03,\n",
       "        7.99608231e-04, 4.14752960e-04, 6.01243973e-04, 7.92074203e-04,\n",
       "        4.11891937e-04, 5.99527359e-04, 4.00018692e-04, 1.10659599e-03,\n",
       "        8.07142258e-04, 1.30610466e-03, 9.97591019e-04, 1.59521103e-03,\n",
       "        7.98702240e-04, 5.96332550e-04, 4.03165817e-04, 3.99875641e-04,\n",
       "        3.96966934e-04, 9.96160507e-04, 9.10091400e-04, 1.31082535e-03,\n",
       "        1.09133720e-03, 1.26557350e-03, 8.19635391e-04, 8.15963745e-04,\n",
       "        6.09922409e-04, 8.61024857e-04, 5.81645966e-04, 1.00731850e-03,\n",
       "        1.21064186e-03, 4.24242020e-04, 8.67414474e-04, 9.96303558e-04,\n",
       "        5.42163849e-04, 7.56645203e-04, 6.10065460e-04, 1.04422569e-03,\n",
       "        8.16774368e-04, 9.24921036e-04, 9.68694687e-04, 2.01129913e-04,\n",
       "        1.21531487e-03, 1.56245232e-03, 2.30512619e-03, 9.97304916e-04,\n",
       "        1.40790939e-03, 1.23252869e-03, 1.14107132e-04, 1.42207146e-03,\n",
       "        6.47830963e-04, 8.06522369e-04, 8.14723969e-04, 3.84330750e-04,\n",
       "        1.11875534e-03, 4.02927399e-04, 1.00345612e-03, 1.02424622e-03,\n",
       "        1.02362633e-03, 4.11605835e-04, 4.00686264e-04, 1.00769997e-03,\n",
       "        7.98654556e-04, 7.49731064e-04, 2.71224976e-04, 1.13902092e-03]),\n",
       " 'std_score_time': array([6.31432817e-04, 0.00000000e+00, 0.00000000e+00, 4.91264050e-04,\n",
       "        3.99494171e-04, 4.83427943e-04, 4.91440229e-04, 3.99875641e-04,\n",
       "        4.91264421e-04, 4.00282042e-04, 4.89629490e-04, 4.90527704e-04,\n",
       "        3.99875641e-04, 4.01782990e-04, 4.83640832e-04, 4.14800685e-04,\n",
       "        2.77512000e-04, 2.23026307e-04, 4.51103123e-04, 4.96817342e-04,\n",
       "        4.89343999e-04, 4.00352478e-04, 4.08458710e-04, 3.99854283e-04,\n",
       "        4.51268332e-04, 4.05801970e-04, 4.90212936e-04, 4.06455994e-04,\n",
       "        3.99971008e-04, 4.90388514e-04, 4.95006985e-04, 0.00000000e+00,\n",
       "        3.94927704e-04, 4.94034694e-04, 4.00638580e-04, 4.95387384e-04,\n",
       "        4.98097038e-04, 4.03813877e-04, 0.00000000e+00, 4.93614046e-04,\n",
       "        5.83407876e-04, 4.91827251e-04, 4.93113179e-04, 4.06646729e-04,\n",
       "        4.00161743e-04, 1.88697423e-06, 3.98723208e-04, 4.74019404e-04,\n",
       "        6.03228229e-04, 5.02793556e-04, 0.00000000e+00, 5.36034803e-04,\n",
       "        5.06408659e-04, 4.00352478e-04, 3.36197149e-04, 3.93062298e-04,\n",
       "        9.21170387e-04, 4.23045159e-04, 4.01975506e-04, 4.89356893e-04,\n",
       "        3.99804609e-04, 3.64208971e-04, 2.46793916e-04, 6.50915330e-04,\n",
       "        4.92632572e-04, 3.88905340e-04, 4.05934741e-04, 5.45748625e-04,\n",
       "        3.69820126e-04, 4.89920894e-04, 4.04141178e-04, 6.42837336e-04,\n",
       "        4.97217725e-04, 4.85982887e-04, 4.91381221e-04, 4.99785648e-04,\n",
       "        8.61664668e-06, 4.90214722e-04, 6.50787354e-04, 4.00526677e-04,\n",
       "        4.03976440e-04, 4.91852585e-04, 4.95767174e-04, 6.03429720e-04,\n",
       "        4.89687246e-04, 6.29152477e-04, 4.93715969e-04, 5.03613108e-04,\n",
       "        4.05521154e-04, 4.05693054e-04, 4.96812125e-04, 4.94061572e-04,\n",
       "        4.77185918e-04, 4.94909376e-04, 6.02245331e-04, 4.94442711e-04,\n",
       "        4.91206119e-04, 4.92789589e-04, 4.10829115e-04, 4.92640488e-04,\n",
       "        4.00733948e-04, 4.89278646e-04, 4.06742096e-04, 4.14551141e-04,\n",
       "        2.67815283e-05, 4.02339207e-04, 3.93412659e-04, 3.53006081e-04,\n",
       "        3.28283334e-04, 4.11309639e-04, 4.89512410e-04, 4.92286012e-04,\n",
       "        4.06029413e-04, 0.00000000e+00, 4.01973724e-04, 4.94941751e-04,\n",
       "        0.00000000e+00, 4.98213702e-04, 3.99971008e-04, 4.90389024e-04,\n",
       "        6.15150126e-04, 6.75059589e-04, 0.00000000e+00, 4.90096043e-04,\n",
       "        4.92588654e-04, 4.91147828e-04, 4.89007909e-04, 4.04301985e-04,\n",
       "        4.04453278e-04, 5.95687835e-04, 5.02811717e-04, 6.37052505e-04,\n",
       "        4.90999534e-04, 4.07791138e-04, 3.99303436e-04, 4.53715289e-04,\n",
       "        4.02007464e-04, 7.02670851e-04, 4.90740121e-04, 4.90642789e-04,\n",
       "        4.79200073e-04, 4.02483978e-04, 3.76135951e-04, 3.66514100e-04,\n",
       "        4.91308568e-04, 4.92371978e-04, 4.95073694e-04, 5.57638996e-04,\n",
       "        4.62981929e-04, 4.89474623e-04, 2.75068779e-04, 5.80194116e-04,\n",
       "        3.24165656e-04, 5.14389229e-04, 6.34409262e-04, 4.89045208e-04,\n",
       "        6.88794535e-06, 5.76490662e-04, 4.01787896e-04, 7.56909647e-04,\n",
       "        4.04304532e-04, 3.92532349e-04, 4.98272188e-04, 4.02957656e-04,\n",
       "        4.73302760e-04, 1.20152613e-04, 4.50468959e-04, 7.06301001e-04,\n",
       "        4.47073772e-04, 4.05215420e-04, 4.90020248e-04, 4.04700306e-04,\n",
       "        4.07028198e-04, 4.93076585e-04, 6.70337969e-04, 4.90135193e-04,\n",
       "        0.00000000e+00, 4.01522954e-04, 4.10118046e-04, 4.85683105e-04,\n",
       "        4.03779714e-04, 4.01510790e-04, 4.93740515e-04, 7.55795334e-04,\n",
       "        6.63739162e-04, 4.01496887e-04, 0.00000000e+00, 3.99971008e-04,\n",
       "        4.86148763e-04, 4.05311584e-04, 5.17102767e-04, 6.33562653e-04,\n",
       "        4.81358455e-04, 5.02204193e-04, 2.16579437e-04, 4.06729110e-04,\n",
       "        5.01778902e-04, 6.87659009e-04, 3.85500427e-04, 2.33200253e-04,\n",
       "        3.66159661e-04, 2.37631720e-04, 4.08564762e-04, 3.99875641e-04,\n",
       "        5.19663808e-04, 5.30176587e-04, 0.00000000e+00, 4.95753461e-04,\n",
       "        4.00447845e-04, 4.01115417e-04, 4.07505035e-04, 4.04071808e-04,\n",
       "        5.01113453e-04, 3.75673092e-04, 0.00000000e+00, 4.78721082e-04,\n",
       "        4.01306152e-04, 4.95787307e-04, 4.11354225e-04, 4.95050271e-04,\n",
       "        4.97695494e-04, 4.91099550e-04, 5.80996474e-04, 7.12005861e-06,\n",
       "        7.54240518e-04, 4.02616838e-04, 4.08490321e-04, 4.94071779e-04,\n",
       "        4.05938572e-04, 4.92258669e-04, 3.98540497e-04, 4.02474968e-04,\n",
       "        4.92029486e-04, 4.94412272e-04, 6.03811827e-04, 4.02254864e-04,\n",
       "        6.02882034e-04, 4.53465835e-04, 4.91092466e-04, 4.89687385e-04,\n",
       "        4.91153175e-04, 3.98063660e-04, 4.78354201e-04, 4.01115417e-04,\n",
       "        4.73690364e-04, 4.87471110e-04, 4.88111920e-04, 6.37865443e-04,\n",
       "        4.94600215e-04, 3.95393372e-04, 4.73891252e-04, 4.47443750e-04,\n",
       "        3.99494171e-04, 9.89726652e-04, 8.93083491e-04, 1.74290341e-06,\n",
       "        6.64646646e-04, 6.15888408e-04, 5.69696652e-04, 6.35223155e-04,\n",
       "        3.98036561e-04, 4.98807415e-04, 5.11100941e-04, 4.95097282e-04,\n",
       "        8.79048705e-04, 3.89737256e-04, 6.63934172e-04, 4.85088965e-04,\n",
       "        4.17423248e-04, 0.00000000e+00, 3.93295288e-04, 6.70148177e-04,\n",
       "        6.25265302e-04, 4.01687622e-04, 4.00447845e-04, 4.85980782e-04,\n",
       "        4.89759011e-04, 3.97783776e-04, 2.51107536e-04, 2.32756052e-04,\n",
       "        5.15986721e-05, 4.29085578e-04, 2.69275073e-04, 2.95166376e-04,\n",
       "        2.54841987e-04, 3.55276881e-04, 3.92326523e-04, 2.09617615e-04,\n",
       "        4.43235480e-04, 4.86208602e-04, 4.95847606e-04, 7.27227547e-04,\n",
       "        4.93826098e-04, 5.06269385e-04, 3.99303465e-04, 6.59194206e-04,\n",
       "        4.91523558e-04, 3.99971008e-04, 4.89570724e-04, 5.00562391e-04,\n",
       "        3.96902649e-04, 3.95132140e-04, 6.58218610e-04, 4.90369977e-04,\n",
       "        6.21405202e-04, 4.91913736e-04, 4.90621652e-04, 4.87591967e-04,\n",
       "        4.68704537e-04, 4.84980813e-04, 4.05597687e-04, 4.04357910e-05,\n",
       "        4.53362507e-04, 4.84304993e-04, 3.82232666e-04, 4.00394588e-04,\n",
       "        4.89708544e-04, 8.86583778e-06, 1.18831034e-05, 3.99266587e-04,\n",
       "        5.07228955e-04, 2.17056274e-04, 7.44265730e-04, 6.09866292e-04,\n",
       "        4.39870103e-04, 5.25849796e-04, 7.45682028e-04, 4.81517250e-04,\n",
       "        4.71076274e-04, 4.00248107e-04, 4.84911356e-04, 7.98618133e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.95554131e-04, 9.14319013e-04,\n",
       "        3.99822381e-04, 4.80591717e-04, 4.91354637e-04, 3.96300117e-04,\n",
       "        5.04589288e-04, 4.89512619e-04, 4.89920847e-04, 6.68044427e-04,\n",
       "        7.50628894e-04, 7.53280207e-04, 1.54552626e-05, 7.92635529e-04,\n",
       "        3.99398103e-04, 4.86941997e-04, 4.93809393e-04, 4.89746221e-04,\n",
       "        4.86211048e-04, 8.03636851e-06, 1.85300748e-04, 3.94295836e-04,\n",
       "        2.14700430e-04, 3.72511994e-04, 2.79368771e-04, 2.41623725e-04,\n",
       "        3.70641060e-04, 4.00721749e-04, 4.75220098e-04, 6.28016297e-04,\n",
       "        4.06162894e-04, 5.21613028e-04, 3.30223500e-04, 8.05650776e-05,\n",
       "        4.68063631e-04, 6.84427590e-04, 4.98400029e-04, 5.14574241e-05,\n",
       "        3.97901606e-04, 5.12005963e-04, 3.26161496e-05, 4.02259827e-04,\n",
       "        4.20580418e-04, 9.93306002e-04, 1.69712645e-03, 6.30640280e-04,\n",
       "        4.90799720e-04, 4.65208270e-04, 2.28214264e-04, 1.02606337e-03,\n",
       "        4.77210171e-04, 7.53161739e-04, 4.08442978e-04, 4.70707718e-04,\n",
       "        8.26746483e-04, 4.93503999e-04, 2.03500558e-05, 9.23003367e-04,\n",
       "        5.63894055e-04, 5.04164092e-04, 4.90738916e-04, 9.74732599e-06,\n",
       "        3.99332926e-04, 9.21325522e-04, 3.48548119e-04, 6.88975269e-04]),\n",
       " 'param_C': masked_array(data=[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 1.0, 1.0, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_class_weight': masked_array(data=[{0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1},\n",
       "                    {0: 1, 1: 10}, {0: 1, 1: 10}, {0: 1, 1: 10},\n",
       "                    {0: 1, 1: 50}, {0: 1, 1: 50}, {0: 1, 1: 50},\n",
       "                    {0: 1, 1: 100}, {0: 1, 1: 100}, {0: 1, 1: 100},\n",
       "                    {0: 10, 1: 1}, {0: 10, 1: 1}, {0: 10, 1: 1},\n",
       "                    {0: 10, 1: 10}, {0: 10, 1: 10}, {0: 10, 1: 10},\n",
       "                    {0: 10, 1: 50}, {0: 10, 1: 50}, {0: 10, 1: 50},\n",
       "                    {0: 10, 1: 100}, {0: 10, 1: 100}, {0: 10, 1: 100},\n",
       "                    {0: 50, 1: 1}, {0: 50, 1: 1}, {0: 50, 1: 1},\n",
       "                    {0: 50, 1: 10}, {0: 50, 1: 10}, {0: 50, 1: 10},\n",
       "                    {0: 50, 1: 50}, {0: 50, 1: 50}, {0: 50, 1: 50},\n",
       "                    {0: 50, 1: 100}, {0: 50, 1: 100}, {0: 50, 1: 100},\n",
       "                    {0: 100, 1: 1}, {0: 100, 1: 1}, {0: 100, 1: 1},\n",
       "                    {0: 100, 1: 10}, {0: 100, 1: 10}, {0: 100, 1: 10},\n",
       "                    {0: 100, 1: 50}, {0: 100, 1: 50}, {0: 100, 1: 50},\n",
       "                    {0: 100, 1: 100}, {0: 100, 1: 100}, {0: 100, 1: 100},\n",
       "                    {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1},\n",
       "                    {0: 1, 1: 10}, {0: 1, 1: 10}, {0: 1, 1: 10},\n",
       "                    {0: 1, 1: 50}, {0: 1, 1: 50}, {0: 1, 1: 50},\n",
       "                    {0: 1, 1: 100}, {0: 1, 1: 100}, {0: 1, 1: 100},\n",
       "                    {0: 10, 1: 1}, {0: 10, 1: 1}, {0: 10, 1: 1},\n",
       "                    {0: 10, 1: 10}, {0: 10, 1: 10}, {0: 10, 1: 10},\n",
       "                    {0: 10, 1: 50}, {0: 10, 1: 50}, {0: 10, 1: 50},\n",
       "                    {0: 10, 1: 100}, {0: 10, 1: 100}, {0: 10, 1: 100},\n",
       "                    {0: 50, 1: 1}, {0: 50, 1: 1}, {0: 50, 1: 1},\n",
       "                    {0: 50, 1: 10}, {0: 50, 1: 10}, {0: 50, 1: 10},\n",
       "                    {0: 50, 1: 50}, {0: 50, 1: 50}, {0: 50, 1: 50},\n",
       "                    {0: 50, 1: 100}, {0: 50, 1: 100}, {0: 50, 1: 100},\n",
       "                    {0: 100, 1: 1}, {0: 100, 1: 1}, {0: 100, 1: 1},\n",
       "                    {0: 100, 1: 10}, {0: 100, 1: 10}, {0: 100, 1: 10},\n",
       "                    {0: 100, 1: 50}, {0: 100, 1: 50}, {0: 100, 1: 50},\n",
       "                    {0: 100, 1: 100}, {0: 100, 1: 100}, {0: 100, 1: 100},\n",
       "                    {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1},\n",
       "                    {0: 1, 1: 10}, {0: 1, 1: 10}, {0: 1, 1: 10},\n",
       "                    {0: 1, 1: 50}, {0: 1, 1: 50}, {0: 1, 1: 50},\n",
       "                    {0: 1, 1: 100}, {0: 1, 1: 100}, {0: 1, 1: 100},\n",
       "                    {0: 10, 1: 1}, {0: 10, 1: 1}, {0: 10, 1: 1},\n",
       "                    {0: 10, 1: 10}, {0: 10, 1: 10}, {0: 10, 1: 10},\n",
       "                    {0: 10, 1: 50}, {0: 10, 1: 50}, {0: 10, 1: 50},\n",
       "                    {0: 10, 1: 100}, {0: 10, 1: 100}, {0: 10, 1: 100},\n",
       "                    {0: 50, 1: 1}, {0: 50, 1: 1}, {0: 50, 1: 1},\n",
       "                    {0: 50, 1: 10}, {0: 50, 1: 10}, {0: 50, 1: 10},\n",
       "                    {0: 50, 1: 50}, {0: 50, 1: 50}, {0: 50, 1: 50},\n",
       "                    {0: 50, 1: 100}, {0: 50, 1: 100}, {0: 50, 1: 100},\n",
       "                    {0: 100, 1: 1}, {0: 100, 1: 1}, {0: 100, 1: 1},\n",
       "                    {0: 100, 1: 10}, {0: 100, 1: 10}, {0: 100, 1: 10},\n",
       "                    {0: 100, 1: 50}, {0: 100, 1: 50}, {0: 100, 1: 50},\n",
       "                    {0: 100, 1: 100}, {0: 100, 1: 100}, {0: 100, 1: 100},\n",
       "                    {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1},\n",
       "                    {0: 1, 1: 10}, {0: 1, 1: 10}, {0: 1, 1: 10},\n",
       "                    {0: 1, 1: 50}, {0: 1, 1: 50}, {0: 1, 1: 50},\n",
       "                    {0: 1, 1: 100}, {0: 1, 1: 100}, {0: 1, 1: 100},\n",
       "                    {0: 10, 1: 1}, {0: 10, 1: 1}, {0: 10, 1: 1},\n",
       "                    {0: 10, 1: 10}, {0: 10, 1: 10}, {0: 10, 1: 10},\n",
       "                    {0: 10, 1: 50}, {0: 10, 1: 50}, {0: 10, 1: 50},\n",
       "                    {0: 10, 1: 100}, {0: 10, 1: 100}, {0: 10, 1: 100},\n",
       "                    {0: 50, 1: 1}, {0: 50, 1: 1}, {0: 50, 1: 1},\n",
       "                    {0: 50, 1: 10}, {0: 50, 1: 10}, {0: 50, 1: 10},\n",
       "                    {0: 50, 1: 50}, {0: 50, 1: 50}, {0: 50, 1: 50},\n",
       "                    {0: 50, 1: 100}, {0: 50, 1: 100}, {0: 50, 1: 100},\n",
       "                    {0: 100, 1: 1}, {0: 100, 1: 1}, {0: 100, 1: 1},\n",
       "                    {0: 100, 1: 10}, {0: 100, 1: 10}, {0: 100, 1: 10},\n",
       "                    {0: 100, 1: 50}, {0: 100, 1: 50}, {0: 100, 1: 50},\n",
       "                    {0: 100, 1: 100}, {0: 100, 1: 100}, {0: 100, 1: 100},\n",
       "                    {0: 1, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 1},\n",
       "                    {0: 1, 1: 10}, {0: 1, 1: 10}, {0: 1, 1: 10},\n",
       "                    {0: 1, 1: 50}, {0: 1, 1: 50}, {0: 1, 1: 50},\n",
       "                    {0: 1, 1: 100}, {0: 1, 1: 100}, {0: 1, 1: 100},\n",
       "                    {0: 10, 1: 1}, {0: 10, 1: 1}, {0: 10, 1: 1},\n",
       "                    {0: 10, 1: 10}, {0: 10, 1: 10}, {0: 10, 1: 10},\n",
       "                    {0: 10, 1: 50}, {0: 10, 1: 50}, {0: 10, 1: 50},\n",
       "                    {0: 10, 1: 100}, {0: 10, 1: 100}, {0: 10, 1: 100},\n",
       "                    {0: 50, 1: 1}, {0: 50, 1: 1}, {0: 50, 1: 1},\n",
       "                    {0: 50, 1: 10}, {0: 50, 1: 10}, {0: 50, 1: 10},\n",
       "                    {0: 50, 1: 50}, {0: 50, 1: 50}, {0: 50, 1: 50},\n",
       "                    {0: 50, 1: 100}, {0: 50, 1: 100}, {0: 50, 1: 100},\n",
       "                    {0: 100, 1: 1}, {0: 100, 1: 1}, {0: 100, 1: 1},\n",
       "                    {0: 100, 1: 10}, {0: 100, 1: 10}, {0: 100, 1: 10},\n",
       "                    {0: 100, 1: 50}, {0: 100, 1: 50}, {0: 100, 1: 50},\n",
       "                    {0: 100, 1: 100}, {0: 100, 1: 100}, {0: 100, 1: 100},\n",
       "                    {0: 1, 1: 1}, {0: 1, 1: 10}, {0: 1, 1: 50},\n",
       "                    {0: 1, 1: 100}, {0: 10, 1: 1}, {0: 10, 1: 10},\n",
       "                    {0: 10, 1: 50}, {0: 10, 1: 100}, {0: 50, 1: 1},\n",
       "                    {0: 50, 1: 10}, {0: 50, 1: 50}, {0: 50, 1: 100},\n",
       "                    {0: 100, 1: 1}, {0: 100, 1: 10}, {0: 100, 1: 50},\n",
       "                    {0: 100, 1: 100}, {0: 1, 1: 1}, {0: 1, 1: 10},\n",
       "                    {0: 1, 1: 50}, {0: 1, 1: 100}, {0: 10, 1: 1},\n",
       "                    {0: 10, 1: 10}, {0: 10, 1: 50}, {0: 10, 1: 100},\n",
       "                    {0: 50, 1: 1}, {0: 50, 1: 10}, {0: 50, 1: 50},\n",
       "                    {0: 50, 1: 100}, {0: 100, 1: 1}, {0: 100, 1: 10},\n",
       "                    {0: 100, 1: 50}, {0: 100, 1: 100}, {0: 1, 1: 1},\n",
       "                    {0: 1, 1: 10}, {0: 1, 1: 50}, {0: 1, 1: 100},\n",
       "                    {0: 10, 1: 1}, {0: 10, 1: 10}, {0: 10, 1: 50},\n",
       "                    {0: 10, 1: 100}, {0: 50, 1: 1}, {0: 50, 1: 10},\n",
       "                    {0: 50, 1: 50}, {0: 50, 1: 100}, {0: 100, 1: 1},\n",
       "                    {0: 100, 1: 10}, {0: 100, 1: 50}, {0: 100, 1: 100},\n",
       "                    {0: 1, 1: 1}, {0: 1, 1: 10}, {0: 1, 1: 50},\n",
       "                    {0: 1, 1: 100}, {0: 10, 1: 1}, {0: 10, 1: 10},\n",
       "                    {0: 10, 1: 50}, {0: 10, 1: 100}, {0: 50, 1: 1},\n",
       "                    {0: 50, 1: 10}, {0: 50, 1: 50}, {0: 50, 1: 100},\n",
       "                    {0: 100, 1: 1}, {0: 100, 1: 10}, {0: 100, 1: 50},\n",
       "                    {0: 100, 1: 100}, {0: 1, 1: 1}, {0: 1, 1: 10},\n",
       "                    {0: 1, 1: 50}, {0: 1, 1: 100}, {0: 10, 1: 1},\n",
       "                    {0: 10, 1: 10}, {0: 10, 1: 50}, {0: 10, 1: 100},\n",
       "                    {0: 50, 1: 1}, {0: 50, 1: 10}, {0: 50, 1: 50},\n",
       "                    {0: 50, 1: 100}, {0: 100, 1: 1}, {0: 100, 1: 10},\n",
       "                    {0: 100, 1: 50}, {0: 100, 1: 100}, {0: 1, 1: 1},\n",
       "                    {0: 1, 1: 10}, {0: 1, 1: 50}, {0: 1, 1: 100},\n",
       "                    {0: 10, 1: 1}, {0: 10, 1: 10}, {0: 10, 1: 50},\n",
       "                    {0: 10, 1: 100}, {0: 50, 1: 1}, {0: 50, 1: 10},\n",
       "                    {0: 50, 1: 50}, {0: 50, 1: 100}, {0: 100, 1: 1},\n",
       "                    {0: 100, 1: 10}, {0: 100, 1: 50}, {0: 100, 1: 100},\n",
       "                    {0: 1, 1: 1}, {0: 1, 1: 10}, {0: 1, 1: 50},\n",
       "                    {0: 1, 1: 100}, {0: 10, 1: 1}, {0: 10, 1: 10},\n",
       "                    {0: 10, 1: 50}, {0: 10, 1: 100}, {0: 50, 1: 1},\n",
       "                    {0: 50, 1: 10}, {0: 50, 1: 50}, {0: 50, 1: 100},\n",
       "                    {0: 100, 1: 1}, {0: 100, 1: 10}, {0: 100, 1: 50},\n",
       "                    {0: 100, 1: 100}, {0: 1, 1: 1}, {0: 1, 1: 10},\n",
       "                    {0: 1, 1: 50}, {0: 1, 1: 100}, {0: 10, 1: 1},\n",
       "                    {0: 10, 1: 10}, {0: 10, 1: 50}, {0: 10, 1: 100},\n",
       "                    {0: 50, 1: 1}, {0: 50, 1: 10}, {0: 50, 1: 50},\n",
       "                    {0: 50, 1: 100}, {0: 100, 1: 1}, {0: 100, 1: 10},\n",
       "                    {0: 100, 1: 50}, {0: 100, 1: 100}, {0: 1, 1: 1},\n",
       "                    {0: 1, 1: 10}, {0: 1, 1: 50}, {0: 1, 1: 100},\n",
       "                    {0: 10, 1: 1}, {0: 10, 1: 10}, {0: 10, 1: 50},\n",
       "                    {0: 10, 1: 100}, {0: 50, 1: 1}, {0: 50, 1: 10},\n",
       "                    {0: 50, 1: 50}, {0: 50, 1: 100}, {0: 100, 1: 1},\n",
       "                    {0: 100, 1: 10}, {0: 100, 1: 50}, {0: 100, 1: 100},\n",
       "                    {0: 1, 1: 1}, {0: 1, 1: 10}, {0: 1, 1: 50},\n",
       "                    {0: 1, 1: 100}, {0: 10, 1: 1}, {0: 10, 1: 10},\n",
       "                    {0: 10, 1: 50}, {0: 10, 1: 100}, {0: 50, 1: 1},\n",
       "                    {0: 50, 1: 10}, {0: 50, 1: 50}, {0: 50, 1: 100},\n",
       "                    {0: 100, 1: 1}, {0: 100, 1: 10}, {0: 100, 1: 50},\n",
       "                    {0: 100, 1: 100}],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_penalty': masked_array(data=['l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l1', 'l1', 'l1',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1',\n",
       "                    'l1', 'l1', 'l1', 'l1', 'l1', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet', 'elasticnet',\n",
       "                    'elasticnet', 'elasticnet', 'elasticnet'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_solver': masked_array(data=['newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'newton-cg', 'lbfgs', 'sag', 'newton-cg', 'lbfgs',\n",
       "                    'sag', 'newton-cg', 'lbfgs', 'sag', 'newton-cg',\n",
       "                    'lbfgs', 'sag', 'newton-cg', 'lbfgs', 'sag',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_l1_ratio': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5],\n",
       "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 100,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100, 'class_weight': {0: 1, 1: 1}, 'penalty': 'l2', 'solver': 'lbfgs'},\n",
       "  {'C': 100, 'class_weight': {0: 1, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 100, 'class_weight': {0: 1, 1: 10}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 100, 'class_weight': {0: 1, 1: 50}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 100, 'class_weight': {0: 1, 1: 100}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 100, 'class_weight': {0: 10, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 100, 'class_weight': {0: 10, 1: 10}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 100, 'class_weight': {0: 10, 1: 50}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 100, 'class_weight': {0: 50, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 100, 'class_weight': {0: 50, 1: 10}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 100, 'class_weight': {0: 50, 1: 50}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 100, 'class_weight': {0: 100, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10, 'class_weight': {0: 1, 1: 1}, 'penalty': 'l2', 'solver': 'lbfgs'},\n",
       "  {'C': 10, 'class_weight': {0: 1, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10, 'class_weight': {0: 1, 1: 10}, 'penalty': 'l2', 'solver': 'lbfgs'},\n",
       "  {'C': 10, 'class_weight': {0: 1, 1: 10}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10, 'class_weight': {0: 1, 1: 50}, 'penalty': 'l2', 'solver': 'lbfgs'},\n",
       "  {'C': 10, 'class_weight': {0: 1, 1: 50}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 10, 'class_weight': {0: 1, 1: 100}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10, 'class_weight': {0: 10, 1: 1}, 'penalty': 'l2', 'solver': 'lbfgs'},\n",
       "  {'C': 10, 'class_weight': {0: 10, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 10, 'class_weight': {0: 10, 1: 10}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 10, 'class_weight': {0: 10, 1: 50}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 10, 'class_weight': {0: 10, 1: 100}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10, 'class_weight': {0: 50, 1: 1}, 'penalty': 'l2', 'solver': 'lbfgs'},\n",
       "  {'C': 10, 'class_weight': {0: 50, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 10, 'class_weight': {0: 50, 1: 10}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 10, 'class_weight': {0: 50, 1: 50}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 10, 'class_weight': {0: 50, 1: 100}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 10, 'class_weight': {0: 100, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 10, 'class_weight': {0: 100, 1: 10}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 10, 'class_weight': {0: 100, 1: 50}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0, 'class_weight': {0: 1, 1: 1}, 'penalty': 'l2', 'solver': 'lbfgs'},\n",
       "  {'C': 1.0, 'class_weight': {0: 1, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 1.0, 'class_weight': {0: 1, 1: 10}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 1.0, 'class_weight': {0: 1, 1: 50}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 1.0, 'class_weight': {0: 1, 1: 100}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 1.0, 'class_weight': {0: 10, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 1.0, 'class_weight': {0: 10, 1: 10}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 1.0, 'class_weight': {0: 10, 1: 50}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 1.0, 'class_weight': {0: 50, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 1.0, 'class_weight': {0: 50, 1: 10}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 1.0, 'class_weight': {0: 50, 1: 50}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 1.0, 'class_weight': {0: 100, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1, 'class_weight': {0: 1, 1: 1}, 'penalty': 'l2', 'solver': 'lbfgs'},\n",
       "  {'C': 0.1, 'class_weight': {0: 1, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.1, 'class_weight': {0: 1, 1: 10}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.1, 'class_weight': {0: 1, 1: 50}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.1, 'class_weight': {0: 1, 1: 100}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.1, 'class_weight': {0: 10, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.1, 'class_weight': {0: 10, 1: 10}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.1, 'class_weight': {0: 10, 1: 50}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.1, 'class_weight': {0: 50, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.1, 'class_weight': {0: 50, 1: 10}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.1, 'class_weight': {0: 50, 1: 50}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.1, 'class_weight': {0: 100, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01, 'class_weight': {0: 1, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01, 'class_weight': {0: 1, 1: 10}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01, 'class_weight': {0: 1, 1: 50}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01, 'class_weight': {0: 10, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01, 'class_weight': {0: 50, 1: 1}, 'penalty': 'l2', 'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'newton-cg'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'lbfgs'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l2',\n",
       "   'solver': 'sag'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'penalty': 'l1',\n",
       "   'solver': 'liblinear'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 100,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 10,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 1.0,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.1,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 1, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 10, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 50, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 1},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 10},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 50},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'},\n",
       "  {'C': 0.01,\n",
       "   'class_weight': {0: 100, 1: 100},\n",
       "   'l1_ratio': 0.5,\n",
       "   'penalty': 'elasticnet',\n",
       "   'solver': 'saga'}],\n",
       " 'split0_test_score': array([0.9875 , 0.9875 , 0.9875 , 0.98125, 0.98125, 0.9875 , 0.70625,\n",
       "        0.70625, 0.1875 , 0.25625, 0.25625, 0.08125, 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.86875,\n",
       "        0.98125, 0.98125, 0.8    , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.975  , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.98125, 0.98125, 0.95   , 0.70625, 0.70625,\n",
       "        0.71875, 0.25625, 0.25625, 0.96875, 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.98125, 0.98125,\n",
       "        0.98125, 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.96875, 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.98125,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.98125, 0.98125, 0.96875, 0.70625, 0.70625, 0.5875 ,\n",
       "        0.25625, 0.25625, 0.94375, 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.95625, 0.98125, 0.98125,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.95   ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.96875, 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.94375, 0.9875 , 0.9875 , 0.89375, 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.98125, 0.98125, 0.98125, 0.7125 , 0.7125 , 0.5    , 0.25   ,\n",
       "        0.25   , 0.28125, 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.96875, 0.98125, 0.98125, 0.98125,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.975  , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.975  ,\n",
       "        0.9875 , 0.9875 , 0.96875, 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.725  , 0.725  , 0.725  , 0.18125, 0.18125,\n",
       "        0.59375, 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.98125, 0.98125, 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.98125, 0.70625, 0.25625, 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.98125, 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.98125, 0.70625,\n",
       "        0.25625, 0.9875 , 0.9875 , 0.9875 , 0.98125, 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.98125, 0.7125 , 0.25   , 0.9875 , 0.9875 , 0.9875 , 0.98125,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.98125, 0.7125 , 0.21875, 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.98125, 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.675  , 0.1375 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.98125, 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.98125,\n",
       "        0.64375, 0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.96875, 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.9875 , 0.98125, 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.98125, 0.43125, 0.86875, 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.825  , 0.9875 , 0.9625 , 0.9875 , 0.98125, 0.9875 , 0.98125,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.98125, 0.43125, 0.25   , 0.9875 ,\n",
       "        0.9875 , 0.95   , 0.9875 , 0.9875 , 0.9875 , 0.975  , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.98125, 0.98125, 0.9875 , 0.98125, 0.6    ,\n",
       "        0.9125 , 0.9875 , 0.9875 , 0.9875 , 0.98125, 0.9875 , 0.9875 ,\n",
       "        0.98125, 0.125  , 0.9875 , 0.9875 , 0.98125, 0.98125, 0.9875 ,\n",
       "        0.9875 , 0.98125, 0.00625, 0.9875 , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 , 0.9875 , 0.9875 , 0.975  , 0.9875 , 0.9875 , 0.9875 ,\n",
       "        0.9875 ]),\n",
       " 'split1_test_score': array([0.98125, 0.98125, 0.98125, 0.95625, 0.95625, 0.98125, 0.7875 ,\n",
       "        0.7875 , 0.45   , 0.49375, 0.49375, 0.83125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.975  , 0.975  , 0.98125,\n",
       "        0.95625, 0.95625, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.9625 , 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.9625 ,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.93125, 0.98125,\n",
       "        0.98125, 0.98125, 0.95625, 0.95625, 0.95   , 0.7875 , 0.7875 ,\n",
       "        0.9125 , 0.49375, 0.49375, 0.95625, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.975  , 0.975  , 0.94375, 0.95625,\n",
       "        0.95625, 0.725  , 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.975  , 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.9625 ,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.95625, 0.95625, 0.86875, 0.79375, 0.79375, 0.375  ,\n",
       "        0.49375, 0.49375, 0.7625 , 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.975  , 0.975  , 0.89375, 0.95625, 0.95625,\n",
       "        0.83125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.96875, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.975  , 0.975  , 0.975  , 0.8    , 0.8    , 0.7    , 0.48125,\n",
       "        0.48125, 0.03125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.975  , 0.975  , 0.975  , 0.95625, 0.95625, 0.9875 ,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.975  , 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.8    , 0.8    , 0.8    , 0.44375, 0.44375,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.975  , 0.975  , 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.95625, 0.7875 , 0.49375, 0.98125,\n",
       "        0.98125, 0.975  , 0.95625, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.95625, 0.7875 ,\n",
       "        0.49375, 0.98125, 0.98125, 0.975  , 0.95625, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.95625, 0.7875 , 0.48125, 0.98125, 0.98125, 0.975  , 0.95625,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.96875, 0.78125, 0.46875, 0.98125, 0.98125,\n",
       "        0.975  , 0.95625, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.96875, 0.825  , 0.45625,\n",
       "        0.98125, 0.98125, 0.98125, 0.96875, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.95   ,\n",
       "        0.63125, 0.91875, 0.98125, 0.98125, 0.9    , 0.98125, 0.98125,\n",
       "        0.98125, 0.975  , 0.95   , 0.98125, 0.98125, 0.98125, 0.975  ,\n",
       "        0.98125, 0.95   , 0.975  , 0.8375 , 0.98125, 0.98125, 0.93125,\n",
       "        0.78125, 0.98125, 0.98125, 0.98125, 0.96875, 0.98125, 0.98125,\n",
       "        0.975  , 0.98125, 0.98125, 0.95625, 0.50625, 0.81875, 0.98125,\n",
       "        0.98125, 0.98125, 0.9125 , 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.975  , 0.5875 ,\n",
       "        0.425  , 0.98125, 0.98125, 0.95   , 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.8875 , 0.975  , 0.98125,\n",
       "        0.98125, 0.78125, 0.19375, 0.98125, 0.98125, 0.98125, 0.9125 ,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.975  ]),\n",
       " 'split2_test_score': array([0.98125, 0.98125, 0.98125, 0.975  , 0.975  , 0.98125, 0.73125,\n",
       "        0.73125, 0.38125, 0.3625 , 0.3625 , 0.54375, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.975  , 0.975  , 0.975  , 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.9625 , 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.9375 , 0.98125,\n",
       "        0.98125, 0.98125, 0.975  , 0.975  , 0.975  , 0.73125, 0.73125,\n",
       "        0.98125, 0.3625 , 0.3625 , 0.975  , 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.95   , 0.975  ,\n",
       "        0.975  , 0.80625, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.95   ,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.975  , 0.975  , 0.975  , 0.73125, 0.73125, 0.83125,\n",
       "        0.3625 , 0.3625 , 0.01875, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.91875, 0.975  , 0.975  ,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.9625 ,\n",
       "        0.98125, 0.98125, 0.96875, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.9625 , 0.98125, 0.98125, 0.98125,\n",
       "        0.975  , 0.975  , 0.975  , 0.73125, 0.73125, 0.2    , 0.35   ,\n",
       "        0.35   , 0.01875, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.9375 , 0.975  , 0.975  , 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.9625 , 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.7625 , 0.7625 , 0.7625 , 0.29375, 0.29375,\n",
       "        0.01875, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.975  , 0.975  , 0.975  , 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.975  , 0.73125, 0.3625 , 0.98125,\n",
       "        0.98125, 0.98125, 0.975  , 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.975  , 0.73125,\n",
       "        0.3625 , 0.98125, 0.98125, 0.98125, 0.975  , 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.975  , 0.73125, 0.3625 , 0.98125, 0.98125, 0.98125, 0.975  ,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.975  , 0.70625, 0.35   , 0.98125, 0.98125,\n",
       "        0.98125, 0.975  , 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.73125, 0.29375,\n",
       "        0.98125, 0.98125, 0.98125, 0.975  , 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.975  ,\n",
       "        0.84375, 0.2375 , 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.975  , 0.98125, 0.2875 , 0.98125, 0.98125, 0.975  ,\n",
       "        0.9375 , 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.975  , 0.7375 , 0.60625, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.975  , 0.64375,\n",
       "        0.1625 , 0.98125, 0.98125, 0.98125, 0.8625 , 0.98125, 0.98125,\n",
       "        0.975  , 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.625  , 0.98125, 0.98125, 0.98125, 0.625  ,\n",
       "        0.98125, 0.98125, 0.975  , 0.98125, 0.98125, 0.98125, 0.3    ,\n",
       "        0.98125]),\n",
       " 'split3_test_score': array([0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.76875,\n",
       "        0.76875, 0.80625, 0.43125, 0.43125, 0.79375, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.89375,\n",
       "        0.98125, 0.98125, 0.53125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.90625, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.9875 , 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.76875, 0.76875,\n",
       "        0.8125 , 0.43125, 0.43125, 0.7375 , 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.175  , 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.96875, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.76875, 0.76875, 0.80625,\n",
       "        0.43125, 0.43125, 0.01875, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.93125, 0.98125, 0.98125,\n",
       "        0.93125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.76875, 0.76875, 0.925  , 0.425  ,\n",
       "        0.425  , 0.025  , 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.775  , 0.775  , 0.775  , 0.38125, 0.38125,\n",
       "        0.01875, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.76875, 0.43125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.76875,\n",
       "        0.43125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.76875, 0.43125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.7875 , 0.4375 , 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.96875, 0.84375, 0.5125 ,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.6125 , 0.6875 , 0.98125, 0.98125, 0.98125, 0.95   , 0.98125,\n",
       "        0.98125, 0.98125, 0.95   , 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.95625, 0.63125, 0.98125, 0.98125, 0.98125,\n",
       "        0.78125, 0.98125, 0.98125, 0.95   , 0.9625 , 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.2375 , 0.725  , 0.98125,\n",
       "        0.98125, 0.9625 , 0.98125, 0.98125, 0.98125, 0.98125, 0.5625 ,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.875  ,\n",
       "        0.01875, 0.98125, 0.98125, 0.9625 , 0.83125, 0.98125, 0.98125,\n",
       "        0.98125, 0.96875, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.59375, 0.91875, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.9375 ,\n",
       "        0.98125]),\n",
       " 'split4_test_score': array([0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.95625, 0.7    ,\n",
       "        0.7    , 0.63125, 0.2625 , 0.2625 , 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.7    ,\n",
       "        0.98125, 0.98125, 0.96875, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.95625, 0.7    , 0.7    ,\n",
       "        0.4    , 0.2625 , 0.2625 , 0.21875, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.8625 , 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.95   , 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.7    , 0.7    , 0.59375,\n",
       "        0.2625 , 0.2625 , 0.60625, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.6375 , 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.91875, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.975  , 0.98125, 0.98125,\n",
       "        0.95625, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.70625, 0.70625, 0.20625, 0.25625,\n",
       "        0.25625, 0.30625, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.96875, 0.98125, 0.98125, 0.96875,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.7375 , 0.7375 , 0.7375 , 0.18125, 0.18125,\n",
       "        0.1    , 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.975  , 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.7    , 0.2625 , 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.7    ,\n",
       "        0.2625 , 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.7    , 0.2625 , 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.7    , 0.21875, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.7125 , 0.08125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.58125, 0.225  , 0.98125, 0.98125, 0.925  , 0.98125, 0.98125,\n",
       "        0.98125, 0.975  , 0.9875 , 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.03125, 0.40625, 0.98125, 0.98125, 0.98125,\n",
       "        0.93125, 0.98125, 0.98125, 0.98125, 0.96875, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.4875 , 0.0125 , 0.98125,\n",
       "        0.98125, 0.96875, 0.4625 , 0.98125, 0.98125, 0.98125, 0.975  ,\n",
       "        0.98125, 0.98125, 0.98125, 0.975  , 0.98125, 0.98125, 0.54375,\n",
       "        0.125  , 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125, 0.925  , 0.98125, 0.98125, 0.98125, 0.98125, 0.95   ,\n",
       "        0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125, 0.98125,\n",
       "        0.98125]),\n",
       " 'mean_test_score': array([0.9825 , 0.9825 , 0.9825 , 0.975  , 0.975  , 0.9775 , 0.73875,\n",
       "        0.73875, 0.49125, 0.36125, 0.36125, 0.64625, 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.98125, 0.98125, 0.885  ,\n",
       "        0.975  , 0.975  , 0.85125, 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.96   , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.97875,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9625 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.975  , 0.975  , 0.9625 , 0.73875, 0.73875,\n",
       "        0.765  , 0.36125, 0.36125, 0.77125, 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.98125, 0.98125, 0.9675 , 0.975  ,\n",
       "        0.975  , 0.71125, 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9775 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.97125,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.97625, 0.9825 , 0.9825 , 0.98   , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.975  , 0.975  , 0.955  , 0.74   , 0.74   , 0.63875,\n",
       "        0.36125, 0.36125, 0.47   , 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.98125, 0.98125, 0.93625, 0.975  , 0.975  ,\n",
       "        0.87375, 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.97125,\n",
       "        0.9825 , 0.9825 , 0.98   , 0.9825 , 0.9825 , 0.96625, 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.97875, 0.9825 , 0.9825 ,\n",
       "        0.96875, 0.9825 , 0.9825 , 0.96   , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.97875, 0.97875, 0.97875, 0.74375, 0.74375, 0.50625, 0.3525 ,\n",
       "        0.3525 , 0.1325 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.98125, 0.98125, 0.96625, 0.975  , 0.975  , 0.98   ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.98   , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.98   ,\n",
       "        0.9825 , 0.9825 , 0.97375, 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.76   , 0.76   , 0.76   , 0.29625, 0.29625,\n",
       "        0.3425 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.97875, 0.97875, 0.98   , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.975  , 0.73875, 0.36125, 0.9825 ,\n",
       "        0.9825 , 0.98125, 0.975  , 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.975  , 0.73875,\n",
       "        0.36125, 0.9825 , 0.9825 , 0.98125, 0.975  , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.975  , 0.74   , 0.3575 , 0.9825 , 0.9825 , 0.98125, 0.975  ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9775 , 0.7375 , 0.33875, 0.9825 , 0.9825 ,\n",
       "        0.98125, 0.975  , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9775 , 0.7575 , 0.29625,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9775 , 0.9825 , 0.9825 , 0.9825 ,\n",
       "        0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.9825 , 0.97375,\n",
       "        0.6625 , 0.61125, 0.9825 , 0.9825 , 0.955  , 0.9725 , 0.9825 ,\n",
       "        0.9825 , 0.98   , 0.97125, 0.9825 , 0.98125, 0.9825 , 0.98125,\n",
       "        0.9825 , 0.97375, 0.675  , 0.60625, 0.9825 , 0.9825 , 0.97125,\n",
       "        0.85125, 0.9825 , 0.9775 , 0.97625, 0.9725 , 0.9825 , 0.98125,\n",
       "        0.98125, 0.9825 , 0.9825 , 0.975  , 0.48   , 0.4825 , 0.9825 ,\n",
       "        0.9825 , 0.96875, 0.865  , 0.9825 , 0.9825 , 0.98   , 0.8975 ,\n",
       "        0.9825 , 0.9825 , 0.98125, 0.98   , 0.9825 , 0.97875, 0.65   ,\n",
       "        0.32875, 0.9825 , 0.9825 , 0.9725 , 0.9275 , 0.9825 , 0.9825 ,\n",
       "        0.98   , 0.8075 , 0.9825 , 0.9825 , 0.9625 , 0.98   , 0.9825 ,\n",
       "        0.9825 , 0.8525 , 0.545  , 0.9825 , 0.9825 , 0.9825 , 0.89125,\n",
       "        0.9825 , 0.9825 , 0.98125, 0.98   , 0.9825 , 0.9825 , 0.8375 ,\n",
       "        0.98125]),\n",
       " 'std_test_score': array([0.0025    , 0.0025    , 0.0025    , 0.00968246, 0.00968246,\n",
       "        0.01089725, 0.03432383, 0.03432383, 0.21208047, 0.09298857,\n",
       "        0.09298857, 0.31557487, 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.00395285, 0.00395285,\n",
       "        0.10304732, 0.00968246, 0.00968246, 0.17380844, 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.02866836, 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.00847791, 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.02338536, 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.00968246, 0.00968246, 0.01311011, 0.03432383,\n",
       "        0.03432383, 0.2030471 , 0.09298857, 0.09298857, 0.29022082,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.00395285, 0.00395285, 0.01695582, 0.00968246,\n",
       "        0.00968246, 0.28141384, 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.005     , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.01286954, 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.01334635, 0.0025    , 0.0025    ,\n",
       "        0.00612372, 0.0025    , 0.0025    , 0.0025    , 0.00968246,\n",
       "        0.00968246, 0.04337338, 0.03614208, 0.03614208, 0.16693   ,\n",
       "        0.09298857, 0.09298857, 0.38361765, 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.00395285,\n",
       "        0.00395285, 0.03020761, 0.00968246, 0.00968246, 0.13070721,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.01286954, 0.0025    , 0.0025    , 0.00612372, 0.0025    ,\n",
       "        0.0025    , 0.0242384 , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.00637377, 0.0025    , 0.0025    ,\n",
       "        0.01581139, 0.0025    , 0.0025    , 0.03391165, 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.00306186, 0.00306186, 0.00306186,\n",
       "        0.03557562, 0.03557562, 0.28168023, 0.09122431, 0.09122431,\n",
       "        0.13195643, 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.00395285, 0.00395285, 0.01510381,\n",
       "        0.00968246, 0.00968246, 0.00612372, 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.00728869, 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0266927 , 0.0266927 ,\n",
       "        0.0266927 , 0.1052972 , 0.1052972 , 0.38459394, 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.00306186, 0.00306186,\n",
       "        0.00467707, 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.00968246, 0.03432383, 0.09298857, 0.0025    ,\n",
       "        0.0025    , 0.00395285, 0.00968246, 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.00968246, 0.03432383, 0.09298857,\n",
       "        0.0025    , 0.0025    , 0.00395285, 0.00968246, 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.00968246, 0.03321333,\n",
       "        0.09094985, 0.0025    , 0.0025    , 0.00395285, 0.00968246,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.005     ,\n",
       "        0.03852759, 0.10543066, 0.0025    , 0.0025    , 0.00395285,\n",
       "        0.00968246, 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0075    , 0.06559821, 0.16958589, 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.005     , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.0121192 , 0.09303897, 0.32582587, 0.0025    ,\n",
       "        0.0025    , 0.03566336, 0.01224745, 0.0025    , 0.0025    ,\n",
       "        0.00467707, 0.0175    , 0.0025    , 0.        , 0.0025    ,\n",
       "        0.00395285, 0.0025    , 0.0121192 , 0.38385381, 0.23004755,\n",
       "        0.0025    , 0.0025    , 0.02038688, 0.06975403, 0.0025    ,\n",
       "        0.0075    , 0.01334635, 0.0075    , 0.0025    , 0.        ,\n",
       "        0.00395285, 0.0025    , 0.0025    , 0.00968246, 0.16014642,\n",
       "        0.30401994, 0.0025    , 0.0025    , 0.01185854, 0.20312404,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.16754664, 0.0025    ,\n",
       "        0.0025    , 0.        , 0.0025    , 0.0025    , 0.00306186,\n",
       "        0.11692679, 0.32102375, 0.0025    , 0.0025    , 0.01403122,\n",
       "        0.06656763, 0.0025    , 0.0025    , 0.0025    , 0.34128434,\n",
       "        0.0025    , 0.0025    , 0.0375    , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.14861864, 0.3873064 , 0.0025    , 0.0025    ,\n",
       "        0.0025    , 0.13576174, 0.0025    , 0.0025    , 0.00395285,\n",
       "        0.0025    , 0.0025    , 0.0025    , 0.26934527, 0.00395285]),\n",
       " 'rank_test_score': array([  1,   1,   1, 295, 295, 287, 362, 362, 379, 383, 383, 373,   1,\n",
       "          1,   1,   1,   1,   1, 248, 248, 343, 295, 295, 348,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1, 335,   1,   1,   1,\n",
       "          1,   1, 279,   1,   1,   1,   1,   1, 332,   1,   1,   1, 295,\n",
       "        295, 332, 362, 362, 352, 383, 383, 351,   1,   1,   1,   1,   1,\n",
       "          1, 248, 248, 329, 295, 295, 369,   1,   1,   1,   1,   1, 287,\n",
       "          1,   1,   1,   1,   1, 323,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1, 293,   1,   1, 267,   1,   1,   1, 295, 295, 337, 359, 359,\n",
       "        374, 383, 383, 382,   1,   1,   1,   1,   1,   1, 248, 248, 339,\n",
       "        295, 295, 344,   1,   1,   1,   1,   1, 323,   1,   1, 267,   1,\n",
       "          1, 330,   1,   1,   1,   1,   1, 280,   1,   1, 327,   1,   1,\n",
       "        335,   1,   1,   1, 280, 280, 280, 357, 357, 378, 392, 392, 400,\n",
       "          1,   1,   1,   1,   1,   1, 248, 248, 330, 295, 295, 267,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1, 267,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1, 267,   1,   1, 317,   1,   1,   1,\n",
       "          1,   1,   1, 353, 353, 353, 398, 398, 394,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1, 280, 280, 276,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1, 295, 362, 383,   1,   1, 248,\n",
       "        295,   1,   1,   1,   1,   1,   1,   1,   1,   1, 295, 362, 383,\n",
       "          1,   1, 248, 295,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "        295, 359, 391,   1,   1, 248, 295,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1, 287, 368, 395,   1,   1, 248, 295,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1, 287, 356, 397,   1,   1,   1, 287,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1, 317, 371, 375,   1,\n",
       "          1, 337, 320,   1,   1, 276, 323,   1, 248,   1, 248,   1, 317,\n",
       "        370, 376,   1,   1, 323, 347,   1, 287, 293, 320,   1, 248, 248,\n",
       "          1,   1, 295, 381, 380,   1,   1, 327, 345,   1,   1, 267, 341,\n",
       "          1,   1, 248, 276,   1, 280, 372, 396,   1,   1, 320, 340,   1,\n",
       "          1, 267, 350,   1,   1, 332, 267,   1,   1, 346, 377,   1,   1,\n",
       "          1, 342,   1,   1, 248, 267,   1,   1, 349, 248])}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = grid.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.995\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       200\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99       200\n",
      "   macro avg       0.50      0.50      0.50       200\n",
      "weighted avg       1.00      0.99      1.00       200\n",
      "\n",
      "[[199   1]\n",
      " [  0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\nihar\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "score = accuracy_score(y_pred, y_test)\n",
    "print(score)\n",
    "print(classification_report(y_pred, y_test))\n",
    "print(confusion_matrix(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logestic Regression with ROC curvw and ROC AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc curve and auc\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 2 class dataset\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,y,test_size=0.10,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Created a dummy model with default 0 as output \n",
    "dummy_model_prob = [0 for _ in range(len(y_test))]\n",
    "dummy_model_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-11 {color: black;background-color: white;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" checked><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prob = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99816799, 0.00183201],\n",
       "       [0.98734262, 0.01265738],\n",
       "       [0.36096379, 0.63903621],\n",
       "       [0.36425511, 0.63574489],\n",
       "       [0.22610271, 0.77389729],\n",
       "       [0.8844746 , 0.1155254 ],\n",
       "       [0.74477363, 0.25522637],\n",
       "       [0.95061624, 0.04938376],\n",
       "       [0.35988183, 0.64011817],\n",
       "       [0.54634991, 0.45365009],\n",
       "       [0.07597853, 0.92402147],\n",
       "       [0.71301873, 0.28698127],\n",
       "       [0.21791542, 0.78208458],\n",
       "       [0.21712424, 0.78287576],\n",
       "       [0.92675981, 0.07324019],\n",
       "       [0.9928822 , 0.0071178 ],\n",
       "       [0.7835632 , 0.2164368 ],\n",
       "       [0.48799541, 0.51200459],\n",
       "       [0.15009711, 0.84990289],\n",
       "       [0.01457321, 0.98542679],\n",
       "       [0.05268202, 0.94731798],\n",
       "       [0.92843353, 0.07156647],\n",
       "       [0.95386124, 0.04613876],\n",
       "       [0.27667368, 0.72332632],\n",
       "       [0.08454816, 0.91545184],\n",
       "       [0.30400332, 0.69599668],\n",
       "       [0.18623319, 0.81376681],\n",
       "       [0.99685816, 0.00314184],\n",
       "       [0.29257466, 0.70742534],\n",
       "       [0.26849299, 0.73150701],\n",
       "       [0.01738002, 0.98261998],\n",
       "       [0.64895944, 0.35104056],\n",
       "       [0.01968748, 0.98031252],\n",
       "       [0.79117036, 0.20882964],\n",
       "       [0.99495306, 0.00504694],\n",
       "       [0.95701703, 0.04298297],\n",
       "       [0.00151535, 0.99848465],\n",
       "       [0.71059536, 0.28940464],\n",
       "       [0.15573224, 0.84426776],\n",
       "       [0.15826821, 0.84173179],\n",
       "       [0.28035624, 0.71964376],\n",
       "       [0.60792132, 0.39207868],\n",
       "       [0.99552268, 0.00447732],\n",
       "       [0.96776637, 0.03223363],\n",
       "       [0.76018604, 0.23981396],\n",
       "       [0.90527131, 0.09472869],\n",
       "       [0.96690599, 0.03309401],\n",
       "       [0.40125381, 0.59874619],\n",
       "       [0.98624051, 0.01375949],\n",
       "       [0.21421474, 0.78578526],\n",
       "       [0.99413727, 0.00586273],\n",
       "       [0.92258072, 0.07741928],\n",
       "       [0.34120074, 0.65879926],\n",
       "       [0.11850741, 0.88149259],\n",
       "       [0.57205622, 0.42794378],\n",
       "       [0.99886904, 0.00113096],\n",
       "       [0.91801341, 0.08198659],\n",
       "       [0.13446449, 0.86553551],\n",
       "       [0.7722632 , 0.2277368 ],\n",
       "       [0.03822819, 0.96177181],\n",
       "       [0.7095021 , 0.2904979 ],\n",
       "       [0.82355309, 0.17644691],\n",
       "       [0.21853391, 0.78146609],\n",
       "       [0.96292992, 0.03707008],\n",
       "       [0.58771342, 0.41228658],\n",
       "       [0.95008493, 0.04991507],\n",
       "       [0.93469348, 0.06530652],\n",
       "       [0.47954625, 0.52045375],\n",
       "       [0.8252984 , 0.1747016 ],\n",
       "       [0.25387834, 0.74612166],\n",
       "       [0.01638893, 0.98361107],\n",
       "       [0.76745731, 0.23254269],\n",
       "       [0.00211978, 0.99788022],\n",
       "       [0.17744743, 0.82255257],\n",
       "       [0.12328052, 0.87671948],\n",
       "       [0.09054133, 0.90945867],\n",
       "       [0.99732531, 0.00267469],\n",
       "       [0.95727532, 0.04272468],\n",
       "       [0.55089646, 0.44910354],\n",
       "       [0.97223817, 0.02776183],\n",
       "       [0.34356137, 0.65643863],\n",
       "       [0.95748322, 0.04251678],\n",
       "       [0.71678005, 0.28321995],\n",
       "       [0.13548662, 0.86451338],\n",
       "       [0.97338137, 0.02661863],\n",
       "       [0.03421868, 0.96578132],\n",
       "       [0.95998174, 0.04001826],\n",
       "       [0.02263212, 0.97736788],\n",
       "       [0.69993285, 0.30006715],\n",
       "       [0.00604386, 0.99395614],\n",
       "       [0.92577555, 0.07422445],\n",
       "       [0.06678638, 0.93321362],\n",
       "       [0.02102623, 0.97897377],\n",
       "       [0.08900545, 0.91099455],\n",
       "       [0.00720395, 0.99279605],\n",
       "       [0.15727942, 0.84272058],\n",
       "       [0.78006609, 0.21993391],\n",
       "       [0.9586498 , 0.0413502 ],\n",
       "       [0.92416694, 0.07583306],\n",
       "       [0.09990044, 0.90009956]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets focus on the postieve outcome\n",
    "model_prob = model_prob[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.9017857142857144\n"
     ]
    }
   ],
   "source": [
    "#Lets calculate the score\n",
    "dummy_model_auc=roc_auc_score(y_test,dummy_model_prob)\n",
    "model_auc=roc_auc_score(y_test,model_prob)\n",
    "print(dummy_model_auc)\n",
    "print(model_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate ROC Curves\n",
    "dummy_fpr, dummy_tpr, _ = roc_curve(y_test, dummy_model_prob)\n",
    "model_fpr, model_tpr, thresholds = roc_curve(y_test, model_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.99848465e+00, 9.98484650e-01, 9.85426791e-01, 9.83611075e-01,\n",
       "       8.65535511e-01, 8.64513382e-01, 8.13766815e-01, 7.85785262e-01,\n",
       "       7.81466094e-01, 7.31507007e-01, 7.07425343e-01, 6.95996680e-01,\n",
       "       6.35744891e-01, 5.98746190e-01, 5.20453749e-01, 4.49103540e-01,\n",
       "       4.27943778e-01, 2.19933909e-01, 2.16436802e-01, 2.08829643e-01,\n",
       "       1.76446911e-01, 7.58330569e-02, 7.42244496e-02, 4.47731532e-03,\n",
       "       3.14184433e-03, 1.13095978e-03])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.        , 0.        , 0.        , 0.01785714, 0.01785714,\n",
       "        0.03571429, 0.03571429, 0.05357143, 0.05357143, 0.10714286,\n",
       "        0.10714286, 0.125     , 0.125     , 0.14285714, 0.14285714,\n",
       "        0.19642857, 0.19642857, 0.42857143, 0.42857143, 0.44642857,\n",
       "        0.44642857, 0.55357143, 0.55357143, 0.94642857, 0.94642857,\n",
       "        1.        ]),\n",
       " array([0.        , 0.02272727, 0.11363636, 0.11363636, 0.47727273,\n",
       "        0.47727273, 0.61363636, 0.61363636, 0.68181818, 0.68181818,\n",
       "        0.75      , 0.75      , 0.86363636, 0.86363636, 0.88636364,\n",
       "        0.88636364, 0.90909091, 0.90909091, 0.93181818, 0.93181818,\n",
       "        0.95454545, 0.95454545, 0.97727273, 0.97727273, 1.        ,\n",
       "        1.        ]))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fpr,model_tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(dummy_fpr, dummy_tpr, linestyle='--', label='Dummy Model')\n",
    "pyplot.plot(model_fpr, model_tpr, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x5000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the roc curve for the model\n",
    "import numpy as np\n",
    "fig = pyplot.figure(figsize=(20,50))\n",
    "pyplot.plot(dummy_fpr, dummy_tpr, linestyle='--', label='Dummy Model')\n",
    "pyplot.plot(model_fpr, model_tpr, marker='.', label='Logistic')\n",
    "ax = fig.add_subplot(111)\n",
    "for xyz in zip(model_fpr, model_tpr,thresholds):   \n",
    "    ax.annotate('%s' % np.round(xyz[2],2), xy=(xyz[0],xyz[1]))\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
